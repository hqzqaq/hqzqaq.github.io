<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>hive |  hqz的博客</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <script src="https://cdn.staticfile.org/mermaid/8.14.0/mermaid.min.js"></script>
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
      <canvas width="1777" height="841"
        style="position: fixed; left: 0px; top: 0px; z-index: 99999; pointer-events: none;"></canvas>
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-hive"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  hive
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/10/06/hive/" class="article-date">
  <time datetime="2022-10-06T11:15:38.000Z" itemprop="datePublished">2022-10-06</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigData/">bigData</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">16k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">68 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h1><p>Hive 是由 Facebook 开源用于解决海量结构化日志的数据统计工具，是基于 Hadoop 的一个<code>数据仓库工具</code>，可以将<code>结构化的数据文件映射为一张表</code>，并提供类 SQL 查询功能。</p>
<p>本质是将 HQL 转化成 MapReduce 程序</p>
<ol>
<li>Hive 处理的数据存储在 HDFS 上</li>
<li>Hive 分析数据的底层实现是 MapReduce</li>
<li>程序运行在 Yarn 上</li>
</ol>
<span id="more"></span>

<h2 id="1、Hive-的优缺点"><a href="#1、Hive-的优缺点" class="headerlink" title="1、Hive 的优缺点"></a>1、Hive 的优缺点</h2><h3 id="1-1、优点"><a href="#1-1、优点" class="headerlink" title="1.1、优点"></a>1.1、优点</h3><ol>
<li>操作接口采用类 SQL 语法，提供快速开发的能力（简单、容易上手）</li>
<li>避免了去写 MapReduce，减少了开发人员的学习成本</li>
<li>Hive 的执行延迟比较高，因此 Hive 常用于数据分析，对实时性要求不高的场合</li>
<li>Hive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高</li>
<li>Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数</li>
</ol>
<h3 id="1-2、缺点"><a href="#1-2、缺点" class="headerlink" title="1.2、缺点"></a>1.2、缺点</h3><ol>
<li>Hive 的 HQL 表达能力有限<ul>
<li>迭代式算法无法表达</li>
<li>数据挖掘方面不擅长，由于 MapReduce 数据处理流程的限制，效率更高的算法缺无法实现</li>
</ul>
</li>
<li>Hive 的效率比较低<ul>
<li>Hive 自动生成的 MapReduce 作业通常情况下不够智能化</li>
<li>Hive 调优比较困难，粒度较粗</li>
</ul>
</li>
</ol>
<h2 id="2、Hive-架构"><a href="#2、Hive-架构" class="headerlink" title="2、Hive 架构"></a>2、Hive 架构</h2><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220729110915874.png" alt="Hive 架构"></p>
<ol>
<li><p>用户接口：</p>
<ul>
<li>Client CLI（command-line interface）</li>
<li>JDBC&#x2F;ODBC(jdbc 访问 hive)</li>
<li>WEBUI（浏览器访问 hive）</li>
</ul>
</li>
<li><p>元数据：Metastore 元数据包括：</p>
<ul>
<li>表名、表所属的数据库（默认是 default）</li>
<li>表的拥有者、列&#x2F;分区字段、 表的类型（是否是外部表）</li>
<li>表的数据所在目录等</li>
</ul>
<p><code>默认存储在自带的 derby 数据库中，推荐使用 MySQL 存储 Metastore</code></p>
</li>
<li><p>Hadoop：使用 HDFS 进行存储，使用 MapReduce 进行计算。 </p>
</li>
<li><p>驱动器：Driver </p>
<ul>
<li>解析器（SQL Parser）：将 SQL 字符串转换成抽象语法树 AST，这一步一般都用第 三方工具库完成，比如 antlr；对 AST 进行语法分析，比如表是否存在、字段是否存在、SQL 语义是否有误。</li>
<li>编译器（Physical Plan）：将 AST 编译生成逻辑执行计划。</li>
<li>优化器（Query Optimizer）：对逻辑执行计划进行优化。</li>
<li>执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于 Hive 来 说，就是 MR&#x2F;Spark。</li>
</ul>
</li>
</ol>
<p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220729140108234.png" alt="Hive运行机制"></p>
<p>Hive 通过给用户提供的一系列交互接口，接收到用户的指令（SQL），使用自己的 Driver， 结合元数据（MetaStore），将这些指令翻译成 MapReduce，提交到 Hadoop 中执行，最后，将执行返回的结果输出到用户交互接口。</p>
<h2 id="3、Hive-和数据库比较"><a href="#3、Hive-和数据库比较" class="headerlink" title="3、Hive 和数据库比较"></a>3、Hive 和数据库比较</h2><blockquote>
<p>查询语言</p>
</blockquote>
<p>Hive 使用的是类 SQL 的查询语言 HQL</p>
<blockquote>
<p>数据更新</p>
</blockquote>
<p>Hive 是针对数据仓库应用设计的，而<code>数据仓库的内容是读多写少</code>的。因此，Hive 中 <code>不建议对数据的改写</code>，所有的数据都是在加载的时候确定好的。而数据库中的数据通常是需 要经常进行修改的</p>
<blockquote>
<p>执行延迟</p>
</blockquote>
<p>Hive 在查询数据的时候，由于没有索引和 MapReduce 本身的特性，因此延迟较高。数据库的执行延迟较低，但当数据规模大到超过数据库的处理能力的时候， Hive 的并行计算就能体现出优势</p>
<blockquote>
<p>数据规模</p>
</blockquote>
<p>Hive 建立在集群上并可以利用 MapReduce 进行并行计算，因此可以支持很大规模 的数据</p>
<h2 id="4、Hive-安装"><a href="#4、Hive-安装" class="headerlink" title="4、Hive 安装"></a>4、Hive 安装</h2><p>从官网下载 apache-hive-3.1.3-bin.tar.gz 解压到 hadoop100 &#x2F;opt&#x2F;module 目录下</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改名称为 hive</span></span><br><span class="line"><span class="built_in">mv</span> /opt/module/apache-hive-3.1.3-bin/ /opt/module/hive</span><br><span class="line"><span class="comment"># 增加环境变量</span></span><br><span class="line">vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 添加内容</span></span><br><span class="line"><span class="comment">#HIVE_HOME</span></span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/opt/module/hive</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br><span class="line"><span class="comment"># 解决日志 jar 包冲突</span></span><br><span class="line"> <span class="built_in">mv</span> <span class="variable">$HIVE_HOME</span>/lib/log4j-slf4j-impl-2.17.1.jar <span class="variable">$HIVE_HOME</span>/lib/log4j-slf4j-impl-2.17.1.bak</span><br><span class="line"> <span class="comment"># 初始化数据库，在 /opt/module/hive 路径下执行 derby zhi</span></span><br><span class="line"> bin/schematool -dbType derby -initSchema</span><br></pre></td></tr></table></figure>

<h2 id="5、Hive-启动"><a href="#5、Hive-启动" class="headerlink" title="5、Hive 启动"></a>5、Hive 启动</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先启动hadoop 集群</span></span><br><span class="line">myhadoop.sh start</span><br><span class="line"><span class="comment"># 启动 hive</span></span><br><span class="line">bin/hive</span><br><span class="line"><span class="comment"># 执行命令</span></span><br><span class="line">show databases;</span><br><span class="line">show tables;</span><br><span class="line">create table <span class="built_in">test</span>(<span class="built_in">id</span> int);</span><br><span class="line">insert into <span class="built_in">test</span> values(1001);</span><br><span class="line">select * from <span class="built_in">test</span>;</span><br><span class="line"><span class="comment"># 默认的存储路径在 hdfs 中的 /user/hive/warehouse/</span></span><br></pre></td></tr></table></figure>

<h2 id="6、Mysql-安装"><a href="#6、Mysql-安装" class="headerlink" title="6、Mysql 安装"></a>6、Mysql 安装</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将 mysql-5.7.36-1.el7.x86_64.rpm-bundle.tar 安装包上传到 /opt/software 目录</span></span><br><span class="line"><span class="comment"># 解压安装包</span></span><br><span class="line">tar -xf mysql-5.7.36-1.el7.x86_64.rpm-bundle.tar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除自带的数据库</span></span><br><span class="line">rpm -e --nodeps mariadb-libs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行 rpm 安装</span></span><br><span class="line">rpm -ivh mysql-community-common-5.7.36-1.el7.x86_64.rpm </span><br><span class="line">rpm -ivh mysql-community-libs-5.7.36-1.el7.x86_64.rpm </span><br><span class="line">rpm -ivh mysql-community-libs-compat-5.7.36-1.el7.x86_64.rpm </span><br><span class="line">rpm -ivh mysql-community-client-5.7.36-1.el7.x86_64.rpm </span><br><span class="line">rpm -ivh mysql-community-server-5.7.36-1.el7.x86_64.rpm </span><br><span class="line"></span><br><span class="line"><span class="comment"># mysql 初始化</span></span><br><span class="line">mysqld --initialize --user=mysql</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看初始密码</span></span><br><span class="line"><span class="comment"># 查看日志的存放位置</span></span><br><span class="line"><span class="built_in">cat</span> /etc/my.cnf</span><br><span class="line"><span class="comment"># 查看日志文件路径 log-error=/var/log/mysqld.log</span></span><br><span class="line"><span class="built_in">cat</span> /var/log/mysqld.log</span><br><span class="line"><span class="comment"># A temporary password is generated for root@localhost: ?,IUZo?Cz5&amp;)</span></span><br><span class="line"><span class="comment"># 复制密码：?,IUZo?Cz5&amp;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 mysql 开机自启</span></span><br><span class="line">systemctl start mysqld</span><br><span class="line"></span><br><span class="line"><span class="comment"># 登录</span></span><br><span class="line">mysql -u root -p</span><br><span class="line"><span class="comment"># 修改密码</span></span><br><span class="line"><span class="built_in">set</span> password=password(<span class="string">&quot;root&quot;</span>);</span><br><span class="line"><span class="comment"># 设置 mysql 为任意 ip 可访问</span></span><br><span class="line">use mysql;</span><br><span class="line">update mysql.user <span class="built_in">set</span> host=<span class="string">&quot;%&quot;</span> <span class="built_in">where</span> user=<span class="string">&#x27;root&#x27;</span>;</span><br><span class="line"><span class="comment"># 设置立即生效</span></span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure>

<h2 id="7、Hive-元数据配置到-Mysql"><a href="#7、Hive-元数据配置到-Mysql" class="headerlink" title="7、Hive 元数据配置到 Mysql"></a>7、Hive 元数据配置到 Mysql</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加 mysql 连接驱动包到 hive 目录下的 lib 文件夹下</span></span><br><span class="line"><span class="built_in">cp</span> /opt/software/mysql-connector-java-5.1.37.jar ./lib/</span><br><span class="line"><span class="comment"># 在 hive 目录下的 conf 文件夹下 添加 hive-site.xml 文件</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- jdbc 连接的 URL --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop100:3306/metastore?useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- jdbc 连接的 Driver--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- jdbc 连接的 username--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- jdbc 连接的 password --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Hive 元数据存储版本的验证 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--元数据存储授权--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Hive 默认在 HDFS 的工作目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 登录 MySQL</span></span><br><span class="line">mysql -u root -p</span><br><span class="line"><span class="comment"># 新建 Hive 元数据库</span></span><br><span class="line">create database metastore;</span><br><span class="line">quit;</span><br><span class="line"><span class="comment"># 初始化 Hive 元数据库</span></span><br><span class="line">schematool -initSchema -dbType mysql -verbose</span><br><span class="line"><span class="comment"># 再次启动 hive</span></span><br><span class="line">bin/hive</span><br><span class="line">show databases;</span><br><span class="line">show tables;</span><br><span class="line">create table <span class="built_in">test</span> (<span class="built_in">id</span> int);</span><br><span class="line">insert into <span class="built_in">test</span> values(1);</span><br><span class="line">select * from <span class="built_in">test</span>;</span><br><span class="line"><span class="comment"># 新打开一个 ssh 连接窗口，重复 hive 的命令，看是否能连接 hive ，hive 命令是否能执行</span></span><br></pre></td></tr></table></figure>

<h2 id="8、使用元数据服务的方式访问-Hive"><a href="#8、使用元数据服务的方式访问-Hive" class="headerlink" title="8、使用元数据服务的方式访问 Hive"></a>8、使用元数据服务的方式访问 Hive</h2><ol>
<li><p>在 hive-site.xml 文件中添加一下配置信息</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定存储元数据要连接的地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://hadoop100:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>启动 metastore</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive --service metastore</span><br><span class="line"><span class="comment"># 启动后该窗口不能再操作，需要打开一个新的 shell 窗口执行操作</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="9、使用-JDBC-方式访问-Hive"><a href="#9、使用-JDBC-方式访问-Hive" class="headerlink" title="9、使用 JDBC 方式访问 Hive"></a>9、使用 JDBC 方式访问 Hive</h2><p>这种方式依赖于<code>元数据服务</code>的启动</p>
<ol>
<li><p>在 hive-site.xml 文件中添加一下配置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 指定 hiveserver2 连接的 host --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定 hiveserver2 连接的端口号 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 这个选项设置为false,查询将会使用运行 hiveserver2 的用户 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.enable.doAs <span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>启动 hiveserver2</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive --service hiveserver2</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动 beeline 客户端</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/beeline -u jdbc:hive2://hadoop100:10000</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写 hive 服务启动脚本</p>
<ul>
<li><p>nohup: 放在命令开头，表示不挂起,也就是关闭终端进程也继续保持运行状态</p>
</li>
<li><p>&#x2F;dev&#x2F;null：是 Linux 文件系统中的一个文件，被称为黑洞，所有写入改文件的内容 都会被自动丢弃</p>
</li>
<li><p>2&gt;&amp;1 : 表示将错误重定向到标准输出上</p>
</li>
<li><p>&amp;: 放在命令结尾,表示后台运行 一般会组合使用: nohup [xxx 命令操作]&gt; file 2&gt;&amp;1 &amp;，表示将 xxx 命令运行的结 果输出到 file 中，并保持命令启动的进程在后台运行。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> hive --service metastore 2&gt;&amp;1 &amp;</span><br><span class="line"><span class="built_in">nohup</span> hive --service metastore 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加 hiveservices.sh 脚本到 &#x2F;home&#x2F;hqz&#x2F;bin</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">HIVE_LOG_DIR=<span class="variable">$HIVE_HOME</span>/logs</span><br><span class="line"><span class="keyword">if</span> [ ! -d <span class="variable">$HIVE_LOG_DIR</span> ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">mkdir</span> -p <span class="variable">$HIVE_LOG_DIR</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment">#检查进程是否运行正常，参数 1 为进程名，参数 2 为进程端口</span></span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">check_process</span></span>() &#123;</span><br><span class="line">    pid=$(ps -ef 2&gt;/dev/null | grep -v grep | grep -i <span class="variable">$1</span> | awk <span class="string">&#x27;&#123;print </span></span><br><span class="line"><span class="string">$2&#125;&#x27;</span>)</span><br><span class="line">    ppid=$(</span><br><span class="line">        netstat -nltp 2&gt;/dev/null | grep <span class="variable">$2</span> | awk <span class="string">&#x27;&#123;print $7&#125;&#x27;</span> | <span class="built_in">cut</span> -</span><br><span class="line">        d <span class="string">&#x27;/&#x27;</span> -f 1</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$pid</span></span><br><span class="line">    [[ <span class="string">&quot;<span class="variable">$pid</span>&quot;</span> =~ <span class="string">&quot;<span class="variable">$ppid</span>&quot;</span> ]] &amp;&amp; [ <span class="string">&quot;<span class="variable">$ppid</span>&quot;</span> ] &amp;&amp; <span class="built_in">return</span> 0 || <span class="built_in">return</span> 1</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">hive_start</span></span>() &#123;</span><br><span class="line">    metapid=$(check_process HiveMetastore 9083)</span><br><span class="line">    cmd=<span class="string">&quot;nohup hive --service metastore &gt;<span class="variable">$HIVE_LOG_DIR</span>/metastore.log 2&gt;&amp;1 </span></span><br><span class="line"><span class="string">&amp;&quot;</span></span><br><span class="line">    [ -z <span class="string">&quot;<span class="variable">$metapid</span>&quot;</span> ] &amp;&amp; <span class="built_in">eval</span> <span class="variable">$cmd</span> || <span class="built_in">echo</span> <span class="string">&quot;Metastroe 服务已启动&quot;</span></span><br><span class="line">    server2pid=$(check_process HiveServer2 10000)</span><br><span class="line">    cmd=<span class="string">&quot;nohup hiveserver2 &gt;<span class="variable">$HIVE_LOG_DIR</span>/hiveServer2.log 2&gt;&amp;1 &amp;&quot;</span></span><br><span class="line">    [ -z <span class="string">&quot;<span class="variable">$server2pid</span>&quot;</span> ] &amp;&amp; <span class="built_in">eval</span> <span class="variable">$cmd</span> || <span class="built_in">echo</span> <span class="string">&quot;HiveServer2 服务已启动&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">hive_stop</span></span>() &#123;</span><br><span class="line">    metapid=$(check_process HiveMetastore 9083)</span><br><span class="line">    [ <span class="string">&quot;<span class="variable">$metapid</span>&quot;</span> ] &amp;&amp; <span class="built_in">kill</span> <span class="variable">$metapid</span> || <span class="built_in">echo</span> <span class="string">&quot;Metastore 服务未启动&quot;</span></span><br><span class="line">    server2pid=$(check_process HiveServer2 10000)</span><br><span class="line">    [ <span class="string">&quot;<span class="variable">$server2pid</span>&quot;</span> ] &amp;&amp; <span class="built_in">kill</span> <span class="variable">$server2pid</span> || <span class="built_in">echo</span> <span class="string">&quot;HiveServer2 服务未启动&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">&quot;start&quot;</span>)</span><br><span class="line">    hive_start</span><br><span class="line">    ;;</span><br><span class="line"><span class="string">&quot;stop&quot;</span>)</span><br><span class="line">    hive_stop</span><br><span class="line">    ;;</span><br><span class="line"><span class="string">&quot;restart&quot;</span>)</span><br><span class="line">    hive_stop</span><br><span class="line">    <span class="built_in">sleep</span> 2</span><br><span class="line">    hive_start</span><br><span class="line">    ;;</span><br><span class="line"><span class="string">&quot;status&quot;</span>)</span><br><span class="line">    check_process HiveMetastore 9083 &gt;/dev/null &amp;&amp; <span class="built_in">echo</span> <span class="string">&quot;Metastore 服务运行</span></span><br><span class="line"><span class="string">正常&quot;</span> || <span class="built_in">echo</span> <span class="string">&quot;Metastore 服务运行异常&quot;</span></span><br><span class="line">    check_process HiveServer2 10000 &gt;/dev/null &amp;&amp; <span class="built_in">echo</span> <span class="string">&quot;HiveServer2 服务运</span></span><br><span class="line"><span class="string">行正常&quot;</span> || <span class="built_in">echo</span> <span class="string">&quot;HiveServer2 服务运行异常&quot;</span></span><br><span class="line">    ;;</span><br><span class="line">*)</span><br><span class="line">    <span class="built_in">echo</span> Invalid Args!</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&#x27;Usage: &#x27;</span>$(<span class="built_in">basename</span> <span class="variable">$0</span>)<span class="string">&#x27; start|stop|restart|status&#x27;</span></span><br><span class="line">    ;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加执行权限</span></span><br><span class="line"><span class="built_in">chmod</span> +x hiveservices.sh</span><br><span class="line"><span class="comment"># 启动 hive</span></span><br><span class="line">hiveservices.sh start</span><br><span class="line">bin/beeline -u jdbc:hive2://hadoop100:10000</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Hive-常用交互命令"><a href="#Hive-常用交互命令" class="headerlink" title="Hive 常用交互命令"></a>Hive 常用交互命令</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -e 不进入 hive 的交互窗口执行 sql 语句</span></span><br><span class="line">bin/hive -e <span class="string">&quot;select id from test;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># -f 执行脚本中 sql 语句</span></span><br><span class="line"><span class="comment"># 执行文件中的 sql 语句</span></span><br><span class="line">bin/hive -f ./hivef.sql</span><br><span class="line"><span class="comment"># 执行文件中的 sql 语句并将结果写入文件中</span></span><br><span class="line">bin/hive -f ./hivef.sql &gt; ./hive_result.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># hive 窗口的其它命令</span></span><br><span class="line"><span class="comment"># 退出 hive 窗口</span></span><br><span class="line"><span class="built_in">exit</span>;</span><br><span class="line">quit;</span><br><span class="line"><span class="comment"># 查看 hdfs 文件系统</span></span><br><span class="line">dfs -<span class="built_in">ls</span> /;</span><br><span class="line"><span class="comment"># 查看 hive 中输入的所有历史命令</span></span><br><span class="line"><span class="built_in">cd</span> /root</span><br><span class="line"><span class="built_in">cat</span> .hivehistory</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="10、Hive-常见属性配置"><a href="#10、Hive-常见属性配置" class="headerlink" title="10、Hive 常见属性配置"></a>10、Hive 常见属性配置</h2><ol>
<li><p>Hive 运行日志信息配置</p>
<ul>
<li><p>Hive 的 log 默认存放在 &#x2F;tmp&#x2F;root&#x2F;hive.log 目录下（当前用户名下）</p>
</li>
<li><p>修改 hive 的 log 存放日志到 &#x2F;opt&#x2F;module&#x2F;hive&#x2F;logs</p>
<ol>
<li><p>修改&#x2F;opt&#x2F;module&#x2F;hive&#x2F;conf&#x2F;hive-log4j2.properties.template 文件名称为 hive-log4j2.properties</p>
</li>
<li><p>在 hive-log4j2.properties 文件中修改 log 存放位置</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.log.dir=/opt/module/hive/logs</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
</li>
<li><p>打印当前库和表头</p>
<p>在 hive-site.xml 中加入如下两个配置：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>参数配置方式</p>
<ul>
<li><p>查看当前所有的配置信息</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>参数配置的三种方式</p>
<ol>
<li><p>配置文件方式</p>
<ul>
<li><p>默认配置文件：hive-default.xml</p>
</li>
<li><p>用户自定义的配置文件：hive-site.xml</p>
<p>注意：<font color="orange">用户自定义配置会覆盖默认配置</font>。另外，<strong>Hive 也会读入 Hadoop 的配置</strong>，因为 Hive 是作为 Hadoop 的客户端启动的，Hive 的配置会覆盖 Hadoop 的配置。<code>配置文件的设定对本机启动的所有 Hive 进程都有效</code>。</p>
</li>
</ul>
</li>
<li><p>命令行参数方式</p>
<p>启动 Hive 时，可以在命令行添加 -hiveconf param&#x3D;value 来设定参数</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 仅对本次 hive 有效</span></span><br><span class="line">bin/hive -hiveconf mapred.reduce.tasks=10;</span><br><span class="line"><span class="comment"># 查看参数设置</span></span><br><span class="line"><span class="built_in">set</span> mapred.reduce.tasks;</span><br></pre></td></tr></table></figure>
</li>
<li><p>参数声明方式</p>
<p>可以在 HQL 中使用 SET 关键字设定参数</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 仅对本次 hive 有效</span><br><span class="line"><span class="keyword">set</span> mapred.reduce.tasks<span class="operator">=</span><span class="number">100</span>;</span><br><span class="line"># 查看参数设置</span><br><span class="line"><span class="keyword">set</span> mapred.reduce.tasks;</span><br></pre></td></tr></table></figure></li>
</ol>
<p>配置优先级为：配置文件 &lt; 命令行参数 &lt; 参数声明 （某些系统级的参数，必须用配置文件或者命令行参数设置）</p>
</li>
</ul>
</li>
</ol>
<h2 id="11、Hive-数据类型"><a href="#11、Hive-数据类型" class="headerlink" title="11、Hive 数据类型"></a>11、Hive 数据类型</h2><h3 id="11-1、基本数据类型"><a href="#11-1、基本数据类型" class="headerlink" title="11.1、基本数据类型"></a>11.1、基本数据类型</h3><table>
<thead>
<tr>
<th>Hive 数据类型</th>
<th>java 数据类型</th>
<th>长度</th>
<th>例子</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>byte</td>
<td>1byte 有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>SMALINT</td>
<td>short</td>
<td>2byte 有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>INT</td>
<td>int</td>
<td>4byte 有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BIGINT</td>
<td>long</td>
<td>8byte 有符号整数</td>
<td>20</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>boolean</td>
<td>布尔类型，true 或者 false</td>
<td>TRUE FALSE</td>
</tr>
<tr>
<td>FLOAT</td>
<td>float</td>
<td>单精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>double</td>
<td>双精度浮点数</td>
<td>3.14159</td>
</tr>
<tr>
<td>STRING</td>
<td>string</td>
<td>字符系列。可以指定字符集。可以使用单引号或者双引号。</td>
<td>‘now is the time’ “for all good men”</td>
</tr>
<tr>
<td>TIMESTAMP</td>
<td></td>
<td>时间类型</td>
<td></td>
</tr>
<tr>
<td>BINARY</td>
<td></td>
<td>字节数组</td>
<td></td>
</tr>
</tbody></table>
<p>对于 Hive 的 String 类型相当于数据库的 varchar 类型，该类型是一个可变的字符串，不 过它不能声明其中最多能存储多少个字符，理论上它可以存储 2GB 的字符数。</p>
<h3 id="11-2、集合数据类型"><a href="#11-2、集合数据类型" class="headerlink" title="11.2、集合数据类型"></a>11.2、集合数据类型</h3><table>
<thead>
<tr>
<th>数据类型</th>
<th>描述</th>
<th>语法示例</th>
</tr>
</thead>
<tbody><tr>
<td>STRUCT</td>
<td>和 c 语言中的 struct 类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是 STRUCT{first STRING, last STRING},那么<code>第1个元素可以通过字段.first来引用</code>。</td>
<td>struct() 例如struct&lt;street:string, city:string&gt;</td>
</tr>
<tr>
<td>MAP</td>
<td>MAP 是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是 MAP，其中键-&gt;值对是’first’-&gt;’John’和’last’-&gt;’Doe’，那么<code>可以通过字段名[‘last’]获取最后一个元素</code>。</td>
<td>map() 例如 map&lt;string, int&gt;</td>
</tr>
<tr>
<td>ARRAY</td>
<td>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’, ‘Doe’]，那么<code>第2个元素可以通过数组名[1]进行引用</code>。</td>
<td>Array() 例如 array&lt;string&gt;</td>
</tr>
</tbody></table>
<p>Hive 有三种复杂数据类型 ARRAY、MAP 和 STRUCT。ARRAY 和 MAP 与 Java 中的 Array 和 Map 类似，而 STRUCT 与 C 语言中的 Struct 类似，它封装了一个命名字段集合，复杂数据 类型允许任意层次的嵌套。</p>
<h3 id="11-3、案例"><a href="#11-3、案例" class="headerlink" title="11.3、案例"></a>11.3、案例</h3><ol>
<li><p>假设某表有如下一行，我们用 JSON 格式来表示其数据结构。在 Hive 下访问的格式为：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;songsong&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;friends&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="string">&quot;bingbing&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="string">&quot;lili&quot;</span></span><br><span class="line">    <span class="punctuation">]</span><span class="punctuation">,</span> <span class="comment">//列表 Array, </span></span><br><span class="line">    <span class="attr">&quot;children&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="comment">//键值 Map,</span></span><br><span class="line">        <span class="attr">&quot;xiao song&quot;</span><span class="punctuation">:</span> <span class="number">18</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;xiaoxiao song&quot;</span><span class="punctuation">:</span> <span class="number">19</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span> <span class="comment">//结构 Struct,</span></span><br><span class="line">        <span class="attr">&quot;street&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hui long guan&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;city&quot;</span><span class="punctuation">:</span> <span class="string">&quot;beijing&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>创建本地测试 test.txt 文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing</span><br><span class="line">yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</span><br></pre></td></tr></table></figure>

<p>注意：MAP，STRUCT 和 ARRAY 里的元素间关系都可以用同一个字符表示，这里用“_”。</p>
</li>
<li><p>在 Hive 上创建测试表 test2</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table test2(</span><br><span class="line">name string,</span><br><span class="line">friends array&lt;string&gt;,</span><br><span class="line">children map&lt;string, int&gt;,</span><br><span class="line">address struct&lt;street:string, city:string&gt;</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;,&#x27;</span><br><span class="line">collection items terminated by &#x27;_&#x27;</span><br><span class="line">map keys terminated by &#x27;:&#x27;</span><br><span class="line">lines terminated by &#x27;\n&#x27;;</span><br></pre></td></tr></table></figure>

<p>字段解释： </p>
<ul>
<li>row format delimited fields terminated by ‘,’ – 列分隔符</li>
<li>collection items terminated by ‘_’ –MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</li>
<li>map keys terminated by ‘:’ – MAP 中的 key 与 value 的分隔符</li>
<li>lines terminated by ‘\n’; – 行分隔符</li>
</ul>
</li>
<li><p>导入 test.txt 到 hadoop 中</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put ./test.txt /user/hive/warehouse/test2</span><br></pre></td></tr></table></figure>
</li>
<li><p>访问数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select friends[1],children[&#x27;xiao song&#x27;],address.city from </span><br><span class="line">test2</span><br><span class="line">where name=&quot;songsong&quot;;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="11-4、类型转化"><a href="#11-4、类型转化" class="headerlink" title="11.4、类型转化"></a>11.4、类型转化</h3><ol>
<li>隐式类型转换，规则如下：<ul>
<li>任何整数类型都可以隐式地转换为一个范围更广的类型，如 TINYINT 可以转换成 INT，INT 可以转换成 BIGINT。</li>
<li>所有整数类型、FLOAT 和 STRING 类型都可以隐式地转换成 DOUBLE。</li>
<li>TINYINT、SMALLINT、INT 都可以转换为 FLOAT。</li>
<li>BOOLEAN 类型不可以转换为任何其它的类型。</li>
</ul>
</li>
<li>使用 cast 显示转换<ul>
<li>例如 CAST(‘1’ AS INT) 将把字符串’1’ 转换成整数 1。</li>
<li>如果强制类型转换失败，如执行 CAST(‘X’ AS INT)，表达式返回空值 NULL。</li>
</ul>
</li>
</ol>
<h2 id="12、DDL-数据定义"><a href="#12、DDL-数据定义" class="headerlink" title="12、DDL 数据定义"></a>12、DDL 数据定义</h2><h3 id="12-1、创建数据库"><a href="#12-1、创建数据库" class="headerlink" title="12.1、创建数据库"></a>12.1、创建数据库</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- [里面的内容为可选项]</span></span><br><span class="line"><span class="keyword">CREATE</span> DATABASE [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] database_name</span><br><span class="line">[COMMENT database_comment]</span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[<span class="keyword">WITH</span> DBPROPERTIES (property_name<span class="operator">=</span>property_value, ...)];</span><br><span class="line"><span class="comment">-- 创建的数据库在 HDFS 上的默认存储路径是/user/hive/warehouse/*.db。</span></span><br><span class="line"><span class="comment">-- 指定存放位置</span></span><br><span class="line"><span class="keyword">create</span> database db_hive location <span class="string">&#x27;/db_hive.db&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="12-2、数据库基本命令"><a href="#12-2、数据库基本命令" class="headerlink" title="12.2、数据库基本命令"></a>12.2、数据库基本命令</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 显示数据库</span></span><br><span class="line"><span class="keyword">show</span> databases;</span><br><span class="line"><span class="comment">-- 查询数据库详情</span></span><br><span class="line"><span class="comment">-- 显示数据库信息</span></span><br><span class="line"><span class="keyword">desc</span> database db_hive;</span><br><span class="line"><span class="comment">-- 显示数据库详细信息</span></span><br><span class="line"><span class="keyword">desc</span> database extended db_hive;</span><br><span class="line"><span class="comment">-- 切换当前数据库</span></span><br><span class="line">use db_hive;</span><br><span class="line"><span class="comment">-- 可以使用 ALTER DATABASE 命令为某个数据库的 DBPROPERTIES 设置键-值对属性值，来描述这个数据库的属性信息。</span></span><br><span class="line"><span class="keyword">alter</span> database db_hive <span class="keyword">set</span> dbproperties(<span class="string">&#x27;createtime&#x27;</span><span class="operator">=</span><span class="string">&#x27;20170830&#x27;</span>);</span><br><span class="line"><span class="comment">-- 删除空数据库</span></span><br><span class="line"><span class="keyword">drop</span> database db_hive;</span><br><span class="line"><span class="comment">-- 强制删除数据库</span></span><br><span class="line"><span class="keyword">drop</span> database db_hive cascade;</span><br></pre></td></tr></table></figure>

<h3 id="12-3、建表语句"><a href="#12-3、建表语句" class="headerlink" title="12.3、建表语句"></a>12.3、建表语句</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- hive 有内外表之分，EXTERNAL 创建外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据</span></span><br><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name</span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">[COMMENT table_comment]</span><br><span class="line"><span class="comment">-- PARTITIONED BY 创建分区表</span></span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line"><span class="comment">-- CLUSTERED BY 创建分桶表</span></span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...)</span><br><span class="line"><span class="comment">-- SORTED BY 对桶中的一个或多个列另外排序</span></span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span><span class="operator">|</span><span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS]</span><br><span class="line"><span class="comment">-- 在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive 通过 SerDe 确定表的具体的列的数据</span></span><br><span class="line"><span class="comment">-- SerDe 是 Serialize/Deserilize 的简称， hive 使用 Serde 进行行对象的序列与反序列化</span></span><br><span class="line">[<span class="type">ROW</span> FORMAT row_format]</span><br><span class="line"><span class="comment">-- STORED AS 指定存储文件类型，有：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</span></span><br><span class="line"><span class="comment">-- 纯文本，可以使用 STORED AS TEXTFILE。数据需要压缩，使用 STORED AS SEQUENCEFILE</span></span><br><span class="line">[STORED <span class="keyword">AS</span> file_format]</span><br><span class="line"><span class="comment">-- LOCATION ：指定表在 HDFS 上的存储位置</span></span><br><span class="line">[LOCATION hdfs_path]</span><br><span class="line">[TBLPROPERTIES (property_name<span class="operator">=</span>property_value, ...)]</span><br><span class="line"><span class="comment">-- AS：后跟查询语句，根据查询结果创建表</span></span><br><span class="line">[<span class="keyword">AS</span> select_statement]</span><br></pre></td></tr></table></figure>

<h3 id="12-4、管理表"><a href="#12-4、管理表" class="headerlink" title="12.4、管理表"></a>12.4、管理表</h3><p>默认创建的表是管理表，也被称为内部表。Hive 默认情况下会将这些表的数据存储在由配置项 hive.metastore.warehouse.dir(例如，&#x2F;user&#x2F;hive&#x2F;warehouse) 所定义的目录的子目录下。</p>
<p><font color="orange">删除一个管理表时，Hive 也会删除这个表中数据。</font>管理表不适合和其他工具共享数据。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student(</span><br><span class="line">id <span class="type">int</span>, name string</span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">stored <span class="keyword">as</span> textfile</span><br><span class="line">location <span class="string">&#x27;/user/hive/warehouse/student&#x27;</span>;</span><br></pre></td></tr></table></figure>



<h3 id="12-5、外部表"><a href="#12-5、外部表" class="headerlink" title="12.5、外部表"></a>12.5、外部表</h3><p>Hive 认为其没有完全拥有这份数据，因此删除该表并不会删除掉这份数据，不过会删除掉描述表的元数据信息。</p>
<p>使用场景：每天将收集到的网站日志定期流入 HDFS 文本文件。在外部表（原始日志表）的基础上 做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过 SELECT+INSERT 进入内部表。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> dept(</span><br><span class="line">deptno <span class="type">int</span>,</span><br><span class="line">dname string,</span><br><span class="line">loc <span class="type">int</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br></pre></td></tr></table></figure>

<h3 id="12-6、管理表与外部表的互相转换"><a href="#12-6、管理表与外部表的互相转换" class="headerlink" title="12.6、管理表与外部表的互相转换"></a>12.6、管理表与外部表的互相转换</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 修改内部表为外部表</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span><span class="operator">=</span><span class="string">&#x27;TRUE&#x27;</span>);</span><br><span class="line"><span class="comment">-- 修改外部表为内部表</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> student2 <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span><span class="operator">=</span><span class="string">&#x27;FALSE&#x27;</span>);</span><br><span class="line"><span class="comment">-- 注意区分大小写</span></span><br></pre></td></tr></table></figure>

<h3 id="12-7、修改表"><a href="#12-7、修改表" class="headerlink" title="12.7、修改表"></a>12.7、修改表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 重命名表</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name RENAME <span class="keyword">TO</span> new_table_name</span><br><span class="line"><span class="comment">-- 更新列</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name CHANGE [<span class="keyword">COLUMN</span>] col_old_name col_new_name column_type [COMMENT col_comment] [<span class="keyword">FIRST</span><span class="operator">|</span>AFTER column_name]</span><br><span class="line"><span class="comment">-- 增加和替换列</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">ADD</span><span class="operator">|</span>REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)</span><br><span class="line"><span class="comment">-- 删除表</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> table_name</span><br></pre></td></tr></table></figure>

<h2 id="13、DML-数据操作"><a href="#13、DML-数据操作" class="headerlink" title="13、DML 数据操作"></a>13、DML 数据操作</h2><h3 id="13-1、数据导入"><a href="#13-1、数据导入" class="headerlink" title="13.1、数据导入"></a>13.1、数据导入</h3><ol>
<li><p>向表中装载数据 load</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">load data [<span class="keyword">local</span>] inpath <span class="string">&#x27;data path&#x27;</span> [overwrite] <span class="keyword">into</span> <span class="keyword">table</span> student [<span class="keyword">partition</span> (partcol1<span class="operator">=</span>val1,…)];</span><br><span class="line"><span class="comment">-- load data:表示加载数据</span></span><br><span class="line"><span class="comment">-- local:表示从本地加载数据到 hive 表；否则从 HDFS 加载数据到 hive 表</span></span><br><span class="line"><span class="comment">-- inpath:表示加载数据的路径</span></span><br><span class="line"><span class="comment">-- overwrite:表示覆盖表中已有数据，否则表示追加</span></span><br><span class="line"><span class="comment">-- into table:表示加载到哪张表</span></span><br><span class="line"><span class="comment">-- student:表示具体的表</span></span><br><span class="line"><span class="comment">-- partition:表示上传到指定分区</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>通过查询语句向表中插入数据 insert</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 基本插入数据</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> student <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">&#x27;wangwu&#x27;</span>),(<span class="number">2</span>,<span class="string">&#x27;zhaoliu&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 基本模式插入（根据单张表查询结果）</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">select</span> id, name <span class="keyword">from</span> student_par <span class="keyword">where</span> <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;201709&#x27;</span>;</span><br><span class="line"><span class="comment">-- insert into：以追加数据的方式插入到表或分区，原有数据不会删除</span></span><br><span class="line"><span class="comment">-- insert overwrite：会覆盖表中已存在的数据</span></span><br><span class="line"><span class="comment">-- 注意：insert 不支持插入部分字段</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>多表（多分区）插入模式（根据多张表查询结果）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> student</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;201707&#x27;</span>)</span><br><span class="line"><span class="keyword">select</span> id, name <span class="keyword">where</span> <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;201709&#x27;</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> student <span class="keyword">partition</span>(<span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;201706&#x27;</span>)</span><br><span class="line"><span class="keyword">select</span> id, name <span class="keyword">where</span> <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;201709&#x27;</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询语句中创建表并加载数据 As Select</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student2 <span class="keyword">as</span> <span class="keyword">select</span> id, name <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建表时通过 Location 指定加载数据路径</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dfs -<span class="built_in">mkdir</span> /student;</span><br><span class="line">dfs -put /opt/module/datas/student.txt /student;</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> student5(</span><br><span class="line"> id <span class="type">int</span>, name string</span><br><span class="line"> )</span><br><span class="line"> <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"> location <span class="string">&#x27;/student;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>import 数据到指定 Hive 表中</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 先用 export 导出后，再将数据导入</span></span><br><span class="line">import <span class="keyword">table</span> student2 <span class="keyword">from</span> <span class="string">&#x27;/user/hive/warehouse/export/student&#x27;</span>;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="13-2、数据导出"><a href="#13-2、数据导出" class="headerlink" title="13.2、数据导出"></a>13.2、数据导出</h3><ol>
<li><p>Insert 导出</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/opt/module/hive/data/export/student&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>将查询的结果格式化导出到本地</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/opt/module/hive/data/export/student1&#x27;</span></span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>将查询结果导出到 HDFS 上（没有 local）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite directory <span class="string">&#x27;/user/hqz/student2&#x27;</span></span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span> </span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Hadoop 命令导出到本地</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfs -get /user/hive/warehouse/student/student.txt /opt/module/data/export/student3.txt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Hive Shell 命令导出</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/hive -e <span class="string">&#x27;select * from default.student;&#x27;</span> &gt; /opt/module/hive/data/export/student4.txt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Export 导出到 HDFS 上</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> table db_hive.student to <span class="string">&#x27;/user/hive/warehouse/export/student&#x27;</span>;</span><br><span class="line"><span class="comment"># export 和 import 主要用于两个 Hadoop 平台集群之间 Hive 表迁移。</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>清除表中的数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="14、查询"><a href="#14、查询" class="headerlink" title="14、查询"></a>14、查询</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> <span class="operator">|</span> <span class="keyword">DISTINCT</span>] select_expr, select_expr, ...</span><br><span class="line">	<span class="keyword">FROM</span> table_reference</span><br><span class="line">	[<span class="keyword">WHERE</span> where_condition]</span><br><span class="line">	[<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">	[<span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list]</span><br><span class="line">	[CLUSTER <span class="keyword">BY</span> col_list</span><br><span class="line">		<span class="operator">|</span> [DISTRIBUTE <span class="keyword">BY</span> col_list] [SORT <span class="keyword">BY</span> col_list]</span><br><span class="line">	]</span><br><span class="line">	[LIMIT number]</span><br></pre></td></tr></table></figure>

<h3 id="14-1、基本查询（select…From）"><a href="#14-1、基本查询（select…From）" class="headerlink" title="14.1、基本查询（select…From）"></a>14.1、基本查询（select…From）</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 全表查询</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 选择特定列查询</span></span><br><span class="line"><span class="keyword">select</span> empno, ename <span class="keyword">from</span> emp;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- SQL 语言大小写不敏感</span></span><br><span class="line"><span class="comment">-- SQL 可以写在一行或者多行</span></span><br><span class="line"><span class="comment">-- 关键字不能被缩写也不能分行</span></span><br><span class="line"><span class="comment">-- 各子句一般要分行写</span></span><br><span class="line"><span class="comment">-- 使用缩进提高语句的可读性</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 列别名</span></span><br><span class="line"><span class="keyword">select</span> ename <span class="keyword">AS</span> name, deptno dn <span class="keyword">from</span> emp;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- LIMIT 子句用于限制返回的行数。</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp limit <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- WHERE 子句，将不满足条件的行过滤掉</span></span><br><span class="line"><span class="comment">-- WHERE 子句紧随 FROM 子句</span></span><br><span class="line"><span class="comment">-- where 子句中不能使用字段别名。</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">where</span> sal <span class="operator">&gt;</span> <span class="number">1000</span>;</span><br></pre></td></tr></table></figure>

<h4 id="14-1-1、算数运算符"><a href="#14-1-1、算数运算符" class="headerlink" title="14.1.1、算数运算符"></a>14.1.1、算数运算符</h4><table>
<thead>
<tr>
<th align="center">运算符</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center">A + B</td>
<td align="center">A 加 B 的结果</td>
</tr>
<tr>
<td align="center">A - B</td>
<td align="center">A 减去 B 的结果</td>
</tr>
<tr>
<td align="center">A * B</td>
<td align="center">A 乘以 B 的结果</td>
</tr>
<tr>
<td align="center">A &#x2F; B</td>
<td align="center">A 除以 B 的结果</td>
</tr>
<tr>
<td align="center">A % B</td>
<td align="center">A 除以 B 产生的余数</td>
</tr>
<tr>
<td align="center">A &amp; B</td>
<td align="center">A 和 B 的按位与结果</td>
</tr>
<tr>
<td align="center">A | B</td>
<td align="center">A 和 B 的按位或结果</td>
</tr>
<tr>
<td align="center">A ^ B</td>
<td align="center">A 和 B 的按位异或结果</td>
</tr>
<tr>
<td align="center">~A</td>
<td align="center">A 按位非的结果</td>
</tr>
</tbody></table>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> sal <span class="operator">+</span> <span class="number">1</span> <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure>

<h4 id="14-1-2、常用函数"><a href="#14-1-2、常用函数" class="headerlink" title="14.1.2、常用函数"></a>14.1.2、常用函数</h4><table>
<thead>
<tr>
<th align="center">函数</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center">count()</td>
<td align="center">求总行数</td>
</tr>
<tr>
<td align="center">max(col_name)</td>
<td align="center">求最大值</td>
</tr>
<tr>
<td align="center">min(col_name)</td>
<td align="center">求最小值</td>
</tr>
<tr>
<td align="center">sum(col_name)</td>
<td align="center">求总和</td>
</tr>
<tr>
<td align="center">avg(col_name)</td>
<td align="center">求平均值</td>
</tr>
</tbody></table>
<h4 id="14-1-3、比较运算符（Betweeb-x2F-In-x2F-Is-x2F-Is-Null）"><a href="#14-1-3、比较运算符（Betweeb-x2F-In-x2F-Is-x2F-Is-Null）" class="headerlink" title="14.1.3、比较运算符（Betweeb&#x2F;In&#x2F;Is&#x2F;Is Null）"></a>14.1.3、比较运算符（Betweeb&#x2F;In&#x2F;Is&#x2F;Is Null）</h4><p>谓词操作符，可以用于 <strong>JOIN…ON</strong> 和 <strong>HAVING</strong> 语句中</p>
<table>
<thead>
<tr>
<th align="center">运算符</th>
<th align="center">操作</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center">A &#x3D; B</td>
<td align="center">所有基本类型</td>
<td align="center">如果表达 A 等于表达 B，结果 TRUE ，否则 FALSE</td>
</tr>
<tr>
<td align="center">A &lt;&#x3D;&gt; B</td>
<td align="center">所有基本类型</td>
<td align="center">如果 A 和 B 都为 NULL，则返回 TRUE，如果一边为 NULL， 返回 False</td>
</tr>
<tr>
<td align="center">A !&#x3D; B</td>
<td align="center">所有基本类型</td>
<td align="center">如果 A 不等于表达式 B 表达返回 TRUE ，否则 FALSE</td>
</tr>
<tr>
<td align="center">A &lt;&gt; B</td>
<td align="center">所有基本类型</td>
<td align="center">A 或者 B 为 NULL 则返回 NULL</td>
</tr>
<tr>
<td align="center">A &lt; B</td>
<td align="center">所有基本类型</td>
<td align="center">TRUE，如果表达式 A 小于表达式 B，否则 FALSE</td>
</tr>
<tr>
<td align="center">A &lt;&#x3D; B</td>
<td align="center">所有基本类型</td>
<td align="center">TRUE，如果表达式 A 小于或等于表达式 B，否则 FALSE</td>
</tr>
<tr>
<td align="center">A &gt; B</td>
<td align="center">所有基本类型</td>
<td align="center">TRUE，如果表达式 A 大于表达式 B，否则 FALSE</td>
</tr>
<tr>
<td align="center">A &gt;&#x3D; B</td>
<td align="center">所有基本类型</td>
<td align="center">TRUE，如果表达式 A 大于或等于表达式 B，否则 FALSE</td>
</tr>
<tr>
<td align="center">A [NOT] BETWEEN B AND C</td>
<td align="center">所有基本类型</td>
<td align="center">如果 A，B 或者 C 任一为 NULL，则结果为 NULL；如果 A 的 值大于等于 B 而且小于或等于 C，则结果为 TRUE，反之为 FALSE。 如果使用 NOT 关键字则可达到相反的效果</td>
</tr>
<tr>
<td align="center">A IS NULL</td>
<td align="center">所有类型</td>
<td align="center">TRUE，如果表达式的计算结果为 NULL，否则 FALSE</td>
</tr>
<tr>
<td align="center">A IS NOT NULL</td>
<td align="center">所有类型</td>
<td align="center">FALSE，如果表达式 A 的计算结果为 NULL，否则 TRUE</td>
</tr>
<tr>
<td align="center">IN(num_1,num_2)</td>
<td align="center">所有类型</td>
<td align="center">使用 IN 运算显示列表中的值</td>
</tr>
<tr>
<td align="center">A [NOT] LIKE B</td>
<td align="center">字符串</td>
<td align="center">TRUE，如果字符串模式 A 匹配到 B，否则 FALSE，如果使用 NOT 关键字则可达到相反的效果</td>
</tr>
<tr>
<td align="center">A RLIKE B</td>
<td align="center">字符串</td>
<td align="center">NULL，如果 A 或 B 为 NULL；TRUE，如果 A 任何子字符串匹配 Java 正则表达式 B；否则FALSE</td>
</tr>
<tr>
<td align="center">A REGEXP B</td>
<td align="center">字符串</td>
<td align="center">等同于RLIKE</td>
</tr>
</tbody></table>
<blockquote>
<p>LIKE 和 RLIKE</p>
</blockquote>
<ol>
<li><p>使用 LIKE 运算选择类似的值</p>
</li>
<li><p>选择条件可以包含字符或数字：</p>
<ul>
<li>% 代表零个或多个字符（任意个字符）</li>
<li>_ 代表一个字符</li>
</ul>
</li>
<li><p>RLIKE 子句</p>
<p>RLIKE 子句是 HIVE 中这个功能的一个扩展，其可以通过 <strong>java 的正则表达式</strong>这个更强大的语言来指定匹配条件</p>
</li>
</ol>
<h4 id="14-1-4、逻辑运算符"><a href="#14-1-4、逻辑运算符" class="headerlink" title="14.1.4、逻辑运算符"></a>14.1.4、逻辑运算符</h4><table>
<thead>
<tr>
<th align="center">运算符</th>
<th align="center">操作</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center">A AND B</td>
<td align="center">boolean</td>
<td align="center">TRUE，如果 A 和 B 都是 TRUE，否则 FALSE</td>
</tr>
<tr>
<td align="center">A &amp;&amp; B</td>
<td align="center">boolean</td>
<td align="center">类似于 A AND B</td>
</tr>
<tr>
<td align="center">A OR B</td>
<td align="center">boolean</td>
<td align="center">TRUE，如果 A 或 B 或两者都是 TRUE，否则 FALSE</td>
</tr>
<tr>
<td align="center">A || B</td>
<td align="center">boolean</td>
<td align="center">类似于 A OR B</td>
</tr>
<tr>
<td align="center">NOT A</td>
<td align="center">boolean</td>
<td align="center">TRUE，如果 A 是 FALSE，否则 FALSE</td>
</tr>
<tr>
<td align="center">!A</td>
<td align="center">boolean</td>
<td align="center">类似于 NOT A</td>
</tr>
</tbody></table>
<h3 id="14-2、分组"><a href="#14-2、分组" class="headerlink" title="14.2、分组"></a>14.2、分组</h3><h4 id="14-2-1、Group-By-语句"><a href="#14-2-1、Group-By-语句" class="headerlink" title="14.2.1、Group By 语句"></a>14.2.1、Group By 语句</h4><p>GROUP BY 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 计算每个部门的平均工资</span></span><br><span class="line"><span class="keyword">select</span> t.deptno, <span class="built_in">avg</span>(t.sal) avg_sal <span class="keyword">from</span> emp t <span class="keyword">group</span> <span class="keyword">by</span> t.deptno;</span><br></pre></td></tr></table></figure>

<h4 id="14-2-2、Having-语句"><a href="#14-2-2、Having-语句" class="headerlink" title="14.2.2、Having 语句"></a>14.2.2、Having 语句</h4><blockquote>
<p>having 与 where 不同点</p>
</blockquote>
<ol>
<li>where 后面不能写分组函数，而 having 后面可以使用分组函数。</li>
<li>having 只用于 group by 分组统计语句。</li>
</ol>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查询平均薪水大于 2000 的部门</span></span><br><span class="line"><span class="keyword">select</span> deptno, <span class="built_in">avg</span>(sal) avg_sal <span class="keyword">from</span> emp <span class="keyword">group</span> <span class="keyword">by</span> deptno <span class="keyword">having</span> avg_sal <span class="operator">&gt;</span> <span class="number">2000</span>;</span><br></pre></td></tr></table></figure>

<h4 id="14-2-3、Join-语句"><a href="#14-2-3、Join-语句" class="headerlink" title="14.2.3、Join 语句"></a>14.2.3、Join 语句</h4><ol>
<li><p>等值 Join</p>
<p>Hive 支持通常的 SQL JOIN 语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称</span></span><br><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno, d.dname <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>表的别名</p>
</blockquote>
<ul>
<li>使用别名可以简化查询</li>
<li>使用表名前缀可以提高执行效率</li>
</ul>
</li>
<li><p>内连接</p>
<p>只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>
</li>
<li><p>左外连接</p>
<p>JOIN 操作符左边表中符合 WHERE 子句的所有记录将会被返回。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">left</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>
</li>
<li><p>右外连接</p>
<p>JOIN 操作符右边表中符合 WHERE 子句的所有记录将会被返回。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">right</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>
</li>
<li><p>满外连接</p>
<p>将会返回所有表中符合 WHERE 语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值替代。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> e.empno, e.ename, d.deptno <span class="keyword">from</span> emp e <span class="keyword">full</span> <span class="keyword">join</span> dept d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure>
</li>
<li><p>多表连接</p>
<p>连接 n 个表，至少需要 n-1 个连接条件</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> e.ename, d.dname, l.loc_name</span><br><span class="line">	<span class="keyword">FROM</span> emp e </span><br><span class="line"><span class="keyword">JOIN</span> dept d</span><br><span class="line">	<span class="keyword">ON</span> d.deptno <span class="operator">=</span> e.deptno </span><br><span class="line"><span class="keyword">JOIN</span> location l</span><br><span class="line">	<span class="keyword">ON</span> d.loc <span class="operator">=</span> l.loc;</span><br></pre></td></tr></table></figure>

<ul>
<li>大多数情况下，Hive 会对每对 JOIN 连接对象启动一个 MapReduce 任务</li>
<li>Hive 总是按照从左到右的顺序执行</li>
<li>当 3 个或者更多表进行 join 连接时，如果每个 on 子句都使用相同的连接键的话，那么只会产生一个 MapReduce Job</li>
</ul>
</li>
</ol>
<h4 id="14-2-4、笛卡尔积"><a href="#14-2-4、笛卡尔积" class="headerlink" title="14.2.4、笛卡尔积"></a>14.2.4、笛卡尔积</h4><p>A 表中的每一条数据与 B 表中的每一条数据都能连接上</p>
<p>产生条件：</p>
<ol>
<li>省略连接条件</li>
<li>连接条件无效</li>
<li>所有表中的所有行互相连接</li>
</ol>
<h4 id="14-2-5、排序"><a href="#14-2-5、排序" class="headerlink" title="14.2.5、排序"></a>14.2.5、排序</h4><ol>
<li><p>全局排序（Order By）</p>
<p>Order By：全局排序，只有一个 Reducer</p>
<ul>
<li><p>使用 ORDER BY 子句排序</p>
<p>ASC：升序（默认）</p>
<p>DESC：降序</p>
</li>
<li><p>ORDER BY 子句在 SELECT 语句的结尾</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 按工资降序排序</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> sal <span class="keyword">desc</span>;</span><br><span class="line"><span class="comment">-- 按别名排序</span></span><br><span class="line"><span class="keyword">select</span> ename, sal<span class="operator">*</span><span class="number">2</span> twosal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> twosal;</span><br><span class="line"><span class="comment">-- 多个列排序</span></span><br><span class="line"><span class="keyword">select</span> ename, deptno, sal <span class="keyword">from</span> emp <span class="keyword">order</span> <span class="keyword">by</span> deptno, sal;</span><br></pre></td></tr></table></figure>
</li>
<li><p>每个 Reduce 内部排序（Sort By)</p>
<p>对于大规模的数据集 order by 的效率非常低，在很多情况下不需要全局排序，此时可以使用 sort by</p>
<p>sort by 为每个 reducer 产生一个排序文件，每个 Reducer 内部进行排序，对全局结果集来说不是排序</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 设置 reduce 个数</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces <span class="operator">=</span> <span class="number">3</span>;</span><br><span class="line"><span class="comment">-- 查看设置 reduce 个数</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces;</span><br><span class="line"><span class="comment">-- 根据部门编号降序查看员工信息</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp sort <span class="keyword">by</span> deptno <span class="keyword">desc</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>分区（Distribute By）</p>
<p>在有些情况下，需要控制某个特定行应该到哪个 reducer，通常是为了进行后续的<strong>聚集操作</strong>。distribute by 子句可以做这件事。distribute by 类似 MR 中 partition （自定义分区），进行分区，结合 sort by 使用。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 设置多 reduce 才能看到效果</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">3</span>;</span><br><span class="line"><span class="comment">-- 按照部门编号分区，再按照员工编号降序排序</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/opt/module/data/distribute-result&#x27;</span> </span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp distribute <span class="keyword">by</span> deptno sort <span class="keyword">by</span> empno <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>distribute by 的分区规则是根据分区字段的 hash 码与 reduce 的个数进行模除后，余数相同的分到一个区。</p>
</li>
<li><p>Hive 要求 DISTRIBUTE BY 语句要写在 SORT BY 语句之前。</p>
</li>
</ul>
</li>
<li><p>Cluster By</p>
<p>当 distribute by 和 sorts by 字段相同时，可以使用 cluster by 方式。 </p>
<p>cluster by 除了具有 distribute by 的功能外还兼具 sort by 的功能。但是排序只能是升序 排序，不能指定排序规则为 ASC 或者 DESC。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 这两种写法等价</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp distribute <span class="keyword">by</span> deptno sort <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="15、分区表和分桶表"><a href="#15、分区表和分桶表" class="headerlink" title="15、分区表和分桶表"></a>15、分区表和分桶表</h2><h3 id="15-1、分区表"><a href="#15-1、分区表" class="headerlink" title="15.1、分区表"></a>15.1、分区表</h3><p>分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。<font color="red">Hive 中的分区就是分目录</font>，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率 会提高很多。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">-- 创建分区表</span><br><span class="line">CREATE table dept_partition(</span><br><span class="line">	deptno int,</span><br><span class="line">	dname string,</span><br><span class="line">	loc string</span><br><span class="line">)</span><br><span class="line">partitioned by (day string) -- 创建表之后，day相当于表的一个字段，注意：partitioned</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br><span class="line">show tables;</span><br><span class="line">-- 在 /opt/moudule/data 目录下准备三个文件：dept1.log dept2.log dept3.log</span><br><span class="line">-- dept1.log</span><br><span class="line">-- 10	ACCOUNTING	1700</span><br><span class="line">-- 20	RESEARCH	1800</span><br><span class="line"></span><br><span class="line">-- dept2.log</span><br><span class="line">-- 30	SALES	1900</span><br><span class="line">-- 40	OPERATIONS	1700</span><br><span class="line"></span><br><span class="line">-- dept3.log</span><br><span class="line">-- 50	TEST	2000</span><br><span class="line">-- 60	DEV	1900</span><br><span class="line"></span><br><span class="line">-- 加载数据到分区表中</span><br><span class="line">load data local inpath </span><br><span class="line">&#x27;/opt/module/data/dept1.log&#x27; into table dept_partition </span><br><span class="line">partition(day=&#x27;20220913&#x27;);</span><br><span class="line"></span><br><span class="line">load data local inpath </span><br><span class="line">&#x27;/opt/module/data/dept2.log&#x27; into table dept_partition </span><br><span class="line">partition(day=&#x27;20220914&#x27;);</span><br><span class="line"></span><br><span class="line">load data local inpath </span><br><span class="line">&#x27;/opt/module/data/dept3.log&#x27; into table dept_partition </span><br><span class="line">partition(day=&#x27;20220915&#x27;);</span><br><span class="line"></span><br><span class="line">-- 查询数据</span><br><span class="line">SELECT * from dept_partition; -- 全表扫描</span><br><span class="line">SELECT * FROM db_hive.dept_partition WHERE `day` = &#x27;20220913&#x27;; -- 分区扫描</span><br><span class="line"></span><br><span class="line">-- 增加分区</span><br><span class="line">alter table dept_partition add partition(day=&#x27;20220916&#x27;); -- 增加单个分区</span><br><span class="line">alter table dept_partition add partition(day=&#x27;20220917&#x27;) partition(day=&#x27;20220918&#x27;); -- 增加多个分区</span><br><span class="line">-- 删除分区</span><br><span class="line">alter table dept_partition drop partition (day=&#x27;20220916&#x27;); -- 删除单个分区</span><br><span class="line">alter table dept_partition drop partition (day=&#x27;20220917&#x27;), partition(day=&#x27;20220918&#x27;); -- 删除多个分区，注意：多个分区之间用 , 分割</span><br><span class="line"></span><br><span class="line">-- 查看分区表中有多少分区</span><br><span class="line">show partitions dept_partition;</span><br><span class="line"></span><br><span class="line">-- 查看分区表结构</span><br><span class="line">desc formatted dept_partition;</span><br></pre></td></tr></table></figure>

<h4 id="15-1-1、二级分区"><a href="#15-1-1、二级分区" class="headerlink" title="15.1.1、二级分区"></a>15.1.1、二级分区</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">-- 二级分区</span><br><span class="line">create table dept_partition2(</span><br><span class="line">deptno int, dname string, loc string</span><br><span class="line">)</span><br><span class="line">partitioned by (day string, hour string) -- 可以加多个分区字段</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br><span class="line"></span><br><span class="line">-- 加载数据</span><br><span class="line">load data local inpath </span><br><span class="line">&#x27;/opt/module/data/dept1.log&#x27; into table</span><br><span class="line">dept_partition2 partition(day=&#x27;20220913&#x27;, hour=&#x27;10&#x27;);</span><br><span class="line"></span><br><span class="line">-- 查询数据</span><br><span class="line">select * from dept_partition2 where day=&#x27;20220913&#x27; and hour=&#x27;10&#x27;;</span><br></pre></td></tr></table></figure>

<h4 id="15-1-2、导入数据到分区表"><a href="#15-1-2、导入数据到分区表" class="headerlink" title="15.1.2、导入数据到分区表"></a>15.1.2、导入数据到分区表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">-- 把数据直接上传到分区目录上，让分区表和数据产生关联的三种方式</span><br><span class="line">-- 1.上传数据后修复</span><br><span class="line">-- 创建文件夹</span><br><span class="line">hdfs -mkdir -p /user/hive/warehouse/db_hive.db/dept_partition2/day=20220916;</span><br><span class="line">-- 上传文件</span><br><span class="line">hdfs -put /opt/module/data/dept1.log /user/hive/warehouse/db_hive.db/dept_partition/day=20220916;</span><br><span class="line">-- 因为 hdfs 的操作不会更新元数据，所以查询不到数据，需要执行修复命令</span><br><span class="line">msck repair table dept_partition;</span><br><span class="line">select * from dept_partition where day=&#x27;20220916&#x27;;</span><br><span class="line"></span><br><span class="line">-- 2.上传数据之后添加分区</span><br><span class="line">-- 创建文件夹</span><br><span class="line">hdfs -mkdir -p /user/hive/warehouse/db_hive.db/dept_partition2/day=20220916;</span><br><span class="line">-- 上传文件</span><br><span class="line">hdfs -put /opt/module/data/dept1.log /user/hive/warehouse/db_hive.db/dept_partition/day=20220916;</span><br><span class="line">-- 添加分区</span><br><span class="line">alter table dept_partition add partition(day=&#x27;20220916&#x27;);</span><br><span class="line"></span><br><span class="line">-- 3.创建文件夹之后 load 数据到分区</span><br><span class="line">-- 创建文件夹</span><br><span class="line">hdfs -mkdir -p /user/hive/warehouse/db_hive.db/dept_partition2/day=20220916;</span><br><span class="line">-- 上传数据</span><br><span class="line">load data local inpath &#x27;/opt/module/data/dept1.log&#x27; into table dept_partition partition(day=&#x27;20220916&#x27;);</span><br></pre></td></tr></table></figure>

<h4 id="15-1-3、动态分区"><a href="#15-1-3、动态分区" class="headerlink" title="15.1.3、动态分区"></a>15.1.3、动态分区</h4><p>关系型数据库中，对分区表 Insert 数据时候，数据库自动会根据分区字段的值，将数据 插入到相应的分区中，Hive 中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过， 使用 Hive 的动态分区，需要进行相应的配置。</p>
<p>开启动态分区参数设置：</p>
<ol>
<li><p>开启动态分区功能（默认 true，开启）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.dynamic.partition=true</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置为非严格模式（动态分区的模式，默认 strict，表示必须指定至少一个分区为静态分区，nonstrict 模式表示允许所有的分区字段都可以使用动态分区）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.dynamic.partition.mode=nonstrict</span><br></pre></td></tr></table></figure>
</li>
<li><p>在所有执行 MR 的节点上，最大一共可以创建多少个动态分区，默认 1000</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions=1000</span><br></pre></td></tr></table></figure>
</li>
<li><p>在每个执行 MR 的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即 <code>day 字段有 365 个值，那么该参数就需要设置成大于 365</code>，如果使用默认值 100，则会报错</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.dynamic.partitions.pernode=100</span><br></pre></td></tr></table></figure>
</li>
<li><p>整个 MR Job 中，最大可以创建多少个 HDFS 文件，默认 100000</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.exec.max.created.files=100000</span><br></pre></td></tr></table></figure>
</li>
<li><p>当有空分区生成时，是否抛出异常，一般不需要设置，默认 false</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.error.on.empty.partition=false</span><br></pre></td></tr></table></figure></li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">-- 案例</span><br><span class="line">-- 创建分区表</span><br><span class="line">create table dept_partition_dy(id int, name string) </span><br><span class="line">partitioned by (loc int)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br><span class="line">-- 设置为非严格模式</span><br><span class="line">set hive.exec.dynamic.partition.mode = nonstrict;</span><br><span class="line">-- 插入数据</span><br><span class="line">insert into table dept_partition_dy partition(loc) </span><br><span class="line">select deptno, dname, loc from dept; -- loc 字段自动为动态分区字段</span><br><span class="line">-- 查看目标分区表情况</span><br><span class="line">show partitions dept_partition_dy;</span><br></pre></td></tr></table></figure>

<h3 id="15-2、分桶表"><a href="#15-2、分桶表" class="headerlink" title="15.2、分桶表"></a>15.2、分桶表</h3><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。</p>
<p>分桶是将数据集分解成更容易管理的若干部分的另一个技术。</p>
<p>分区针对的是<code>数据的存储路径</code>；分桶针对的是<code>数据文件</code>。</p>
<ol>
<li><p>创建分桶表</p>
<ul>
<li><p>准备数据，在 &#x2F;opt&#x2F;module&#x2F;data 目录下创建 stu_buck.txt</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1001	ss1</span><br><span class="line">1002	ss2</span><br><span class="line">1003	ss3</span><br><span class="line">1004	ss4</span><br><span class="line">1005	ss5</span><br><span class="line">1006	ss6</span><br><span class="line">1007	ss7</span><br><span class="line">1008	ss8</span><br><span class="line">1009	ss9</span><br><span class="line">1010	ss10</span><br><span class="line">1011	ss11</span><br><span class="line">1012	ss12</span><br><span class="line">1013	ss13</span><br><span class="line">1014	ss14</span><br><span class="line">1015	ss15</span><br><span class="line">1016	ss16</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建分桶表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CREATE table stu_buck(id int, name string)</span><br><span class="line">clustered by (id) -- clustered</span><br><span class="line">INTO 4 buckets -- 分成 4 个桶</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看表结构</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">desc formatted stu_buck;</span><br><span class="line">-- 查看 Num Buckets 参数</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入数据到分桶表中，load 的方式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-- local inpath 有可能找不到数据，可以将数据导入到 hdfs 中，使用 inpath 导入，也可以直接在 hive 的命令行中执行下面的命令</span><br><span class="line">load data local inpath &#x27;/opt/module/data/stu_buck.txt&#x27; into table stu_buck;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 hdfs 中查看分桶表中是否包含 4 个桶</p>
</li>
<li><p>查询分桶数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from stu_buck;</span><br></pre></td></tr></table></figure>
</li>
<li><p>分桶规则</p>
<p>Hive 的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方法决定该条记录存放在哪个桶当中</p>
</li>
</ul>
</li>
<li><p>分桶表注意事项</p>
<ul>
<li><p>reduce 的个数设置为-1,让 Job 自行决定需要用多少个 reduce 或者将 reduce 的个数设置为大于等于分桶表的桶数</p>
</li>
<li><p>从 hdfs 中 load 数据到分桶表中，避免本地文件找不到问题</p>
</li>
<li><p>不要使用本地模式</p>
</li>
</ul>
</li>
<li><p>insert 方式将数据导入分桶表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into table stu_buck select * from student_insert;</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="15-2-1、抽样查询"><a href="#15-2-1、抽样查询" class="headerlink" title="15.2.1、抽样查询"></a>15.2.1、抽样查询</h4><p>对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive 可以通过对表进行抽样来满足这个需求。</p>
<p>语法: TABLESAMPLE(BUCKET x OUT OF y)</p>
<p>查询表 stu_buck 中的数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM stu_buck tablesample(bucket 1 out of 4 on id);</span><br></pre></td></tr></table></figure>

<h2 id="16、函数"><a href="#16、函数" class="headerlink" title="16、函数"></a>16、函数</h2><h3 id="16-1、系统内置函数"><a href="#16-1、系统内置函数" class="headerlink" title="16.1、系统内置函数"></a>16.1、系统内置函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-- 查看系统自带的函数</span><br><span class="line">show functions;</span><br><span class="line">-- 函数分为 三类</span><br><span class="line">-- UDF：一进一出</span><br><span class="line">-- UDAF：多进一出</span><br><span class="line">-- UDTF：一进多出</span><br><span class="line">-- 显示自带的函数的用法</span><br><span class="line">desc function </span><br><span class="line">-- 详细显示自带的函数的用法</span><br><span class="line">desc function extended upper</span><br></pre></td></tr></table></figure>

<h3 id="16-2、常用内置函数"><a href="#16-2、常用内置函数" class="headerlink" title="16.2、常用内置函数"></a>16.2、常用内置函数</h3><h4 id="16-2-1、空字段赋值"><a href="#16-2-1、空字段赋值" class="headerlink" title="16.2.1、空字段赋值"></a>16.2.1、空字段赋值</h4><p><strong>NVL</strong>：给值为 NULL 的数据赋值，它的格式是 NVL( value，default_value)。它的功能是如果 value 为 NULL，则 NVL 函数返回 default_value 的值，否则返回 value 的值，如果两个参数 都为 NULL ，则返回 NULL。</p>
<h4 id="16-2-2、case-when-then-else-end"><a href="#16-2-2、case-when-then-else-end" class="headerlink" title="16.2.2、case when then else end"></a>16.2.2、case when then else end</h4><ol>
<li><p>创建本地 emp_sex.txt，导入数据</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">悟空	A	男</span><br><span class="line">大海	A	男</span><br><span class="line">宋宋	B	男</span><br><span class="line">凤姐	A	女</span><br><span class="line">婷姐	B	女</span><br><span class="line">婷婷	B	女</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建 hive 表并导入数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-- 创建表</span><br><span class="line">create table emp_sex(</span><br><span class="line">	name string, </span><br><span class="line">	dept_id string, </span><br><span class="line">	sex string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">-- 导入数据</span><br><span class="line">load data local inpath &#x27;/opt/module/data/emp_sex.txt&#x27; into table emp_sex;</span><br></pre></td></tr></table></figure>
</li>
<li><p>查询数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-- 需求：求出不同部门男女各多少人</span><br><span class="line">select</span><br><span class="line"> dept_id,</span><br><span class="line"> sum(case sex when &#x27;男&#x27; then 1 else 0 end) male_count, -- 如果用if实现：if(sex=&#x27;男&#x27;,1,0)</span><br><span class="line"> sum(case sex when &#x27;女&#x27; then 1 else 0 end) female_count</span><br><span class="line">from emp_sex</span><br><span class="line">group by dept_id;</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="16-2-3、行转列"><a href="#16-2-3、行转列" class="headerlink" title="16.2.3、行转列"></a>16.2.3、行转列</h4><p>函数说明：</p>
<ul>
<li><p>CONCAT(string A&#x2F;col, string B&#x2F;col…)：返回输入字符串连接后的结果，支持任意个输入字符串;</p>
</li>
<li><p>CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数是剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将 为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间; </p>
<p><font color="orange">注意: CONCAT_WS must be “string or array</font></p>
</li>
<li><p>COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生 Array 类型字段。</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">-- 数据</span><br><span class="line">-- 孙悟空 白羊座 A</span><br><span class="line">-- 大海 射手座 A</span><br><span class="line">-- 曹操 白羊座 B</span><br><span class="line">-- 猪八戒 白羊座 A</span><br><span class="line">-- 天空 射手座 A</span><br><span class="line">-- 刘备 白羊座 B</span><br><span class="line"></span><br><span class="line">create table person_info(</span><br><span class="line">name string, </span><br><span class="line">constellation string, </span><br><span class="line">blood_type string) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &quot;/opt/module/data/person_info.txt&quot; into table </span><br><span class="line">person_info;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SELECT</span><br><span class="line">t1.c_b,</span><br><span class="line">CONCAT_WS(&quot;|&quot;,collect_set(t1.name))</span><br><span class="line">FROM (</span><br><span class="line">SELECT</span><br><span class="line">NAME,</span><br><span class="line">CONCAT_WS(&#x27;,&#x27;,constellation,blood_type) c_b</span><br><span class="line">FROM person_info</span><br><span class="line">)t1</span><br><span class="line">GROUP BY t1.c_b</span><br><span class="line"></span><br><span class="line">-- 结果</span><br><span class="line">-- 射手座,A 大海|天空</span><br><span class="line">-- 白羊座,A 孙悟空|猪八戒</span><br><span class="line">--  白羊座,B 曹操|刘备</span><br></pre></td></tr></table></figure>

<h4 id="16-2-4、列转行"><a href="#16-2-4、列转行" class="headerlink" title="16.2.4、列转行"></a>16.2.4、列转行</h4><p>函数说明：</p>
<ul>
<li><p>EXPLODE(col)：将 hive 一列中复杂的 Array 或者 Map 结构拆分成多行。</p>
</li>
<li><p>LATERAL VIEW</p>
<p>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</p>
<p>解释：用于和 split, explode 等 UDTF 一起使用，它能够将一列数据拆成多行数据，在此 基础上可以对拆分后的数据进行聚合。</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">--  数据</span><br><span class="line">-- 《疑犯追踪》 悬疑,动作,科幻,剧情</span><br><span class="line">-- 《Lie to me》悬疑,警匪,动作,心理,剧情</span><br><span class="line">-- 《战狼 2》 战争,动作,灾难</span><br><span class="line"></span><br><span class="line">-- 建表</span><br><span class="line">create table movie_info(</span><br><span class="line"> movie string,</span><br><span class="line"> category string)</span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &quot;/opt/module/data/movie_info.txt&quot; into table </span><br><span class="line">movie_info;</span><br><span class="line"></span><br><span class="line">-- 查询数据</span><br><span class="line">SELECT</span><br><span class="line">movie,</span><br><span class="line">category_name</span><br><span class="line">FROM</span><br><span class="line">movie_info</span><br><span class="line">lateral VIEW -- 侧写表，如果炸裂出来的数据还需要与原来的表进行关联，就需要侧写表</span><br><span class="line">explode(split(category,&quot;,&quot;)) movie_info_tmp AS category_name;</span><br><span class="line"></span><br><span class="line">-- 结果</span><br><span class="line">-- 《疑犯追踪》 悬疑</span><br><span class="line">-- 《疑犯追踪》 动作</span><br><span class="line">-- 《疑犯追踪》 科幻</span><br><span class="line">-- 《疑犯追踪》 剧情</span><br><span class="line">-- 《Lie to me》 悬疑</span><br><span class="line">-- 《Lie to me》 警匪</span><br><span class="line">-- 《Lie to me》 动作</span><br><span class="line">-- 《Lie to me》 心理</span><br><span class="line">-- 《Lie to me》 剧情</span><br><span class="line">-- 《战狼 2》 战争</span><br><span class="line">-- 《战狼 2》 动作</span><br><span class="line">-- 《战狼 2》 灾难</span><br></pre></td></tr></table></figure>

<h4 id="16-2-5、窗口函数"><a href="#16-2-5、窗口函数" class="headerlink" title="16.2.5、窗口函数"></a>16.2.5、窗口函数</h4><p><code>即需要原表数据又需要聚合后的数据时使用较多</code></p>
<p>能不用窗口函数解决的不要用窗口函数解决，效率相对较低</p>
<p>函数说明：</p>
<ul>
<li><p>OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化。</p>
<ul>
<li><p>写在 OVER 函数里面</p>
<ul>
<li><p>CURRENT ROW：当前行</p>
</li>
<li><p>n PRECEDING：往前 n 行数据</p>
</li>
<li><p>n FOLLOWING：往后 n 行数据</p>
</li>
<li><p>UNBOUNDED：起点，</p>
<p>UNBOUNDED PRECEDING 表示从前面的起点，</p>
<p>UNBOUNDED FOLLOWING 表示到后面的终点</p>
</li>
</ul>
</li>
<li><p>写在 OVER 函数前面</p>
<ul>
<li><p>LAG(col,n,default_val)：往前第 n 行数据</p>
</li>
<li><p>LEAD(col,n,default_val)：往后第 n 行数据</p>
</li>
<li><p>NTILE(n)：把有序窗口的行分发到指定数据的组中，各个组有编号，编号从 1 开始，对于每一行，NTILE 返回此行所属的组的编号。<code>注意：n 必须为 int 类型</code>。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">-- 引入数据</span><br><span class="line">-- jack,2017-01-01,10</span><br><span class="line">-- tony,2017-01-02,15</span><br><span class="line">-- jack,2017-02-03,23</span><br><span class="line">-- tony,2017-01-04,29</span><br><span class="line">-- jack,2017-01-05,46</span><br><span class="line">-- jack,2017-04-06,42</span><br><span class="line">-- tony,2017-01-07,50</span><br><span class="line">-- jack,2017-01-08,55</span><br><span class="line">-- mart,2017-04-08,62</span><br><span class="line">-- mart,2017-04-09,68</span><br><span class="line">-- neil,2017-05-10,12</span><br><span class="line">-- mart,2017-04-11,75</span><br><span class="line">-- neil,2017-06-12,80</span><br><span class="line">-- mart,2017-04-13,94</span><br><span class="line"></span><br><span class="line">-- 创建表</span><br><span class="line">create table business(</span><br><span class="line">name string,</span><br><span class="line">orderdate string,</span><br><span class="line">cost int</span><br><span class="line">) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;,&#x27;;</span><br><span class="line">load data local inpath &quot;/opt/module/data/business.txt&quot; into table </span><br><span class="line">business;</span><br><span class="line"></span><br><span class="line">-- 查询在 2017 年 4 月份购买过的顾客及总人数</span><br><span class="line">select name,count(*) over()</span><br><span class="line">from business</span><br><span class="line">where substring(orderdate,1,7) = &#x27;2017-04&#x27;</span><br><span class="line">group by name; -- 执行出错，不知道原因</span><br><span class="line"></span><br><span class="line">-- 理论结果为：</span><br><span class="line">-- jack 2</span><br><span class="line">-- mart 2</span><br><span class="line"></span><br><span class="line">-- 查询顾客的购买明细及月购买总额</span><br><span class="line">select name,orderdate,cost,sum(cost) over(partition by name,MONTH(orderdate)) </span><br><span class="line">from business;</span><br><span class="line">-- 查询结果</span><br><span class="line">-- jack	2017-01-05	46	111</span><br><span class="line">-- jack	2017-01-08	55	111</span><br><span class="line">-- jack	2017-01-01	10	111</span><br><span class="line">-- jack	2017-02-03	23	23</span><br><span class="line">-- jack	2017-04-06	42	42</span><br><span class="line"></span><br><span class="line">-- 将每个顾客的 cost 按照日期进行累加</span><br><span class="line">select name,orderdate,cost,</span><br><span class="line">sum(cost) over(partition by name ORDER BY orderdate) </span><br><span class="line">-- sum(cost) over(partition by name order by orderdate rows between UNBOUNDED PRECEDING and current row ) as sample4 ,--和 sample3 一样,由起点到当前行的聚合</span><br><span class="line">-- sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING and current row) as sample5, --当前行和前面一行做聚合</span><br><span class="line">-- sum(cost) over(partition by name order by orderdate rows between 1 PRECEDING AND 1 FOLLOWING ) as sample6,--当前行和前边一行及后面一行</span><br><span class="line">-- sum(cost) over(partition by name order by orderdate rows between current row and UNBOUNDED FOLLOWING ) as sample7 --当前行及后面所有行</span><br><span class="line">from business; -- 开窗大小默认从第一行到当前行</span><br><span class="line">-- rows 必须跟在 order by 子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量</span><br><span class="line">-- 查询结果</span><br><span class="line">-- jack	2017-01-01	10	10</span><br><span class="line">-- jack	2017-01-05	46	56</span><br><span class="line">-- jack	2017-01-08	55	111</span><br><span class="line">-- jack	2017-02-03	23	134</span><br><span class="line">-- jack	2017-04-06	42	176</span><br><span class="line"></span><br><span class="line">-- 查看顾客上次的购买时间</span><br><span class="line">select name,orderdate,cost,lag(orderdate,1,&#x27;1970-01-01&#x27;) over(partition by name order by orderdate ) as time1 from business;</span><br><span class="line">-- 查询结果</span><br><span class="line">-- jack	2017-01-01	10	1970-01-01</span><br><span class="line">-- jack	2017-01-05	46	2017-01-01</span><br><span class="line">-- jack	2017-01-08	55	2017-01-05</span><br><span class="line">-- jack	2017-02-03	23	2017-01-08</span><br><span class="line">-- jack	2017-04-06	42	2017-02-03</span><br><span class="line"></span><br><span class="line">-- 查询前20%时间的订单信息</span><br><span class="line">select * from (</span><br><span class="line"> select name,orderdate,cost, ntile(5) over(order by orderdate) sorted</span><br><span class="line"> from business</span><br><span class="line">) t</span><br><span class="line">where sorted = 1; -- ntile 分为 5 组，取第一组</span><br></pre></td></tr></table></figure>

<h4 id="16-2-6、Rank"><a href="#16-2-6、Rank" class="headerlink" title="16.2.6、Rank"></a>16.2.6、Rank</h4><p>函数说明：</p>
<ul>
<li>RANK() 排序相同时会重复，总数不会变 6-6-8</li>
<li>DENSE_RANK() 排序相同时会重复，总数会减少 6-6-7</li>
<li>ROW_NUMBER() 会根据顺序计算 6-7-8</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">-- 数据</span><br><span class="line">--孙悟空 语文 87</span><br><span class="line">--孙悟空 数学 95</span><br><span class="line">--孙悟空 英语 68</span><br><span class="line">--大海 语文 94</span><br><span class="line">--大海 数学 56</span><br><span class="line">--大海 英语 84</span><br><span class="line">--宋宋 语文 64</span><br><span class="line">--宋宋 数学 86</span><br><span class="line">--宋宋 英语 84</span><br><span class="line">--婷婷 语文 65</span><br><span class="line">--婷婷 数学 85</span><br><span class="line">--婷婷 英语 78</span><br><span class="line"></span><br><span class="line">-- 创建表</span><br><span class="line">create table score(</span><br><span class="line">name string,</span><br><span class="line">subject string, </span><br><span class="line">score int) </span><br><span class="line">row format delimited fields terminated by &quot;\t&quot;;</span><br><span class="line">load data local inpath &#x27;/opt/module/data/score.txt&#x27; into table score;</span><br><span class="line"></span><br><span class="line">-- 计算每门学科成绩排名</span><br><span class="line">select name,subject,score,</span><br><span class="line">rank() over(partition by subject order by score desc) rp,</span><br><span class="line">dense_rank() over(partition by subject order by score desc) drp,</span><br><span class="line">row_number() over(partition by subject order by score desc) rmp</span><br><span class="line">from score;</span><br><span class="line"></span><br><span class="line">--孙悟空	数学	95	1	1	1</span><br><span class="line">--宋宋	数学	86	2	2	2</span><br><span class="line">--婷婷	数学	85	3	3	3</span><br><span class="line">--大海	数学	56	4	4	4</span><br><span class="line">--宋宋	英语	84	1	1	1</span><br><span class="line">--大海	英语	84	1	1	2</span><br><span class="line">--婷婷	英语	78	3	2	3</span><br><span class="line">--孙悟空	英语	68	4	3	4</span><br><span class="line">--大海	语文	94	1	1	1</span><br><span class="line">--孙悟空	语文	87	2	2	2</span><br><span class="line">--婷婷	语文	65	3	3	3</span><br><span class="line">--宋宋	语文	64	4	4	4</span><br></pre></td></tr></table></figure>

<h4 id="16-2-7、自定义函数"><a href="#16-2-7、自定义函数" class="headerlink" title="16.2.7、自定义函数"></a>16.2.7、自定义函数</h4><p>自定义函数类别分为以下三种：</p>
<ol>
<li><p>UDF（User-Defined-Function）</p>
<p>一进一出</p>
</li>
<li><p>UDAF（User-Defined Aggregation Function）</p>
<p>聚集函数，多进一出</p>
<p>类似于：count&#x2F;max&#x2F;min</p>
</li>
<li><p>UDTF（User-Defined Table-Generating Functions）</p>
<p>一进多出</p>
<p>如 lateral view explode()</p>
</li>
</ol>
<h2 id="17、压缩和存储"><a href="#17、压缩和存储" class="headerlink" title="17、压缩和存储"></a>17、压缩和存储</h2><h3 id="17-1、开启-Map-输出阶段压缩（MR-引擎）"><a href="#17-1、开启-Map-输出阶段压缩（MR-引擎）" class="headerlink" title="17.1、开启 Map 输出阶段压缩（MR 引擎）"></a>17.1、开启 Map 输出阶段压缩（MR 引擎）</h3><p>开启 map 输出阶段压缩可以减少 job 中 map 和 Reduce task 间数据传输量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-- 开启 hive 中间传输数据压缩功能</span><br><span class="line">set hive.exec.compress.intermediate=true;</span><br><span class="line">-- 开启 mapreduce 中 map 输出压缩功能</span><br><span class="line">set mapreduce.map.output.compress=true;</span><br><span class="line">-- 设置 mapreduce 中 map 输出数据的压缩方式</span><br><span class="line">set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;</span><br></pre></td></tr></table></figure>

<h3 id="17-2、开启-Reduce-输出阶段压缩"><a href="#17-2、开启-Reduce-输出阶段压缩" class="headerlink" title="17.2、开启 Reduce 输出阶段压缩"></a>17.2、开启 Reduce 输出阶段压缩</h3><p>当 Hive 将输出写入到表中时，输出内容同样可以进行压缩。属性 hive.exec.compress.output 控制着这个功能。用户可能需要保持默认设置文件中的默认值false，这样默认的输出就是非压缩的纯文本文件了。用户可以通过在查询语句或执行脚本中设置这个值为 true，来开启输出结果压缩功能。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-- 开启 hive 最终输出数据压缩功能</span><br><span class="line">set hive.exec.compress.output=true;</span><br><span class="line">-- 开启 mapreduce 最终输出数据压缩</span><br><span class="line">set mapreduce.output.fileoutputformat.compress=true;</span><br><span class="line">-- 设置 mapreduce 最终数据输出压缩方式</span><br><span class="line">set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;</span><br><span class="line">-- 设置 mapreduce 最终数据输出压缩为块压缩</span><br><span class="line">set mapreduce.output.fileoutputformat.compress.type=BLOCK;</span><br></pre></td></tr></table></figure>

<h3 id="17-3、文件存储格式"><a href="#17-3、文件存储格式" class="headerlink" title="17.3、文件存储格式"></a>17.3、文件存储格式</h3><p>Hive 支持的存储数据的格式主要有：TEXTFILE、SEQUENCEFILE、ORC、PARQUET</p>
<h4 id="17-3-1、列式存储和行式存储"><a href="#17-3-1、列式存储和行式存储" class="headerlink" title="17.3.1、列式存储和行式存储"></a>17.3.1、列式存储和行式存储</h4><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E8%A1%8C%E5%BC%8F%E5%AD%98%E5%82%A8.png" alt="列式存储与行式存储"></p>
<ol>
<li><p>行存储的特点：</p>
<p>查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。</p>
</li>
<li><p>列存储的特点：</p>
<p>因为每个字段的数据聚集存储，在查询只需要<code>少数几个字段</code>的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计<strong>压缩</strong>算法。</p>
<p>TEXTFILE 和 SEQUENCEFILE 的存储格式都是基于行存储的；</p>
<p>ORC 和 PARQUET 是基于列式存储的。</p>
</li>
</ol>
<h4 id="17-3-2、TextFile-格式"><a href="#17-3-2、TextFile-格式" class="headerlink" title="17.3.2、TextFile 格式"></a>17.3.2、TextFile 格式</h4><p>默认格式，<code>数据不做压缩，磁盘开销大，数据解析开销大</code>。可结合 Gzip、Bzip2 使用，但使用 Gzip 这种方式，hive 不会对数据进行切分，从而无法对数据进行并行操作。</p>
<h4 id="17-3-3、Orc-格式"><a href="#17-3-3、Orc-格式" class="headerlink" title="17.3.3、Orc 格式"></a>17.3.3、Orc 格式</h4><p>如下图所示，每个 Orc 文件由 1 个或多个 stripe 组成，每个 stripe 一般为 HDFS 的块大小，每一个 stripe 包含多条记录，这些记录按照列进行独立存储，对应到 Parquet 中的 row group 的概念。每个 Stripe 里有三部分组成，分别是 Index Data，Row Data，Stripe Footer</p>
<p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/orc%E6%A0%BC%E5%BC%8F.jpg" alt="orc格式"></p>
<ol>
<li><p>Index Data：一个轻量级的 index，默认是每隔 1W 行做一个索引。这里做的索引应该只是记录某行的各字段在 Row Data 中的 offset。</p>
</li>
<li><p>Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个 Stream 来存储。</p>
</li>
<li><p>Stripe Footer：存的是各个 Stream 的类型，长度等信息。 </p>
<p>每个文件有一个 File Footer，这里面存的是每个 Stripe 的行数，每个 Column 的数据类型信息等；每个文件的尾部是一个 PostScript，这里面记录了整个文件的压缩类型以及 FileFooter 的长度信息等。在读取文件时，会 seek 到文件尾部读 PostScript，从里面解析到 File Footer 长度，再读 FileFooter，从里面解析到各个 Stripe 信息，再读各个 Stripe，即从后往前读。</p>
</li>
</ol>
<h4 id="17-3-4、Parquet-格式"><a href="#17-3-4、Parquet-格式" class="headerlink" title="17.3.4、Parquet 格式"></a>17.3.4、Parquet 格式</h4><p>Parquet 文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，<font color="orange">因此 Parquet 格式文件是自解析的。</font></p>
<ol>
<li>行组(Row Group)：每一个行组包含一定的行数，在一个 HDFS 文件中至少存储一个行组，类似于 orc 的 stripe 的概念。</li>
<li>列块(Column Chunk)：在一个行组中每一列保存在一个列块中，行组中的所有列连续的存储在这个行组文件中。一个列块中的值都是相同类型的，不同的列块可能使用不同的算法进行压缩。</li>
<li>页(Page)：每一个列块划分为多个页，一个页是最小的编码的单位，在同一个列块的不同页可能使用不同的编码方式。</li>
</ol>
<p>通常情况下，在存储 Parquet 数据的时候会按照 Block 大小设置行组的大小，由于一般情况下每一个 Mapper 任务处理数据的最小单位是一个 Block，这样可以<font color="orange">把每一个行组由一个 Mapper 任务处理，增大任务执行并行度</font>。</p>
<p>![Parquet 文件的格式.png](<a target="_blank" rel="noopener" href="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/Parquet">https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/Parquet</a> 文件的格式.png)</p>
<p>上图展示了一个 Parquet 文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的 Magic Code，用于校验它是否是一个 Parquet 文件，Footer length 记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的 Schema 信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在 Parquet 中，有三种类型的页：<font color="orange">数据页、字典页和索引页</font>。</p>
<ul>
<li>数据页用于存储当前行组中该列的值</li>
<li>字典页存储该列值的编码字典，每一个列块中最 多包含一个字典页</li>
<li>索引页用来存储当前行组下该列的索引，目前 Parquet 中还不支持索引页。</li>
</ul>
<h4 id="17-3-5、创建压缩表"><a href="#17-3-5、创建压缩表" class="headerlink" title="17.3.5、创建压缩表"></a>17.3.5、创建压缩表</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">create table log_orc_zlib(</span><br><span class="line">track_time string,</span><br><span class="line">url string,</span><br><span class="line">session_id string,</span><br><span class="line">referer string,</span><br><span class="line">ip string,</span><br><span class="line">end_user_id string,</span><br><span class="line">city_id string</span><br><span class="line">)</span><br><span class="line">row format delimited fields terminated by &#x27;\t&#x27;</span><br><span class="line">stored as orc</span><br><span class="line">tblproperties(&quot;orc.compress&quot;=&quot;ZLIB&quot;);</span><br><span class="line">-- 所有关于 ORCFile 的参数都是在 HQL 语句的 TBLPROPERTIES 字段里面出现</span><br><span class="line">-- ，hive 表的数据存储格式一般选择：orc 或 parquet。压缩方式一般选择 snappy，lzo。</span><br></pre></td></tr></table></figure>

<h2 id="18、企业级调优"><a href="#18、企业级调优" class="headerlink" title="18、企业级调优"></a>18、企业级调优</h2><h3 id="18-1、执行计划（Explain）"><a href="#18-1、执行计划（Explain）" class="headerlink" title="18.1、执行计划（Explain）"></a>18.1、执行计划（Explain）</h3><ol>
<li><p>基本语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query</span><br></pre></td></tr></table></figure>
</li>
<li><p>案例</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-- 没有生成 MR 任务</span><br><span class="line">explain select * from emp;</span><br><span class="line">-- 生成 MR 任务</span><br><span class="line">explain select deptno, avg(sal) avg_sal from emp group by deptno;</span><br><span class="line">-- 查看详细执行计划</span><br><span class="line">explain extended select * from emp;</span><br><span class="line">explain extended select deptno, avg(sal) avg_sal from emp group by deptno;</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="18-2、Fetch-抓取"><a href="#18-2、Fetch-抓取" class="headerlink" title="18.2、Fetch 抓取"></a>18.2、Fetch 抓取</h3><p>Fetch 抓取是指，<font color="orange">Hive 中对某些情况的查询可以不必使用 MapReduce 计算</font>。例如：SELECT * FROM employees;在这种情况下，Hive 可以简单地读取 employee 对应的存储目录下的文件，然后输出查询结果到控制台。</p>
<p>在 hive-default.xml.template 文件中 hive.fetch.task.conversion 默认是 more，老版本 hive 默认是 <code>minimal</code>，该属性修改为 more 以后，在全局查找、字段查找、limit 查找等都不走 mapreduce。</p>
<h3 id="18-3、本地模式"><a href="#18-3、本地模式" class="headerlink" title="18.3、本地模式"></a>18.3、本地模式</h3><p>大多数的 Hadoop Job 是需要 Hadoop 提供的完整的可扩展性来处理大数据集的。不过，有时 Hive 的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际 job 的执行时间要多的多。对于大多数这种情况，<font color="orange">Hive 可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短</font>。</p>
<p>用户可以通过设置 hive.exec.mode.local.auto 的值为 true，来让 Hive 在适当的时候自动 启动这个优化。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.mode.local.auto=true; -- 开启本地 mr</span><br><span class="line">-- 设置 local mr 的最大输入数据量，当输入数据量小于这个值时采用 local mr 的方式，默认为 134217728，即 128M</span><br><span class="line">set hive.exec.mode.local.auto.inputbytes.max=50000000;</span><br><span class="line">-- 设置 local mr 的最大输入文件个数，当输入文件个数小于这个值时采用 local mr 的方式，默认为 4</span><br><span class="line">set hive.exec.mode.local.auto.input.files.max=10;</span><br></pre></td></tr></table></figure>

<h3 id="18-4、表的优化"><a href="#18-4、表的优化" class="headerlink" title="18.4、表的优化"></a>18.4、表的优化</h3><h4 id="18-4-1、小表大表-Join（MapJOIN）"><a href="#18-4-1、小表大表-Join（MapJOIN）" class="headerlink" title="18.4.1、小表大表 Join（MapJOIN）"></a>18.4.1、小表大表 Join（MapJOIN）</h4><p>将 key 相对分散，并且数据量小的表放在 join 的左边，可以使用 map join 让小的维度表先进内存。在 map 端完成 join。</p>
<p><font color="orange">实际测试发现：新版的 hive 已经对小表 JOIN 大表和大表 JOIN 小表进行了优化。小表放在左边和右边已经没有区别。</font></p>
<p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20221004210454046.png" alt="MapJoin 工作机制"></p>
<h4 id="18-4-2、大表-Join-大表"><a href="#18-4-2、大表-Join-大表" class="headerlink" title="18.4.2、大表 Join 大表"></a>18.4.2、大表 Join 大表</h4><ol>
<li><p>空 key 过滤</p>
<p>有时 join 超时是因为某些 key 对应的数据太多，而相同 key 对应的数据都会发送到相同的 reducer 上，从而导致内存不够。此时我们应该仔细分析这些异常的 key，很多情况下，这些 key 对应的数据是异常数据，我们需要在 SQL 语句中进行过滤。例如 key 对应的字段为空，操作如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table jointable select n.* from (select * from nullidtable where id is not null) n left join bigtable o on n.id = o.id;</span><br></pre></td></tr></table></figure>
</li>
<li><p>空 key 转换</p>
<p>有时虽然某个 key 为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在 join 的结果中，此时我们可以为表 a 中 key 为空的字段赋一个随机的值，使得数据随机均匀地分到不同的 reducer 上。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table jointable select n.* from nullidtable n full join bigtable o on nvl(n.id,rand()) = o.id;</span><br></pre></td></tr></table></figure>
</li>
<li><p>SMB(Sort Merge Bucket join)</p>
</li>
</ol>
<h4 id="18-4-3、Group-By"><a href="#18-4-3、Group-By" class="headerlink" title="18.4.3、Group By"></a>18.4.3、Group By</h4><p>默认情况下，Map 阶段同一 Key 数据分发给一个 reduce，当一个 key 数据过大时就倾斜了。</p>
<p>并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进行部分聚合，最后在 Reduce 端得出最终结果。</p>
<ol>
<li><p>开启 Map 端聚合参数设置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-- 是否在 Map 端聚合，默认为 True</span><br><span class="line">set hive.map.aggr = true</span><br><span class="line">-- 在 Map 端进行聚合操作的条目数目</span><br><span class="line">set hive.groupby.mapaggr.checkinterval = 100000</span><br><span class="line">-- 有数据倾斜的时候进行负载均衡（默认是 false）</span><br><span class="line">set hive.groupby.skewindata = true</span><br></pre></td></tr></table></figure>

<p><font color="orange">当有数据倾斜的时候进行负载均衡选项设定为 true 时，生成的查询计划会有两个 MR Job</font>。第一个 MR Job 中，Map 的输出结果会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是<font color="orange">相同的 Group By Key 有可能被分发到不同的 Reduce 中</font>，从而达到<strong>负载均衡</strong>的目的；第二 个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证 相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。</p>
<h4 id="Count（Distinct）去重统计"><a href="#Count（Distinct）去重统计" class="headerlink" title="Count（Distinct）去重统计"></a>Count（Distinct）去重统计</h4><p>数据量小的时候无所谓，数据量大的情况下，由于 COUNT DISTINCT 操作需要用一个 Reduce Task 来完成，这一个 Reduce 需要处理的数据量太大，就会导致整个 Job 很难完成， 一般 COUNT DISTINCT 使用先 GROUP BY 再 COUNT 的方式替换,但是<strong>需要注意 group by 造成 的数据倾斜问题</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-- 虽然会多用一个 Job 来完成，但在数据量大的情况下，这个绝对是值得的。</span><br><span class="line">select count(id) from (select id from bigtable group by id) a;</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="18-4-4、笛卡尔积"><a href="#18-4-4、笛卡尔积" class="headerlink" title="18.4.4、笛卡尔积"></a>18.4.4、笛卡尔积</h4><p>尽量避免笛卡尔积，join 的时候不加 on 条件，或者无效的 on 条件，Hive 只能使用 1 个 reducer 来完成笛卡尔积。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select T1.*, T2.* from (select * from table1) T1</span><br><span class="line">join</span><br><span class="line">(select * from table2) T2</span><br><span class="line">on 1=1;</span><br></pre></td></tr></table></figure>

<h4 id="18-4-5、行列过滤"><a href="#18-4-5、行列过滤" class="headerlink" title="18.4.5、行列过滤"></a>18.4.5、行列过滤</h4><p>列处理：在 SELECT 中，只拿需要的列，如果有分区，尽量使用<strong>分区过滤</strong>，少用 SELECT  *。</p>
<p>行处理：在<strong>分区剪裁</strong>中，当使用外关联时，如果将副表的过滤条件写在 <strong>Where</strong> 后面， 那么就会先全表关联，之后再过滤，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">-- 先关联两张表，再用 where 条件过滤</span><br><span class="line">select o.id from bigtable b</span><br><span class="line">join </span><br><span class="line">bigtable o</span><br><span class="line">on o.id = b.id</span><br><span class="line">where o.id &lt;= 10;</span><br><span class="line">-- 通过子查询后，再关联表</span><br><span class="line">select b.id from bigtable b</span><br><span class="line">join</span><br><span class="line">(select id from bigtable where id &lt;= 10) o </span><br><span class="line">on b.id = o.id;</span><br></pre></td></tr></table></figure>

<h4 id="18-4-6、分区"><a href="#18-4-6、分区" class="headerlink" title="18.4.6、分区"></a>18.4.6、分区</h4><h4 id="18-4-7、分桶"><a href="#18-4-7、分桶" class="headerlink" title="18.4.7、分桶"></a>18.4.7、分桶</h4><h3 id="18-5、合理设置-Map-及-Reduce-数"><a href="#18-5、合理设置-Map-及-Reduce-数" class="headerlink" title="18.5、合理设置 Map 及 Reduce 数"></a>18.5、合理设置 Map 及 Reduce 数</h3><ol>
<li><p>通常情况下，作业会通过 input 的目录产生一个或者多个 map 任务。</p>
<p>主要的决定因素有：input 的文件总个数，input 的文件大小，集群设置的文件块大小。</p>
</li>
<li><p>是不是 map 数越多越好？</p>
<p>答案是否定的。如果一个任务有很多小文件（远远小于块大小 128m），则每个小文件 也会被当做一个块，用一个 map 任务来完成，而一个 map 任务启动和初始化的时间远远大 于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 map 数是受限的。</p>
</li>
<li><p>是不是保证每个 map 处理接近 128m 的文件块，就高枕无忧了？</p>
<p>答案也是不一定。</p>
<ul>
<li>比如有一个 127m 的文件，正常会用一个 map 去完成，但这个文件只 有一个或者两个小字段，却有几千万的记录(增加 Map 数)；</li>
<li>如果 map 处理的逻辑比较复杂，用一个 map 任务去做，肯定也比较耗时（增加 Map 数）。</li>
</ul>
</li>
</ol>
<h4 id="18-5-1、复杂文件增加-Map-数"><a href="#18-5-1、复杂文件增加-Map-数" class="headerlink" title="18.5.1、复杂文件增加 Map 数"></a>18.5.1、复杂文件增加 Map 数</h4><p>当 input 的文件都很大，任务逻辑复杂，map 执行非常慢的时候，可以考虑增加 Map 数， 来使得每个 map 处理的数据量减少，从而提高任务的执行效率。</p>
<p>增加 map 的方法为：根据 computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))&#x3D;blocksize&#x3D;128M 公式， 调整 maxSize 最大值。让 maxSize 最大值低于 blocksize 就可以增加 map 的个数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-- 设置最大切片值为 100 个字节</span><br><span class="line">set mapreduce.input.fileinputformat.split.maxsize = 100;</span><br></pre></td></tr></table></figure>

<h4 id="18-5-2、小文件进行合并"><a href="#18-5-2、小文件进行合并" class="headerlink" title="18.5.2、小文件进行合并"></a>18.5.2、小文件进行合并</h4><ol>
<li><p>在 map 执行前合并小文件，减少 map 数：CombineHiveInputFormat 具有对小文件进行合 并的功能（系统默认的格式）。HiveInputFormat 没有对小文件合并功能。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 Map-Reduce 的任务结束时合并小文件的设置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-- 在 map-only 任务结束时合并小文件，默认 true</span><br><span class="line">SET hive.merge.mapfiles = true;</span><br><span class="line">-- 在 map-reduce 任务结束时合并小文件，默认 false</span><br><span class="line">SET hive.merge.mapredfiles = true;</span><br><span class="line">-- 合并文件的大小，默认 256M</span><br><span class="line">SET hive.merge.size.per.task = 268435456;</span><br><span class="line">-- 当输出文件的平均大小小于该值时，启动一个独立的 map-reduce 任务进行文件 merge</span><br><span class="line">SET hive.merge.smallfiles.avgsize = 16777216;</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="18-5-3、合理设置-Reduce-数"><a href="#18-5-3、合理设置-Reduce-数" class="headerlink" title="18.5.3、合理设置 Reduce 数"></a>18.5.3、合理设置 Reduce 数</h4><ol>
<li><p>方法一</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-- 参数1、每个 Reduce 处理的数据量默认是 256MB</span><br><span class="line">set hive.exec.reducers.bytes.per.reducer = 256000000</span><br><span class="line">-- 参数2、每个任务最大的 reduce 数，默认为 1009</span><br><span class="line">set hive.exec.reducers.max = 1009</span><br><span class="line">-- 计算 reducer 个数的公式</span><br><span class="line">N = min(参数2，总输入数据量/参数1)</span><br></pre></td></tr></table></figure>
</li>
<li><p>方法二</p>
<p>在 hadoop 的 mapred-default.xml 文件中修改，设置每个 job 的 Reduce 个数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.job.reduces = 15;</span><br></pre></td></tr></table></figure>
</li>
<li><p>reduce 的个数不是越多越好</p>
<ul>
<li>过多的启动和初始化 reduce 也会消耗时间和资源；</li>
<li>另外，有多少个 reduce，就会有多少个输出文件，如果生成了很多个小文件，那 么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题。</li>
</ul>
<p>在设置 reduce 个数的时候也需要考虑这两个原则：</p>
<ul>
<li>处理大数据量利用合适的 reduce 数；</li>
<li>使单个 reduce 任务处理数据量大小要合适。</li>
</ul>
</li>
</ol>
<h3 id="18-6、并行执行"><a href="#18-6、并行执行" class="headerlink" title="18.6、并行执行"></a>18.6、并行执行</h3><p>Hive 会将一个查询转化成一个或者多个阶段。这样的阶段可以是 MapReduce 阶段、抽样阶段、合并阶段、limit 阶段或者 Hive 执行过程中可能需要的其他阶段。默认情况下， Hive 一次只会执行一个阶段。不过，某个特定的 job 可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以<strong>并行执行</strong>的，这样可能使得整个 job 的执行 时间缩短。如果有更多的阶段可以并行执行，那么 job 可能就越快完成。</p>
<p>通过设置参数 hive.exec.parallel 值为 true，就可以开启并发执行。不过，在共享集群中， 需要注意下，如果 job 中并行阶段增多，那么集群利用率就会增加。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-- 打开任务并行执行</span><br><span class="line">set hive.exec.parallel=true;</span><br><span class="line">-- 同一个 sql 允许最大并行度，默认为 8</span><br><span class="line">set hive.exec.parallel.thread.number=16; </span><br></pre></td></tr></table></figure>

<h3 id="18-7、严格模式"><a href="#18-7、严格模式" class="headerlink" title="18.7、严格模式"></a>18.7、严格模式</h3><p>Hive 可以通过设置防止一些危险操作：</p>
<ol>
<li><p>分区表不使用分区过滤</p>
<p>将 hive.strict.checks.no.partition.filter 设置为 true 时，对于分区表，<font color="orange">除非 where 语句中含有分区字段过滤条件来限制范围，否则不允许执行</font>。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。</p>
</li>
<li><p>使用 order by 没有 limit 过滤</p>
<p>将 hive.strict.checks.orderby.no.limit 设置为 true 时，对于<font color="orange">使用了 order by 语句的查询，要求必须使用 limit 语句</font>。因为 order by 为了执行排序过程会将所有的结果数据分发到同一个 Reducer 中进行处理，强制要求用户增加这个 LIMIT 语句可以防止 Reducer 额外执行很长一段时间。</p>
</li>
<li><p>笛卡尔积</p>
<p>将 hive.strict.checks.cartesian.product 设置为 true 时，<font color="orange">会限制笛卡尔积的查询</font>。对关系型数据库非常了解的用户可能期望在执行 JOIN 查询的时候不使用 ON 语句而是使用 where 语句，这样关系数据库的执行优化器就可以高效地将 WHERE 语句转化成那个 ON 语句。不幸的是，Hive 并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。</p>
<h3 id="JVM-重用"><a href="#JVM-重用" class="headerlink" title="JVM 重用"></a>JVM 重用</h3><h3 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h3></li>
</ol>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://hqzqaq.github.io/2022/10/06/hive/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hive/" rel="tag">hive</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/sql/" rel="tag">sql</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2022/10/07/vscode-LaTeX%E8%AE%BA%E6%96%87%E6%8E%92%E7%89%88/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            vscode+LaTeX论文排版
          
        </div>
      </a>
    
    
      <a href="/2022/09/23/spring-security/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">spring security</div>
      </a>
    
  </nav>

  
   
  
   
    <script src="https://cdn.staticfile.org/twikoo/1.4.18/twikoo.all.min.js"></script>
    <div id="twikoo" class="twikoo"></div>
    <script>
        twikoo.init({
            envId: ""
        })
    </script>
 
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2022-2023
        <i class="ri-heart-fill heart_icon"></i> hqz
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="hqz的博客"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/java/">java</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/machineLearning/">机器学习</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/webGIS/">webGIS</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/bigData/">大数据</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/gis/">GIS</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/academic/">学术</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/other/">other</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2019/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js"></script>
<script src="https://cdn.staticfile.org/mathjax/2.7.7/config/TeX-AMS-MML_HTMLorMML-full.js"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->

<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->
 
<script src="/js/clickBoom2.js"></script>
 
<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->
 
<script src="/js/dz.js"></script>
 
<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>