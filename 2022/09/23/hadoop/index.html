<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>hadoop |  hqz的博客</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <script src="https://cdn.staticfile.org/mermaid/8.14.0/mermaid.min.js"></script>
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
      <canvas width="1777" height="841"
        style="position: fixed; left: 0px; top: 0px; z-index: 99999; pointer-events: none;"></canvas>
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-hadoop"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  hadoop
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/09/23/hadoop/" class="article-date">
  <time datetime="2022-09-23T05:35:17.000Z" itemprop="datePublished">2022-09-23</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/bigData/">bigData</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">15.7k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">67 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><h2 id="1、大数据"><a href="#1、大数据" class="headerlink" title="1、大数据"></a>1、大数据</h2><p>指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。</p>
<p>大数据主要解决<font color="red">海量</font>数据的<font color="red">采集</font>、<font color="red">存储</font>和<font color="red">分析</font>计算问题。</p>
<span id="more"></span>

<h2 id="2、大数据特点（4V）"><a href="#2、大数据特点（4V）" class="headerlink" title="2、大数据特点（4V）"></a>2、大数据特点（4V）</h2><ol>
<li>Volume（大量）</li>
<li>Velocity（高速）</li>
<li>Variety（多样）</li>
<li>Value（低价值密度）</li>
</ol>
<h2 id="3、大数据部门组织架构"><a href="#3、大数据部门组织架构" class="headerlink" title="3、大数据部门组织架构"></a>3、大数据部门组织架构</h2><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220613150547638.png" alt="大数据部门组织架构"></p>
<h1 id="Hadoop入门"><a href="#Hadoop入门" class="headerlink" title="Hadoop入门"></a>Hadoop入门</h1><h2 id="4、Hadoop是什么"><a href="#4、Hadoop是什么" class="headerlink" title="4、Hadoop是什么"></a>4、Hadoop是什么</h2><ol>
<li>Hadoop 是一个由 Apache 基金会所开发的分布式系统基础架构。</li>
<li>主要解决海量数据的存储和海量数据的分析计算问题。</li>
</ol>
<h2 id="5、Hadoop的优势"><a href="#5、Hadoop的优势" class="headerlink" title="5、Hadoop的优势"></a>5、Hadoop的优势</h2><ol>
<li>高可靠性：hadoop 底层维护多个数据副本，所以即使 Hadoop 某个计算元素或存储出现故障，也不会导致数据的丢失。</li>
<li>高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。</li>
<li>高效性：在 MapReduce 的思想下，Hadoop 是并行工作的，以加快任务处理速度。</li>
<li>高容错性：能够自动将失败的任务重新分配。</li>
</ol>
<h1 id="HDFS-简介"><a href="#HDFS-简介" class="headerlink" title="HDFS 简介"></a>HDFS 简介</h1><p>Hadoop Distributed File System 简称 HDFS，是一个分布式文件系统。</p>
<ol>
<li>NameNode(nn)：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的 DataNode 等。</li>
<li>DataNode(dn)：在本地文件系统存储文件块数据，以及块数据的校验和。</li>
<li>Secondary NameNode(2nn)：每隔一段时间对 NameNode 元数据备份。</li>
</ol>
<h1 id="YARN-简介"><a href="#YARN-简介" class="headerlink" title="YARN 简介"></a>YARN 简介</h1><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/yarn.png" alt="yarn"></p>
<h1 id="MapReduce-简介"><a href="#MapReduce-简介" class="headerlink" title="MapReduce 简介"></a>MapReduce 简介</h1><p>MapReduce 将计算过程分为两个阶段：Map 和 Reduce</p>
<ol>
<li>Map 阶段并行处理输入数据</li>
<li>Reduce 阶段对 Map 结果进行汇总</li>
</ol>
<p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220614212644361.png" alt="image-20220614212644361"></p>
<h1 id="三者关系"><a href="#三者关系" class="headerlink" title="三者关系"></a>三者关系</h1><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/hdfs-yarn-mapreduce%E4%B8%89%E8%80%85%E5%85%B3%E7%B3%BB.png" alt="hdfs-yarn-mapreduce三者关系"></p>
<h1 id="大数据技术生态体系"><a href="#大数据技术生态体系" class="headerlink" title="大数据技术生态体系"></a>大数据技术生态体系</h1><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220614213906591.png" alt="image-20220614213906591"></p>
<h1 id="Hadoop集群搭建"><a href="#Hadoop集群搭建" class="headerlink" title="Hadoop集群搭建"></a>Hadoop集群搭建</h1><h2 id="6、虚拟机环境准备"><a href="#6、虚拟机环境准备" class="headerlink" title="6、虚拟机环境准备"></a>6、虚拟机环境准备</h2><p>IP地址 192.168.174.100、主机名称 hadoop100、内存4G、硬盘50G，linux系统采用centos 7.5。</p>
<ol>
<li><p>按照 epel-release</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y epel-release</span><br></pre></td></tr></table></figure>
</li>
<li><p>按照 net-tool 工具包和 vim</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y net-tools</span><br><span class="line">yum install -y vim</span><br></pre></td></tr></table></figure>
</li>
<li><p>关闭防火墙及其开机自启</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld.service</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建 hqz 用户，并修改 hqz 用户的密码</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">useradd hqz</span><br><span class="line">passwd hqz</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置 hqz 用户具有 root 权限，方便后期加 sudo 执行 root权限的命令</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sudoers</span><br><span class="line"><span class="comment"># 在 %wheel 这行下面添加一行</span></span><br><span class="line">hqz		ALL=(ALL)	NOPASSWD:ALL</span><br></pre></td></tr></table></figure>
</li>
<li><p>在&#x2F;opt目录下创建文件夹，并修改所属主和所属组</p>
<ul>
<li><p>在&#x2F;opt目录下创建module、software文件夹</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> /opt/module</span><br><span class="line"><span class="built_in">mkdir</span> /opt/software</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改module、software文件夹的所有者和所属组均为hqz用户</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chown</span> hqz:root /opt/module</span><br><span class="line"><span class="built_in">chown</span> hqz:root /opt/software</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看module、software文件夹的所有者和所属组</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/</span><br><span class="line">ll</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h2 id="7、克隆虚拟机"><a href="#7、克隆虚拟机" class="headerlink" title="7、克隆虚拟机"></a>7、克隆虚拟机</h2><ol>
<li><p>利用 hadoop100，克隆两台虚拟机：hadoop101、hadoop102</p>
</li>
<li><p>修改克隆机 IP，以 hadoop100 为例</p>
<ul>
<li><p>修改静态ip</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br><span class="line"></span><br><span class="line">BOOTPROTO=static</span><br><span class="line">NAME=<span class="string">&quot;ens33&quot;</span></span><br><span class="line">IPADDR=192.168.174.102</span><br><span class="line">PREFIX=24</span><br><span class="line">GATEWAY=192.168.174.2</span><br><span class="line">DNS1=192.168.174.2</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看 Linux 虚拟机的虚拟网络编辑器，编辑 -&gt; 虚拟网络编辑器 -&gt; VMnet8，设置子网 ip 为 192.168.174.0 和网关为 192.168.174.2</p>
</li>
<li><p>查看Windows系统适配器VMware Network Adapter VMnet8 的 IP 地址 192.168.174.15 网关和 DNS 都为 192.168.174.2</p>
</li>
</ul>
</li>
<li><p>修改主机名</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hostname</span><br><span class="line"><span class="comment"># 添加</span></span><br><span class="line">hadoop100</span><br></pre></td></tr></table></figure>


</li>
<li><p>配置主机名称映射</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br><span class="line"><span class="comment"># 添加</span></span><br><span class="line">192.168.174.100 hadoop100</span><br><span class="line">192.168.174.101 hadoop101</span><br><span class="line">192.168.174.102 hadoop102</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改 windows 的 host 文件 C:\Windows\System32\drivers\etc</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.174.100 hadoop100</span><br><span class="line">192.168.174.101 hadoop101</span><br><span class="line">192.168.174.102 hadoop102</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="8、安装-jdk"><a href="#8、安装-jdk" class="headerlink" title="8、安装 jdk"></a>8、安装 jdk</h2><ol>
<li><p>将 jdk8 的安装包传入 &#x2F;opt&#x2F;software&#x2F; （我下载的</p>
</li>
<li><p>解压 jdk 安装包到 &#x2F;opt&#x2F;module&#x2F;</p>
</li>
<li><p>配置 jdk 环境变量</p>
<ul>
<li><p>新建 &#x2F;etc&#x2F;profile.d&#x2F;my_env.sh 文件</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile.d/my_env.sh</span><br><span class="line"><span class="comment"># 添加以下内容</span></span><br><span class="line"><span class="comment">#JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br></pre></td></tr></table></figure>
</li>
<li><p>刷新环境变量</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h2 id="9、安装-Hadoop"><a href="#9、安装-Hadoop" class="headerlink" title="9、安装 Hadoop"></a>9、安装 Hadoop</h2><ol>
<li><p>将在官网上下载的 Hadoop 安装包拷贝到 &#x2F;opt&#x2F;software&#x2F; 目录</p>
</li>
<li><p>将 Hadoop 安装包解压到 &#x2F;opt&#x2F;module&#x2F;</p>
</li>
<li><p>将 Hadoop 添加到环境变量</p>
<ul>
<li><p>打开 &#x2F;etc&#x2F;profile.d&#x2F;my_env.sh 添加以下内容</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#HADOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/module/hadoop-3.3.3</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure>
</li>
<li><p>source &#x2F;etc&#x2F;profile 刷新环境变量</p>
</li>
</ul>
</li>
</ol>
<h2 id="10、编写集群分发脚本-xsync"><a href="#10、编写集群分发脚本-xsync" class="headerlink" title="10、编写集群分发脚本 xsync"></a>10、编写集群分发脚本 xsync</h2><ol>
<li><p>安装 rsync</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y rsync</span><br></pre></td></tr></table></figure>
</li>
<li><p>xsync 集群分发脚本</p>
<ul>
<li><p>打开 &#x2F;etc&#x2F;profile.d&#x2F;my_env.sh，添加一下内容</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># xsync</span></span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/home/hqz/bin</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 &#x2F;home&#x2F;hqz&#x2F;bin 目录下创建 xsync 文件，在文件中添加一下内容</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">#1. 判断参数个数</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -lt 1 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> Not Enough Arguement!</span><br><span class="line">    <span class="built_in">exit</span>;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="comment">#2. 遍历集群所有机器，在 hadoop101 上遍历的机器改为 hadoop100 hadoop102 </span></span><br><span class="line"><span class="keyword">for</span> host <span class="keyword">in</span> hadoop101 hadoop102</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> ====================  <span class="variable">$host</span>  ====================</span><br><span class="line">    <span class="comment">#3. 遍历所有目录，挨个发送</span></span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> <span class="variable">$@</span></span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">        <span class="comment">#4. 判断文件是否存在</span></span><br><span class="line">        <span class="keyword">if</span> [ -e <span class="variable">$file</span> ]</span><br><span class="line">            <span class="keyword">then</span></span><br><span class="line">                <span class="comment">#5. 获取父目录</span></span><br><span class="line">                pdir=$(<span class="built_in">cd</span> -P $(<span class="built_in">dirname</span> <span class="variable">$file</span>); <span class="built_in">pwd</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment">#6. 获取当前文件的名称</span></span><br><span class="line">                fname=$(<span class="built_in">basename</span> <span class="variable">$file</span>)</span><br><span class="line">                ssh <span class="variable">$host</span> <span class="string">&quot;mkdir -p <span class="variable">$pdir</span>&quot;</span></span><br><span class="line">                rsync -av <span class="variable">$pdir</span>/<span class="variable">$fname</span> <span class="variable">$host</span>:<span class="variable">$pdir</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="variable">$file</span> does not exists!</span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 vim 模式下执行以下命令</span></span><br><span class="line">:<span class="built_in">set</span> ff</span><br><span class="line">:<span class="built_in">set</span> fileformat=unix</span><br><span class="line">:wq</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改脚本 xsync 具有执行权限</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +x xsync</span><br></pre></td></tr></table></figure>
</li>
<li><p>同步环境变量配置</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ol>
<h2 id="11、SSH-无密码登录配置"><a href="#11、SSH-无密码登录配置" class="headerlink" title="11、SSH 无密码登录配置"></a>11、SSH 无密码登录配置</h2><ol>
<li><p>生成公钥和私钥</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>

<p>在 &#x2F;root&#x2F;.ssh 目录下可以找到 id_rsa 和 id_rsa.pub 两个文件</p>
</li>
<li><p>切换到 &#x2F;root&#x2F;.ssh 目录，将公钥拷贝到要免密登录的目标机器上</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id hadoop101</span><br><span class="line">ssh-copy-id hadoop102</span><br></pre></td></tr></table></figure>

<p>其它的机器做类似的操作即可</p>
</li>
</ol>
<h2 id="12、集群配置"><a href="#12、集群配置" class="headerlink" title="12、集群配置"></a>12、集群配置</h2><h3 id="12-1、集群规划部署"><a href="#12-1、集群规划部署" class="headerlink" title="12.1、集群规划部署"></a>12.1、集群规划部署</h3><p><font color="orange">注意：</font></p>
<ul>
<li>NameNode 和 SecondaryNameNode 不要安装在同一个服务器</li>
<li>ResourceManager 也很消耗内存，不要和 NameNode、SecondaryNameNode 配置在同一台机器上</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>hadoop100</th>
<th>hadoop101</th>
<th>hadoop102</th>
</tr>
</thead>
<tbody><tr>
<td>HDFS</td>
<td>NameNode<br>DataNode</td>
<td>DataNode</td>
<td>SecondaryNameNode<br>DataNode</td>
</tr>
<tr>
<td>YARN</td>
<td>NodeManager</td>
<td>ResourceManager<br>NodeManager</td>
<td>NodeManager</td>
</tr>
</tbody></table>
<h3 id="12-2、配置文件说明"><a href="#12-2、配置文件说明" class="headerlink" title="12.2、配置文件说明"></a>12.2、配置文件说明</h3><p>Hadoop 配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。自定义配置文件：</p>
<p><code>core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml</code> 四个配置文件存放在 <code>$HADOOP_HOME/etc/hadoop</code> 这个路径上，用户可以根据项目需求重新进行修改配置。</p>
<h3 id="12-3、配置集群"><a href="#12-3、配置集群" class="headerlink" title="12.3、配置集群"></a>12.3、配置集群</h3><ul>
<li><p>核心配置文件 core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定NameNode的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop100:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定hadoop数据的存储目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.3.3/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置HDFS网页登录使用的静态用户为root --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>HDFS 配置文件 hdfs-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- nn web端访问地址--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop100:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 2nn web端访问地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>YARN 配置文件 yarn-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定MR走shuffle --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定ResourceManager的地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop101<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 环境变量的继承 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 客户端通过该地址向RM提交对应用程序操作 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop100:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--ResourceManager 对Applicationhadoop100暴露的访问地址。Applicationhadoop100通过该地址向RM申请资源、释放资源等。 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop100:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- RM HTTP访问地址,查看集群信息--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop100:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- NodeManager通过该地址交换信息 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop100:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--管理员通过该地址向RM发送管理命令 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.admin.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop100:8033<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.admin.address.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop100:23142<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.admin.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:8033<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.admin.address.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:23142<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>MapReduce配置文件 mapred-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>配置 workers </p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /opt/module/hadoop-3.3.3/etc/hadoop/workers</span><br><span class="line"><span class="comment"># 添加以下内容</span></span><br><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure>


</li>
<li><p>在集群上分发配置好的 Hadoop 配置文件</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync /opt/module/hadoop-3.3.3/etc/hadoop/</span><br></pre></td></tr></table></figure>
</li>
<li><p><font color="orange">如果是 root 用户配置 hadoop</font></p>
<ul>
<li><p>在 start-dfs.sh，stop-dfs.sh 两个文件顶部添加以下参数</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HDFS_DATANODE_USER=root</span><br><span class="line">HADOOP_SECURE_DN_USER=hdfs</span><br><span class="line">HDFS_NAMENODE_USER=root</span><br><span class="line">HDFS_SECONDARYNAMENODE_USER=root</span><br></pre></td></tr></table></figure>


</li>
<li><p>在 start-yarn.sh，stop-yarn.sh 顶部添加以下参数</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">HADOOP_SECURE_DN_USER=yarn</span><br><span class="line">YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行 xsync .&#x2F;sbin 同步文件内容到其它机器</p>
</li>
</ul>
</li>
</ul>
<h3 id="12-4、启动集群"><a href="#12-4、启动集群" class="headerlink" title="12.4、启动集群"></a>12.4、启动集群</h3><ul>
<li><p>如果是第一次启动需要在 hadoop100 格式化 NameNode</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果集群在启动过程中报错，在格式化 NameNode 之前需要先停下 NameNode 和 DataNode 进程，并删除所有机器的 data 和 logs 目录，然后再进行格式化</p>
</li>
<li><p>启动 HDFS</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>在配置了 ResourceManager 的节点（hadoop101）启动 YARN</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>Web端查看 HDFS 的 NameNode</p>
<ul>
<li>在浏览器输入：<a href="http://hadoop100:9870（在修改了">http://hadoop100:9870（在修改了</a> windows 上的 host 文件的情况下，否则就把 hadoop100 替换为 ip）</li>
<li>查看 HDFS 上的存储的数据信息</li>
</ul>
</li>
<li><p>Web端查看 YARN 的 ResourceManager</p>
<ul>
<li>浏览器中输入：<a target="_blank" rel="noopener" href="http://hadoop101:8088/">http://hadoop101:8088</a></li>
<li>查看 YARN 上运行的 Job 信息</li>
</ul>
</li>
</ul>
<h3 id="12-5、集群测试"><a href="#12-5、集群测试" class="headerlink" title="12.5、集群测试"></a>12.5、集群测试</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上传文件到集群</span></span><br><span class="line"><span class="comment"># 先创建集群的文件夹</span></span><br><span class="line">hadoop fs -<span class="built_in">mkdir</span> /input</span><br><span class="line"><span class="comment"># 上传小文件</span></span><br><span class="line">hadoop fs ./README.txt /input</span><br><span class="line"><span class="comment"># 上传大文件</span></span><br><span class="line">hadoop fs -put /opt/software/jdk-8u202-linux-x64.tar.gz /input</span><br><span class="line"><span class="comment"># 可以在 HDFS 的 web 面板中看到数据</span></span><br><span class="line"><span class="comment"># 下载</span></span><br><span class="line">hadoop fs -get /jdk-8u202-linux-x64.tar.gz ./</span><br><span class="line"><span class="comment"># 执行 wordcount 任务</span></span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.3.jar wordcount /input /output</span><br></pre></td></tr></table></figure>



<h3 id="12-6、配置历史服务器"><a href="#12-6、配置历史服务器" class="headerlink" title="12.6、配置历史服务器"></a>12.6、配置历史服务器</h3><p>为了查看程序的历史运行情况，需要配置一下历史服务器。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hadoop/mapred-site.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 历史服务器地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop100:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分发配置</span></span><br><span class="line">xsync etc/hadoop/mapred-site.xml</span><br><span class="line"><span class="comment"># 启动历史服务器</span></span><br><span class="line">bin/mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>



<h3 id="12-7、配置日志的聚集"><a href="#12-7、配置日志的聚集" class="headerlink" title="12.7、配置日志的聚集"></a>12.7、配置日志的聚集</h3><p>应用运行完成之后，将程序运行日志信息上传到 HDFS 系统上。</p>
<p>好处：可以方便的查看到程序运行详情，方便开发调试。</p>
<p>注意：开启日志聚集功能，需要重新启动 NodeManager、ResourceManager 和 HistoryServer</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hadoop/</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">&lt;!-- 开启日志聚集功能 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 设置日志聚集服务器地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://hadoop100:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 设置日志保留的时间为7天 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分发配置</span></span><br><span class="line">xsync etc/hadoop/yarn-site.xml</span><br><span class="line"><span class="comment"># 关闭 NodeManager、ResourceManager、HistoryServer hadoop101</span></span><br><span class="line">sbin/stop-yarn.sh</span><br><span class="line"><span class="comment"># hadoop100</span></span><br><span class="line">bin/stop-dfs.sh</span><br><span class="line">bin/mapred --daemon stop historyserver</span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop100</span></span><br><span class="line">bin/start-dfs.sh</span><br><span class="line">bin/mapred --daemon start historyserver</span><br><span class="line"><span class="comment">#启动 NodeManager、ResourceManager、HistoryServer hadoop101</span></span><br><span class="line">sbin/start-yarn.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 HDFS 上已经存在的输出文件</span></span><br><span class="line">hadoop fs -<span class="built_in">rm</span> -r /optput</span><br><span class="line"><span class="comment"># 执行 wordCount 程序</span></span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.3.jar wordcount /input /output</span><br></pre></td></tr></table></figure>



<h3 id="12-8、编写集群启停脚本"><a href="#12-8、编写集群启停脚本" class="headerlink" title="12.8、编写集群启停脚本"></a>12.8、编写集群启停脚本</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /home/hqz/bin</span><br><span class="line">vim myhadoop.sh</span><br><span class="line"><span class="comment"># 脚本内容</span></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -lt 1 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;No Args Input...&quot;</span></span><br><span class="line">    <span class="built_in">exit</span> ;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">&quot;start&quot;</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; =================== 启动 hadoop集群 ===================&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------------- 启动 hdfs ---------------&quot;</span></span><br><span class="line">        ssh hadoop100 <span class="string">&quot;/opt/module/hadoop-3.3.3/sbin/start-dfs.sh&quot;</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------------- 启动 yarn ---------------&quot;</span></span><br><span class="line">        ssh hadoop101 <span class="string">&quot;/opt/module/hadoop-3.3.3/sbin/start-yarn.sh&quot;</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------------- 启动 historyserver ---------------&quot;</span></span><br><span class="line">        ssh hadoop100 <span class="string">&quot;/opt/module/hadoop-3.3.3/bin/mapred --daemon start historyserver&quot;</span></span><br><span class="line">;;</span><br><span class="line"><span class="string">&quot;stop&quot;</span>)</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; =================== 关闭 hadoop集群 ===================&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------------- 关闭 historyserver ---------------&quot;</span></span><br><span class="line">        ssh hadoop100 <span class="string">&quot;/opt/module/hadoop-3.3.3/bin/mapred --daemon stop historyserver&quot;</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------------- 关闭 yarn ---------------&quot;</span></span><br><span class="line">        ssh hadoop101 <span class="string">&quot;/opt/module/hadoop-3.3.3/sbin/stop-yarn.sh&quot;</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot; --------------- 关闭 hdfs ---------------&quot;</span></span><br><span class="line">        ssh hadoop100 <span class="string">&quot;/opt/module/hadoop-3.3.3/sbin/stop-dfs.sh&quot;</span></span><br><span class="line">;;</span><br><span class="line">*)</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Input Args Error...&quot;</span></span><br><span class="line">;;</span><br><span class="line"><span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存后退出，然后赋予脚本执行权限</span></span><br><span class="line"><span class="built_in">chmod</span> +x myhadoop.sh</span><br><span class="line"><span class="comment"># 启动、停止</span></span><br><span class="line">myhadoop stop</span><br><span class="line">myhadoop start</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同步文件</span></span><br><span class="line">xsync /home/hqz/bin</span><br></pre></td></tr></table></figure>

<h3 id="12-9、编写查询集群运行情况的脚本"><a href="#12-9、编写查询集群运行情况的脚本" class="headerlink" title="12.9、编写查询集群运行情况的脚本"></a>12.9、编写查询集群运行情况的脚本</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vim jpsall</span><br><span class="line"></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> host <span class="keyword">in</span> hadoop100 hadoop101 hadoop102</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> =============== <span class="variable">$host</span> ===============</span><br><span class="line">        ssh <span class="variable">$host</span> jps </span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存后退出，然后赋予脚本执行权限</span></span><br><span class="line"><span class="built_in">chmod</span> +x jpsall</span><br></pre></td></tr></table></figure>



<h3 id="12-10、常用端口号"><a href="#12-10、常用端口号" class="headerlink" title="12.10、常用端口号"></a>12.10、常用端口号</h3><table>
<thead>
<tr>
<th>端口名称</th>
<th>hadoop2.x</th>
<th>hadoop3.x</th>
</tr>
</thead>
<tbody><tr>
<td>NameNode 内部通信端口</td>
<td>8020&#x2F;9000</td>
<td>8020&#x2F;9000&#x2F;9820</td>
</tr>
<tr>
<td>NameNode HTTP UI</td>
<td>50070</td>
<td>9870</td>
</tr>
<tr>
<td>MapReduce 查看执行任务端口</td>
<td>8088</td>
<td>8088</td>
</tr>
<tr>
<td>历史服务器通信端口</td>
<td>19888</td>
<td>19888</td>
</tr>
</tbody></table>
<h3 id="12-11、常用配置文件"><a href="#12-11、常用配置文件" class="headerlink" title="12.11、常用配置文件"></a>12.11、常用配置文件</h3><table>
<thead>
<tr>
<th>版本</th>
<th>配置文件</th>
</tr>
</thead>
<tbody><tr>
<td>2.x</td>
<td>hdfs-site.xml yarn-site.xml mapred-site.xml workers</td>
</tr>
<tr>
<td>3.x</td>
<td>hdfs-site.xml yarn-site.xml mapred-site.xml slaves</td>
</tr>
</tbody></table>
<h3 id="12-12、集群时间同步"><a href="#12-12、集群时间同步" class="headerlink" title="12.12、集群时间同步"></a>12.12、集群时间同步</h3><p>如果服务器在公网环境（能连接外网），可以不采用集群时间同步。</p>
<p>如果服务器在内网，需要配置集群时间同步，否则可能会出现时间偏差，导致集群执行任务时间不同步。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看所有节点 ntpd 服务状态和开机自启动状态</span></span><br><span class="line">systemctl status ntpd</span><br><span class="line"><span class="comment"># 如果服务不存在，则安装</span></span><br><span class="line">yum install ntp</span><br><span class="line">systemctl start ntpd</span><br><span class="line"><span class="comment"># 设为开机自启</span></span><br><span class="line">systemctl <span class="built_in">enable</span> ntpd</span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop100 修改配置文件</span></span><br><span class="line">vim /etc/ntp.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># Hosts on local network are less restricted. 解开17行的注释 192.168.174.0 为我的网段</span></span><br><span class="line">restrict 192.168.174.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第21行，将这四行注释掉，不向互联网获取时间</span></span><br><span class="line"><span class="comment"># server 0.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment"># server 1.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment"># server 2.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment"># server 3.centos.pool.ntp.org iburst</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 末尾添加注释，当节点丢失网络时，可以采用本地时间作为时间服务器，为集群中的其它节点提供时间同步</span></span><br><span class="line">server 127.127.1.0</span><br><span class="line">fudge 127.127.1.0 stratum 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存后退出</span></span><br><span class="line">vim /etc/sysconfig/ntpd</span><br><span class="line"><span class="comment"># 增加一行，让硬件时间与系统时间一起同步</span></span><br><span class="line">SYNC_HWCLOCK=<span class="built_in">yes</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 hadoop101 hadoop102 创建定时任务，一分钟同步一次时间</span></span><br><span class="line">crontab -e</span><br><span class="line">*/1 * * * * /usr/sbin/ntpdate hadoop100</span><br><span class="line"><span class="comment"># 保存后退出</span></span><br></pre></td></tr></table></figure>





<h1 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h1><h2 id="13、概述"><a href="#13、概述" class="headerlink" title="13、概述"></a>13、概述</h2><h3 id="13-1、HDFS-的产生背景和定义"><a href="#13-1、HDFS-的产生背景和定义" class="headerlink" title="13.1、HDFS 的产生背景和定义"></a>13.1、HDFS 的产生背景和定义</h3><p>为了解决海量数据的存储问题。</p>
<p>HDFS 是一个分布式文件系统，用于存储文件，通过目录树来定位文件。</p>
<p>适合一次写入，多次读出的场景。</p>
<h3 id="13-2、优缺点"><a href="#13-2、优缺点" class="headerlink" title="13.2、优缺点"></a>13.2、优缺点</h3><blockquote>
<p>优点</p>
</blockquote>
<ol>
<li>高容错性<ul>
<li>数据自动保存多个副本，通过增加副本的形式，提高容错性。</li>
<li>某一个副本丢失以后，可以自动恢复。</li>
</ul>
</li>
<li>适合处理大数据<ul>
<li>数据规模：能处理的数据规模达到 GB、TB、甚至 PB 级别的数据。</li>
<li>文件规模：能处理百万规模以上的文件数量。</li>
</ul>
</li>
<li>可构建在廉价的机器上，通过多副本机制，提高可靠性。</li>
</ol>
<blockquote>
<p>缺点</p>
</blockquote>
<ol>
<li>不适合低延时数据访问。</li>
<li>无法高效的对大量小文件进行存储<ul>
<li>存储大量小文件的话，会占用 NameNode 大量的内存来存储文件目录和块信息，这样是不可取的，因为 NameNode 的内存总是有限的。</li>
<li>小文件存储的寻址时间会超过读取时间，违反了 HDFS 的设计目标。</li>
</ul>
</li>
<li>不支持并发写入和文件随机修改<ul>
<li>一个文件只能有一个写，不允许多个线程同时写。</li>
<li>仅支持数据 append（追加），不支持文件的随机修改。</li>
</ul>
</li>
</ol>
<h3 id="13-3、组成"><a href="#13-3、组成" class="headerlink" title="13.3、组成"></a>13.3、组成</h3><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/hdfsarchitecture.png" alt="hdfs"></p>
<ol>
<li>NameNode(nn):<ul>
<li>管理 HDFS 的名称空间</li>
<li>配置副本策略</li>
<li>管理数据块（Block）映射信息</li>
<li>处理客户端读写请求</li>
</ul>
</li>
<li>DataNode：NameNode 下达命令，DataNode 执行实际的操作<ul>
<li>存储实际的数据块</li>
<li>执行数据块的读&#x2F;写操作</li>
</ul>
</li>
<li>Client：<ul>
<li>文件切分，文件上传 HDFS 的时候，Client 将文件切分成一个一个的 Block，然后进行上传</li>
<li>与 NameNode 交互，获取文件的位置信息</li>
<li>与 DataNode 交互，读取或者写入数据</li>
<li>Client 提供一些命令来管理 HDFS，比如 NameNode 格式化</li>
<li>Clent 可以通过一些命令来访问 HDFS，比如对 HDFS 增删改查操作</li>
</ul>
</li>
<li>Secondary NameNode：并非 NameNode 的热备，当 NameNode 挂掉的时候，它并不是能马上替换 NameNode 并提供服务<ul>
<li>辅助 NameNode，分担其工作量，比如定期合并 Fsimage 和 Edits，并推送给 NameNode</li>
<li>在紧急情况下，可辅助恢复 NameNode</li>
</ul>
</li>
</ol>
<h3 id="13-4、文件块的大小"><a href="#13-4、文件块的大小" class="headerlink" title="13.4、文件块的大小"></a>13.4、文件块的大小</h3><p>HDFS 中的文件在物理上是分块存储（Block），块的大小可以用过配置参数（dfs.blocksize）来规定默认大小在 Hadoop2.x&#x2F;3.x 版本中是 128M，1.x 版本中是 64 M。</p>
<ol>
<li>如果寻址时间为 10ms，即 查找到目标 block 的时间为 10ms</li>
<li><font color="orange">寻址时间为传输时间的 1% 时，为最佳状态</font>，因此传输时间 &#x3D; 10ms&#x2F;0.01 &#x3D; 1000mx &#x3D; 1s</li>
<li>目前磁盘的传输速率普遍为 100MB&#x2F;s</li>
</ol>
<p>为什么块的大小不能设置太小，也不能设置太大？</p>
<ol>
<li>HDFS 的块设置太小，会增加寻址时间，程序一直在找块的开始位置</li>
<li>如果块设置的太大，从<code>磁盘传输数据的时间</code>会明显<code>大于定位这个块开始位置所需的时间</code>，导致程序在处理这块数据时，会非常慢。</li>
</ol>
<p>总结：<font color="orange">HDFS 块的大小设置主要取决于磁盘传输速率</font></p>
<h2 id="14、HDFS-的-Shell-相关操作"><a href="#14、HDFS-的-Shell-相关操作" class="headerlink" title="14、HDFS 的 Shell 相关操作"></a>14、HDFS 的 Shell 相关操作</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出这个命令的参数</span></span><br><span class="line">hadoop fs -<span class="built_in">help</span> <span class="built_in">rm</span></span><br><span class="line"><span class="comment"># 创建文件夹</span></span><br><span class="line">hadoop fs -<span class="built_in">mkdir</span> /sanguo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上传 </span></span><br><span class="line"><span class="comment"># 从本地剪切粘贴到 HDFS</span></span><br><span class="line">hadoop fs -moveFromLocal ./guanyu.txt /sanguo</span><br><span class="line"><span class="comment"># 从本地文件系统中拷贝文件到 HDFS 路径中</span></span><br><span class="line">hadoop fs -copyFromLocal ./zhaoyun.txt /sanguo</span><br><span class="line"><span class="comment"># -put 等同于 copyFromLocal</span></span><br><span class="line">hadoop fs -put ./caocao.tx /sanguo</span><br><span class="line"><span class="comment"># 追加一个文件到已经存在的文件末尾</span></span><br><span class="line">hadoop fs -appendToFile ./caopei.txt /sanguo/caocao.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载</span></span><br><span class="line"><span class="comment"># 从 HDFS 拷贝到本地</span></span><br><span class="line">hadoop fs -copyToLocal /sanguo/caocao.txt ./</span><br><span class="line"><span class="comment"># -get 等同于 -copyToLocal</span></span><br><span class="line">hadoop fs -get /sanguo/caocao.txt ./</span><br><span class="line"></span><br><span class="line"><span class="comment"># HDFS 直接操作</span></span><br><span class="line"><span class="comment"># -ls: 显示目录信息</span></span><br><span class="line">hadoop fs -<span class="built_in">ls</span> /sanguo</span><br><span class="line"><span class="comment"># -cat：显示文件内容</span></span><br><span class="line">hadoop fs -<span class="built_in">cat</span> /sanguo/caocao.txt</span><br><span class="line"><span class="comment"># -chgrp、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限</span></span><br><span class="line">hadoop fs -<span class="built_in">chmod</span> 666 /sanguo/caocao.txt</span><br><span class="line">hadoop fs -<span class="built_in">chown</span> root:root /sanguo/caocao.txt</span><br><span class="line"><span class="comment"># -mkdir：创建路径</span></span><br><span class="line">hadoop fs -<span class="built_in">mkdir</span> /shuguo</span><br><span class="line"><span class="comment"># -cp：从HDFS的一个路径拷贝到HDFS的另一个路径</span></span><br><span class="line">hadoop fs -<span class="built_in">cp</span> /sanguo/guanyu.txt /shuguo</span><br><span class="line"><span class="comment"># -mv：在HDFS目录中移动文件</span></span><br><span class="line">hadoop fs -<span class="built_in">mv</span> /sanguo/zhaoyun.txt /shuguo</span><br><span class="line"><span class="comment"># -tail：显示一个文件的末尾1kb的数据</span></span><br><span class="line">hadoop fs -<span class="built_in">tail</span> /shuguo/guanyu.txt</span><br><span class="line"><span class="comment"># -rm：删除文件或文件夹</span></span><br><span class="line">hadoop fs -<span class="built_in">rm</span> /sanguo/guanyu.txt</span><br><span class="line"><span class="comment"># -rm -r：递归删除目录及目录里面内容</span></span><br><span class="line">hadoop fs -<span class="built_in">rm</span> -r /sanguo</span><br><span class="line"><span class="comment"># -du统计文件夹的大小信息</span></span><br><span class="line">hadoop fs -<span class="built_in">du</span> -s -h /shuguo</span><br><span class="line">hadoop fs -<span class="built_in">du</span> -h /shuguo</span><br><span class="line"></span><br><span class="line"><span class="comment"># -setrep：设置 HDFS 中文件的副本数量</span></span><br><span class="line">hadoop fs -setrep 10 /shuguo/guanyu.txt</span><br><span class="line"><span class="comment"># 设置的副本数只是记录在 NameNode 的元数据中，是否真的会有这么多副本，还得看 DataNode 的数量。只有节点数增加到 10 台时，副本数才能达到 10</span></span><br></pre></td></tr></table></figure>



<h2 id="15、HDFS-的读写流程"><a href="#15、HDFS-的读写流程" class="headerlink" title="15、HDFS 的读写流程"></a>15、<font color="orange">HDFS 的读写流程</font></h2><h3 id="15-1、HDFS-的写数据流程"><a href="#15-1、HDFS-的写数据流程" class="headerlink" title="15.1、HDFS 的写数据流程"></a>15.1、HDFS 的写数据流程</h3><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220713161914818.png" alt="HDFS 写流程"></p>
<ol>
<li><p>客户端通过 Distributed FileSystem 模块向 NameNode 请求上传文件，NameNode 检查目标文件是否已存在，父目录是否存在。</p>
</li>
<li><p>NameNode 返回是否可以上传。</p>
</li>
<li><p>客户端请求第一个 Block 上传到哪几个 DataNode 服务器上。</p>
</li>
<li><p>NameNode 返回3个 DataNode 节点，分别为 dn1、dn2、dn3。</p>
</li>
<li><p>客户端通过 FSDataOutputStream 模块请求 dn1 上传数据，dn1 收到请求会继续调用 dn2，然后 dn2 调用 dn3，将这个通信管道建立完成。</p>
</li>
<li><p>dn1、dn2、dn3 逐级应答客户端。</p>
</li>
<li><p>客户端开始往 dn1 上传第一个 Block（先从磁盘读取数据放到一个本地内存缓存），以 Packet 为单位，dn1 收到一个 Packet 就会传给 dn2，dn2 传给 dn3；<code>dn1 每传一个 packet 会放入一个应答队列等待应答</code>。</p>
</li>
<li><p>当一个 Block 传输完成之后，客户端再次请求 NameNode 上传第二个 Block 的服务器。（重复执行3-7步）。</p>
</li>
</ol>
<h3 id="15-2、网络拓扑-节点距离计算"><a href="#15-2、网络拓扑-节点距离计算" class="headerlink" title="15.2、网络拓扑-节点距离计算"></a>15.2、网络拓扑-节点距离计算</h3><p>在 HDFS 写数据的过程中，NameNode 会选择距离待上传数据最近距离的 DataNode 接收数据。</p>
<p>节点距离：两个节点到达最近的共同祖先的距离总和</p>
<h3 id="15-3、机架感知"><a href="#15-3、机架感知" class="headerlink" title="15.3、机架感知"></a>15.3、机架感知</h3><ol>
<li>第一个副本在 Client 所处的节点上，如果客户端在集群外，随机选一个。</li>
<li>第二个副本在另一个机架的随机一个节点。</li>
<li>第三个副本在第二个副本所在机架的随机节点。</li>
</ol>
<h3 id="15-4、HDFS-读数据流程"><a href="#15-4、HDFS-读数据流程" class="headerlink" title="15.4、HDFS 读数据流程"></a>15.4、HDFS 读数据流程</h3><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220713165158260.png" alt="HDFS 读数据流程"></p>
<ol>
<li>客户端通过 DistributedFileSystem 向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的 DataNode 地址。</li>
<li>挑选一台 DataNode（就近原则，然后随机）服务器，请求读取数据。</li>
<li>DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以 Packet 为单位来做校验）。</li>
<li>客户端以 Packet 为单位接收，先在本地缓存，然后写入目标文件。</li>
</ol>
<h2 id="16、NN-和-2NN"><a href="#16、NN-和-2NN" class="headerlink" title="16、NN 和 2NN"></a>16、NN 和 2NN</h2><p>FsImage：磁盘中备份元数据的文件。</p>
<p>Edits：每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到 Edits 中，只进行追加操作。</p>
<p>SecondaryNameNode：用于 FsImage 和 Edits 的合并。</p>
<p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220713170144576.png" alt="NN 和 2NN 工作机制"></p>
<ol>
<li><p>第一阶段 NameNode 启动</p>
<ul>
<li>第一次启动 NameNode 格式化后，创建 Fsimage 和 Edits 文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</li>
<li>客户端对元数据进行增删改的请求。</li>
<li>NameNode 记录操作日志，更新滚动日志。</li>
<li>NameNode 在内存中对元数据进行增删改。</li>
</ul>
</li>
<li><p>第二阶段：Secondary NameNode工作</p>
<ul>
<li>Secondary NameNode 询问 NameNode 是否需要 CheckPoint。直接带回 NameNode 是否检查结果。</li>
<li>Secondary NameNode 请求执行 CheckPoint。</li>
<li>NameNode 滚动正在写的 Edits 日志。</li>
<li>将滚动前的编辑日志和镜像文件拷贝到 Secondary NameNode。</li>
<li>Secondary NameNode 加载编辑日志和镜像文件到内存，并合并。</li>
<li>生成新的镜像文件 fsimage.chkpoint。</li>
<li>拷贝 fsimage.chkpoint 到 NameNode。</li>
<li>NameNode 将 fsimage.chkpoint 重新命名成 fsimage。</li>
</ul>
</li>
</ol>
<h3 id="16-1、FsImage-和-Edits"><a href="#16-1、FsImage-和-Edits" class="headerlink" title="16.1、FsImage 和 Edits"></a>16.1、FsImage 和 Edits</h3><ol>
<li>FsImage文件：HDFS 文件系统元数据的一个永久性的检查点，其中包含 HDFS 文件系统的所有目录和文件 inode 的序列化信息</li>
<li>Edits 文件：存放 HDFS 文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到 Edits 文件中</li>
<li>seen_txid 文件保存的是一个数字，就是最后一个 edits_ 的数字</li>
<li>每次 NameNode 启动的时候，都会将 FsImage 文件读入内存，加载 Edits 里面的更新操作，保证内存中的元数据是最新的、同步的，可以看成 NameNode 启动的时候就将 FsImage 和 Edits 文件进行了合并</li>
</ol>
<h3 id="16-2、CheckPoint-时间设置"><a href="#16-2、CheckPoint-时间设置" class="headerlink" title="16.2、CheckPoint 时间设置"></a>16.2、CheckPoint 时间设置</h3><ol>
<li><p>通常情况下，SecondaryNameNode 每隔一小时执行一次（hdfs-default.xml）</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>一分钟检查一次操作次数，当操作次数达到一百万时，SecondaryNameNode 执行一次。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>操作动作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>60s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span> 1分钟检查一次操作次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="17、DataNode-工作机制"><a href="#17、DataNode-工作机制" class="headerlink" title="17、DataNode 工作机制"></a>17、DataNode 工作机制</h2><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220713215927332.png" alt="DataNode工作机制"></p>
<ol>
<li><p>一个数据块在 DataNode 上以文件形式存储在磁盘上，包括两个文件，一个是<code>数据本身</code>，一个是<code>元数据</code>包括数据块的长度，块数据的校验和，以及时间戳。</p>
</li>
<li><p>DataNode 启动后向 NameNode 注册，通过后，周期性（6小时）的向 NameNode 上报所有的块信息。</p>
</li>
<li><p>DN 向 NN 汇报当前解读信息的时间间隔，默认6小时；</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blockreport.intervalMsec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>21600000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>Determines block reporting interval in milliseconds.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>DN 扫描自己节点块信息列表的时间，默认6小时</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.directoryscan.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>21600s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>Interval in seconds for Datanode to scan data directories and reconcile the 	difference between blocks in memory and on the disk.Support multiple time unit suffix(case insensitive), as describedin dfs.heartbeat.interval.</span><br><span class="line">	<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>心跳是每3秒一次，心跳返回结果带有 NameNode 给该 DataNode 的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个 DataNode 的心跳，则认为该节点不可用。</p>
</li>
<li><p>集群运行中可以安全加入和退出一些机器。</p>
</li>
</ol>
<h3 id="17-1、数据完整性"><a href="#17-1、数据完整性" class="headerlink" title="17.1、数据完整性"></a>17.1、数据完整性</h3><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220713220519078.png" alt="数据完整性"></p>
<ol>
<li>当 DataNode 读取 Block 的时候，它会计算 CheckSum。</li>
<li>如果计算后的 CheckSum，与 Block 创建时值不一样，说明 Block 已经损坏。</li>
<li>Client 读取其他 DataNode 上的 Block。</li>
<li>常见的校验算法crc（32），md5（128），sha1（160）</li>
<li>DataNode 在其文件创建后周期验证 CheckSum。</li>
</ol>
<h3 id="17-2、掉线时限参数限制"><a href="#17-2、掉线时限参数限制" class="headerlink" title="17.2、掉线时限参数限制"></a>17.2、掉线时限参数限制</h3><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220713221409807.png" alt="掉线时限参数限制"></p>
<p>hdfs-site.xml 配置文件中的 heartbeat.recheck.interval 的单位为毫秒，dfs.heartbeat.interval 的单位为秒。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.heartbeat.recheck-interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>300000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>





<h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><h2 id="18、概述"><a href="#18、概述" class="headerlink" title="18、概述"></a>18、概述</h2><p>MapReduce 是一个分布式运算程序的编程框架，是用户开发”基于 Hadoop 的数据分析应用“的核心框架，其核心功能是将用户编写的<code>业务逻辑代码</code>和<code>自带默认组件</code>整合成一个完整的分布式运算程序，并发运行在一个 Hadoop 集群上。</p>
<blockquote>
<p>优点</p>
</blockquote>
<ol>
<li>易于开发</li>
<li>良好的扩展性，可以通过简单的增加机器来扩展它的计算能力</li>
<li>高容错性，其中一个机器挂了，可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败</li>
<li>适合 TB&#x2F;PB 级的海量数据处理</li>
</ol>
<blockquote>
<p>缺点</p>
</blockquote>
<ol>
<li>不擅长实时计算</li>
<li>不擅长流式计算，流式计算输入的数据是动态的，MapReduce 的输入数据集是静态的</li>
<li>不擅长 DAG（有向无环图）计算，DAG 指多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出</li>
</ol>
<blockquote>
<p>MapReduce 核心思想</p>
</blockquote>
<p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220714113629758.png" alt="MapReduce 核心思想"></p>
<ol>
<li><p>分布式的运算程序往往需要分成至少2个阶段。</p>
</li>
<li><p>第一个阶段的 MapTask 并发实例，完全并行运行，互不相干。</p>
</li>
<li><p>第二个阶段的 ReduceTask 并发实例互不相干，但是他们的数据依赖于上一个阶段的所有 MapTask 并发实例的输出。</p>
</li>
<li><p>MapReduce 编程模型只能包含一个 Map 阶段和一个 Reduce 阶段，如果用户的业务逻辑非常复杂，那就只能多个 MapReduce 程序，串行运行。</p>
</li>
</ol>
<blockquote>
<p>MapReduce 实例进程</p>
</blockquote>
<p>一个完整的 MapReduce 程序在分布式运行时有三类实例进程：</p>
<ol>
<li>MrAppMaster：负责整个程序的过程调度及状态协调</li>
<li>MapTask：负责 Map 阶段的整个数据处理流程</li>
<li>ReduceTask：负责 Reduce 阶段的整个数据处理流程</li>
</ol>
<blockquote>
<p>常用数据序列化类型</p>
</blockquote>
<table>
<thead>
<tr>
<th><strong>Java类型</strong></th>
<th><strong>Hadoop Writable类型</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Boolean</td>
<td>BooleanWritable</td>
</tr>
<tr>
<td>Byte</td>
<td>ByteWritable</td>
</tr>
<tr>
<td>Int</td>
<td>IntWritable</td>
</tr>
<tr>
<td>Float</td>
<td>FloatWritable</td>
</tr>
<tr>
<td>Long</td>
<td>LongWritable</td>
</tr>
<tr>
<td>Double</td>
<td>DoubleWritable</td>
</tr>
<tr>
<td>String</td>
<td>Text</td>
</tr>
<tr>
<td>Map</td>
<td>MapWritable</td>
</tr>
<tr>
<td>Array</td>
<td>ArrayWritable</td>
</tr>
<tr>
<td>Null</td>
<td>NullWritable</td>
</tr>
</tbody></table>
<blockquote>
<p>MapReduce 编程规范</p>
</blockquote>
<ol>
<li><p>Mapper 阶段</p>
<ul>
<li>用户自定义的 Mapper 要继承自己的父类</li>
<li>Mapper 的输入数据是 KV 对的形式（KV类型可自定义）</li>
<li>Mapper 中的业务逻辑写在 map() 方法中</li>
<li>Mapper 的输出数据是 KV 对的形式（KV类型可自定义）</li>
<li><code>map() 方法（MapTask 进程）对每一个 &lt;K,V&gt; 调用一次</code></li>
</ul>
</li>
<li><p>Reducer 阶段</p>
<ul>
<li>用户自定义的 Reducer 要继承自己的父类</li>
<li>Reducer 的输入数据类型对应 Mapper 的输出数据类型，也是 KV</li>
<li>Reducer 的业务逻辑写在 reduce() 方法中</li>
<li><code>ReduceTask 进程对每一组相同 K 的 &lt;K,V&gt; 组调用一次 reduce() 方法</code></li>
</ul>
</li>
<li><p>Driver 阶段</p>
<p>相当于 YARN 集群的客户端，用于提交整个程序到 YARN 集群，提交的是封装了 MapReduce 程序相关运行参数的 job 对象</p>
</li>
</ol>
<h2 id="19、序列化"><a href="#19、序列化" class="headerlink" title="19、序列化"></a>19、序列化</h2><p>Hadoop 序列化特点：</p>
<ol>
<li>紧凑：高效使用存储空间</li>
<li>快速：读写数据的额外开销小</li>
<li>互操作：支持多语言的交互</li>
</ol>
<h2 id="20、核心框架原理"><a href="#20、核心框架原理" class="headerlink" title="20、核心框架原理"></a>20、核心框架原理</h2><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220715210617739.png" alt="框架原理" style="zoom:150%;">

<h3 id="20-1、输入的数据-InputFormat"><a href="#20-1、输入的数据-InputFormat" class="headerlink" title="20.1、输入的数据 InputFormat"></a>20.1、输入的数据 InputFormat</h3><h4 id="20-1-1、切片与-MapTask-并行度决定机制"><a href="#20-1-1、切片与-MapTask-并行度决定机制" class="headerlink" title="20.1.1、切片与 MapTask 并行度决定机制"></a>20.1.1、切片与 MapTask 并行度决定机制</h4><ul>
<li>数据块：Block 是 HDFS 物理上把数据分成一块一块，数据块是 HDFS 存储数据的单位</li>
<li>数据切片：在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。数据切片是 MapReduce 程序计算输入数据的单位，一个切片会对应启动一个 MapTask</li>
</ul>
<p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220715101433013.png" alt="MapTask 并行度决定机制"></p>
<h4 id="20-1-2、FileInputFormat"><a href="#20-1-2、FileInputFormat" class="headerlink" title="20.1.2、FileInputFormat"></a>20.1.2、FileInputFormat</h4><p>切片机制：</p>
<ol>
<li>简单地按照文件的内容长度进行切片</li>
<li>切片大小，默认等于 Block 大小</li>
<li>切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</li>
</ol>
<h3 id="20-2、FileInputFormat-实现类"><a href="#20-2、FileInputFormat-实现类" class="headerlink" title="20.2、FileInputFormat 实现类"></a>20.2、FileInputFormat 实现类</h3><h5 id="20-2-0-1、TextInputFormat"><a href="#20-2-0-1、TextInputFormat" class="headerlink" title="20.2.0.1、TextInputFormat"></a>20.2.0.1、TextInputFormat</h5><p>是默认的 FileInputFormat 实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量， LongWritable 类型。值是这行的内容，不包括任何行终止符（换行符和回车符），Text 类型。</p>
<h5 id="20-2-0-2、CombineTextInputFormat"><a href="#20-2-0-2、CombineTextInputFormat" class="headerlink" title="20.2.0.2、CombineTextInputFormat"></a>20.2.0.2、CombineTextInputFormat</h5><p>CombineTextInputFormat 用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask 处理。</p>
<h3 id="20-3、MapReduce-工作流程"><a href="#20-3、MapReduce-工作流程" class="headerlink" title="20.3、MapReduce 工作流程"></a>20.3、MapReduce 工作流程</h3><h4 id="20-3-1、MapTask-工作机制"><a href="#20-3-1、MapTask-工作机制" class="headerlink" title="20.3.1、MapTask 工作机制"></a>20.3.1、MapTask 工作机制</h4><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/Snipaste_2022-07-15_21-09-58_2022-07-15_21-10-28.png" alt="MapTask 工作机制"></p>
<ol>
<li><p><code>Read 阶段</code>：MapTask 通过 InputFormat 获得的 RecordReader，从输入 InputSplit 中解析出一个个 key&#x2F;value。</p>
</li>
<li><p><code>Map 阶段</code>：该节点主要是将解析出的 key&#x2F;value 交给用户编写 map() 函数处理，并产生一系列新的 key&#x2F;value。</p>
</li>
<li><p><code>Collect 收集阶段</code>：在用户编写 map() 函数中，当数据处理完成后，一般会调用 OutputCollector.collect() 输出结果。在该函数内部，它会将生成的 key&#x2F;value 分区（调用 Partitioner），并写入一个环形内存缓冲区中。</p>
</li>
<li><p><code>Spill 阶段</code>：即“溢写”，当环形缓冲区满后，MapReduce 会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p>
<p>溢写阶段详情：</p>
<ul>
<li>步骤1：利用<code>快速排序</code>算法对<code>缓存区内的数据</code>进行排序，排序方式是，先按照<code>分区编号 Partition</code> 进行排序，然后按照 <code>key 进行排序</code>。这样，经过排序后，数据以<code>分区为单位</code>聚集在一起，且同一分区内所有数据<code>按照key有序</code>。</li>
<li>步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件 output&#x2F;spillN.out（N表示当前溢写次数）中。如果用户设置了 <code>Combiner</code>，则写入文件之前，对每个分区中的数据进行一次聚集操作。</li>
<li>步骤3：将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件 output&#x2F;spillN.out.index 中。</li>
</ul>
</li>
<li><p><code>Merge 阶段</code>：当所有数据处理完成后，MapTask 对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p>
<p>当所有数据处理完后，MapTask 会将所有临时文件合并成一个大文件，并保存到文件 output&#x2F;file.out 中，同时生成相应的索引文件output&#x2F;file.out.index。</p>
<p>在进行文件合并过程中，MapTask 以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并mapreduce.task.io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，<code>对文件排序后</code>，重复以上过程，直到最终得到一个大文件。</p>
<p><code>让每个 MapTask 最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。</code></p>
</li>
</ol>
<h4 id="20-3-2、ReduceTask-工作机制"><a href="#20-3-2、ReduceTask-工作机制" class="headerlink" title="20.3.2、ReduceTask 工作机制"></a>20.3.2、ReduceTask 工作机制</h4><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/asda_2022-07-15_21-26-56.png" alt="ReduceTask 工作机制"></p>
<ol>
<li><p><code>Copy 阶段</code>：ReduceTask 从各个 MapTask 上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p>
</li>
<li><p><code>Sort 阶段</code>：在远程拷贝数据的同时，ReduceTask 启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照 MapReduce 语义，用户编写 reduce() 函数输入数据是按 key 进行聚集的一组数据。为了将 key 相同的数据聚在一起，Hadoop 采用了基于排序的策略。由于各个 MapTask 已经实现对自己的处理结果进行了局部排序，因此，ReduceTask 只需对所有数据进行一次<code>归并排序</code>即可。</p>
</li>
<li><p><code>Reduce 阶段</code>：reduce() 函数将计算结果写到 HDFS 上。</p>
</li>
</ol>
<h3 id="20-4、Shuffle"><a href="#20-4、Shuffle" class="headerlink" title="20.4、Shuffle"></a>20.4、Shuffle</h3><p>处于 Map 方法之后，Reduce 方法之前的数据处理过程称之为 Shuffle。</p>
<p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/Snipaste_2022-07-19_14-54-08_2022-07-19_14-54-48.png" alt="shuffle 流程"></p>
<ol>
<li><p>MapTask 收集我们的 map() 方法输出的 kv 对，放到内存缓冲区中</p>
</li>
<li><p>从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</p>
</li>
<li><p>多个溢出文件会被合并成大的溢出文件</p>
</li>
<li><p>在溢出过程及合并的过程中，都要调用 Partitioner 进行分区和针对 key 进行排序</p>
</li>
<li><p>ReduceTask 根据自己的分区号，去各个 MapTask 机器上取相应的结果分区数据</p>
</li>
<li><p>ReduceTask 会抓取到同一个分区的来自不同 MapTask 的结果文件，ReduceTask 会将这些文件再进行合并（归并排序）</p>
</li>
<li><p>合并成大文件后，Shuffle 的过程也就结束了，后面进入 ReduceTask 的逻辑运算过程（从文件中取出一个一个的键值对 Group，调用用户自定义的 reduce() 方法）</p>
</li>
<li><p>Shuffle 中的缓冲区大小会影响到 MapReduce 程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。</p>
</li>
<li><p>缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb 默认100M。</p>
</li>
</ol>
<h3 id="20-5、Partition-分区"><a href="#20-5、Partition-分区" class="headerlink" title="20.5、Partition 分区"></a>20.5、Partition 分区</h3><p>将统计结果按照条件输出到不同文件中</p>
<p>默认分区是根据 key 的 hashCode 对 ReduceTasks 个数取模得到的，用户没法控制哪个 key 存储到哪个分区。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自定义 Partitioner</span></span><br><span class="line"><span class="comment">// 重写 getPartition() 方法</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">Public <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text key,FlowBean value,<span class="type">int</span> numPartitions)</span>&#123;</span><br><span class="line">    <span class="comment">// 控制分区代码逻辑</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> partition;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 job 驱动中，设置自定义的 Partitioner</span></span><br><span class="line">job.setPartitionerClass(CustomPartitioner.class);</span><br><span class="line"><span class="comment">// 自定义 Partition 后，要根据自定义 Partitioner 的逻辑设置相应数量的 ReduceTask</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);<span class="comment">// 要大于1</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>分区总结</p>
</blockquote>
<ol>
<li>如果 ReduceTask 的数量大于 getPartition 的结果数，则会多产生几个空的输出文件 part-r-000xxx</li>
<li>如果 ReduceTask 的数量大于 1 且 小于 getPartition 的结果数，则有一部分分区数据无法安放，爆 IO&#x2F;Exception</li>
<li>如果 ReduceTask 的数量 &#x3D; 1，则不管 MapTask 端输出多少个分区文件，最终结果都交给一个 ReduceTask，最终也就只会产生一个结果文件 part-r-0000</li>
<li>分区号必须从零开始，逐一累加</li>
</ol>
<h3 id="20-6、排序"><a href="#20-6、排序" class="headerlink" title="20.6、排序"></a>20.6、排序</h3><p>MapTask 和 ReduceTask 均会对数据按照 <code>key</code> 进行排序，该操作属于 Hadoop 的默认行为。<code>任何应用程序中的数据均会被排序，而不管逻辑上是否需要</code>。</p>
<p>默认排序是按照<code>字典顺序排序</code>，且实现该排序的方法是<code>快速排序</code>。</p>
<p>对于 MapTask，它会将处理的结果暂时放到环形缓冲区中，当<code>环形缓冲区使用率达到一定阈值后，再对缓中区中的数据进行一次快速排序</code>，并将这些有序数据溢写到磁盘上，而当数据处理完毕后，它会对<code>磁盘上所有文件进行归并排序</code>。<br>对于 ReduceTask,它从每个 MapTask 上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则溢写在磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，<code>ReduceTaska统一对内存和磁盘上的所有数据进行一次归并排序</code>。</p>
<blockquote>
<p>排序分类</p>
</blockquote>
<ol>
<li><p><font color="orange">部分排序</font></p>
<p>MapReduce 根据输入记录的键对数据集排序，保证<code>输出的每个文件内部有序</code>。</p>
</li>
<li><p>全排序</p>
<p><code>最终输出结果只有一个文件，且文件内部有序</code>，实现方式是只设置一个 ReduceTask，但该方法在处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了 MapReduce 所提供的并行架构。</p>
</li>
<li><p>辅助排序（GroupingComparator 分组）</p>
<p>在 Reduce 端对 key 进行分组，应用于：在接收的 key 为 bean 对象时，想让一个或几个字段相同（全部字段比较不相同）的 key 进入到同一个 Reduce 方法时，可以采用分组排序。</p>
</li>
<li><p>二次排序</p>
<p>在自定义排序过程中，如果 compareTo 中的判断条件为两个即为二次排序。</p>
</li>
</ol>
<blockquote>
<p>自定义排序 WritableComparable</p>
</blockquote>
 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// bean 对象作为 key 传输，需要实现 WritableComparable 接口重写 compareTo 方法</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">WritableComparable</span>&lt;FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> upFlow; <span class="comment">//上行流量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> downFlow; <span class="comment">//下行流量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> sumFlow; <span class="comment">//总流量</span></span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean bean)</span> &#123;</span><br><span class="line">        <span class="type">int</span> result;</span><br><span class="line">        <span class="comment">// 按照总流量大小，倒序排列</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &gt; bean.getSumFlow()) &#123;</span><br><span class="line">            result = -<span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &lt; bean.getSumFlow()) &#123;</span><br><span class="line">            result = <span class="number">1</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">            result = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//实现序列化和反序列化方法,注意顺序一定要一致</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        out.writeLong(<span class="built_in">this</span>.upFlow);</span><br><span class="line">        out.writeLong(<span class="built_in">this</span>.downFlow);</span><br><span class="line">        out.writeLong(<span class="built_in">this</span>.sumFlow);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = in.readLong();</span><br><span class="line">        <span class="built_in">this</span>.downFlow = in.readLong();</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = in.readLong();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="20-7、Combiner-合并"><a href="#20-7、Combiner-合并" class="headerlink" title="20.7、Combiner 合并"></a>20.7、Combiner 合并</h3><ol>
<li>Combiner 是 MR 程序中，Mapper 和 Reducer 之外的一种组件</li>
<li>Combiner 组件的父类就是 Reducer</li>
<li>Combiner 和 Reducer 的区别在于运行的位置<ul>
<li><code>Combiner 是在每一个 MapTask 所在的节点运行</code></li>
<li><code>Reducer 是接收全局所有 Mapper 的输出结果</code></li>
</ul>
</li>
<li>Combiner 的意义就是对每一个 MapTask 的输出进行局部汇总，以减小网络传输量</li>
<li><code>Combiner 能够应用的前提是不能影响最终的业务逻辑</code>，而且 Combiner 的输出 kv 应该跟 Reducer 的输入 kv 类型要对应起来</li>
</ol>
<blockquote>
<p>自定义 Combiner</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//自定义一个Combiner继承Reducer，重写Reduce方法</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 在Job驱动类中设置</span></span><br><span class="line">job.setCombinerClass(WordCountCombiner.class);</span><br></pre></td></tr></table></figure>



<h3 id="20-8、输出数据-OutputFormat"><a href="#20-8、输出数据-OutputFormat" class="headerlink" title="20.8、输出数据 OutputFormat"></a>20.8、输出数据 OutputFormat</h3><p>OutputFormat 是 MapReduce 输出的基类，所有实现 MapReduce 输出都实现了 OutputFormat 接口。</p>
<blockquote>
<p>自定义 OutputFormat</p>
</blockquote>
<ol>
<li>自定义一个类继承 FileOutputFormat</li>
<li>改写 RecordWriter，具体改写输出数据的 write 方法</li>
</ol>
<h3 id="20-9、ReduceTask-并行度决定机制"><a href="#20-9、ReduceTask-并行度决定机制" class="headerlink" title="20.9、ReduceTask 并行度决定机制"></a>20.9、ReduceTask 并行度决定机制</h3><p>ReduceTask 的并行度同样影响整个 Job 的执行并发度和执行效率，但与 MapTask 的并发数由切片数决定不同，ReduceTask 数量的决定是可以直接手动设置：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认值是1，手动设置为4</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意事项</p>
</blockquote>
<ol>
<li>ReduceTask &#x3D; 0，表示没有 Reduce 阶段，输出文件个数和 Map 个数一致</li>
<li>ReduceTask 默认值就是 1，所以输出文件个数为一个</li>
<li>如果数据分布不均匀，就有可能在 Reduce 阶段产生<code>数据倾斜</code></li>
<li>ReduceTask 数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有一个 ReduceTask</li>
<li>具体多少个 ReduceTask 需要根据集群性能而定</li>
<li>如果分区数不是 1，但是 ReduceTask 为 1，不会执行分区过程</li>
</ol>
<h3 id="20-10、Join"><a href="#20-10、Join" class="headerlink" title="20.10、Join"></a>20.10、Join</h3><p>Map 端的主要工作：为来自不同表或文件的 key&#x2F;value 对，<code>打标签以区别不同来源的记录</code>。然后用<code>连接字段作为 key</code>，其余部分和新加的标志作为 value，最后进行输出。</p>
<p>Reduce 端的主要工作：在 Reduce 端以<code>连接字段作为 key 的分组已经完成</code>，我们只需要在每一个分组当中将那些来源于不同文件的记录（在 Map 阶段已经打标志）分开，最后进行合并。</p>
<h4 id="20-10-1、Reduce-Join"><a href="#20-10-1、Reduce-Join" class="headerlink" title="20.10.1、Reduce Join"></a>20.10.1、Reduce Join</h4><p>缺点：合并的操作是在 Reduce 阶段完成，Reduce 端的处理压力太大，Map 节点的运算负载则很低，资源利用率不高，且在 Reduce 阶段极易产生数据倾斜。</p>
<h4 id="20-10-2、Map-Join"><a href="#20-10-2、Map-Join" class="headerlink" title="20.10.2、Map Join"></a>20.10.2、Map Join</h4><p>适用于一张表十分小、一张表很大的场景。</p>
<p>在 Map 端缓存多张表，提前处理业务逻辑，这样增加 Map 端业务，减少 Reduce 端数据的压力，尽可能的减少数据倾斜。</p>
<blockquote>
<p>DistributedCache</p>
</blockquote>
<ol>
<li><p>在 Mapper 的 setup 阶段，将文件读取到缓存集合中</p>
</li>
<li><p>在 Driver 驱动类中加载缓存</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//缓存普通文件到Task运行节点。</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;file://E:/desktop/cache/pd.txt&quot;</span>));</span><br><span class="line"><span class="comment">//如果是集群运行,需要设置HDFS路径</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop100:8020/cache/pd.txt&quot;</span>));</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="20-11、ETL-数据清洗"><a href="#20-11、ETL-数据清洗" class="headerlink" title="20.11、ETL 数据清洗"></a>20.11、ETL 数据清洗</h3><p>ETL（Extract-Transform-Load）用来描述将数据从来源端经过抽取（Extract）、转换（Transform）、加载（Load）至目的端的过程。</p>
<p>在运行核心业务 MapReduce 程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行 Mapper 程序，不需要运行 Reduce 程序。</p>
<h2 id="21、压缩"><a href="#21、压缩" class="headerlink" title="21、压缩"></a>21、压缩</h2><blockquote>
<p>压缩的优缺点</p>
</blockquote>
<p>优点：减少磁盘 IO、减少磁盘存储空间</p>
<p>缺点：增加 cpu 开销</p>
<blockquote>
<p>压缩使用原则</p>
</blockquote>
<ol>
<li>运算密集型的 Job，少用压缩</li>
<li>IO 密集型的 Job，多用压缩</li>
</ol>
<h3 id="21-1、压缩算法"><a href="#21-1、压缩算法" class="headerlink" title="21.1、压缩算法"></a>21.1、压缩算法</h3><table>
<thead>
<tr>
<th>压缩格式</th>
<th>Hadoop自带？</th>
<th>算法</th>
<th>文件扩展名</th>
<th>是否可切片</th>
<th>换成压缩格式后，原来的程序是否需要修改</th>
<th>优缺点</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.deflate</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
<td></td>
</tr>
<tr>
<td>Gzip</td>
<td>是，直接使用</td>
<td>DEFLATE</td>
<td>.gz</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
<td>压缩率比较高；不支持split，压缩&#x2F;解压速度一般</td>
</tr>
<tr>
<td>bzip2</td>
<td>是，直接使用</td>
<td>bzip2</td>
<td>.bz2</td>
<td><code>是</code></td>
<td>和文本处理一样，不需要修改</td>
<td>压缩率高，支持split；压缩&#x2F;解压速度慢</td>
</tr>
<tr>
<td>LZO</td>
<td>否，需要安装</td>
<td>LZO</td>
<td>.lzo</td>
<td><code>是</code></td>
<td><code>需要建索引，还需要指定输入格式</code></td>
<td>压缩&#x2F;解压速度比较快，支持split；压缩率一般，想支持切片需要额外创建索引</td>
</tr>
<tr>
<td>Snappy</td>
<td>是，直接使用</td>
<td>Snappy</td>
<td>.snappy</td>
<td>否</td>
<td>和文本处理一样，不需要修改</td>
<td>压缩&#x2F;解压速度快；不支持split，压缩率一般</td>
</tr>
</tbody></table>
<h3 id="21-2、性能比较"><a href="#21-2、性能比较" class="headerlink" title="21.2、性能比较"></a>21.2、性能比较</h3><table>
<thead>
<tr>
<th>压缩算法</th>
<th>原始文件大小</th>
<th>压缩文件大小</th>
<th>压缩速度</th>
<th>解压速度</th>
</tr>
</thead>
<tbody><tr>
<td>gzip</td>
<td>8.3GB</td>
<td>1.8GB</td>
<td>17.5MB&#x2F;s</td>
<td>58MB&#x2F;s</td>
</tr>
<tr>
<td>bzip2</td>
<td>8.3GB</td>
<td>1.1GB</td>
<td>2.4MB&#x2F;s</td>
<td>9.5MB&#x2F;s</td>
</tr>
<tr>
<td>LZO</td>
<td>8.3GB</td>
<td>2.9GB</td>
<td>49.3MB&#x2F;s</td>
<td>74.6MB&#x2F;s</td>
</tr>
</tbody></table>
<h3 id="21-3、压缩方式的选择"><a href="#21-3、压缩方式的选择" class="headerlink" title="21.3、压缩方式的选择"></a>21.3、压缩方式的选择</h3><p>选择压缩方式时应考虑：压缩&#x2F;解压缩速度、压缩率（压缩后存储大小）、压缩后是否可以支持切片</p>
<p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220723221209119.png" alt="压缩方式的选择"></p>
<h3 id="21-4、压缩参数配置"><a href="#21-4、压缩参数配置" class="headerlink" title="21.4、压缩参数配置"></a>21.4、压缩参数配置</h3><blockquote>
<p>编码器&amp;解码器</p>
</blockquote>
<table>
<thead>
<tr>
<th>压缩格式</th>
<th>对应的编码&#x2F;解码器</th>
</tr>
</thead>
<tbody><tr>
<td>DEFLATE</td>
<td>org.apache.hadoop.io.compress.DefaultCodec</td>
</tr>
<tr>
<td>gzip</td>
<td>org.apache.hadoop.io.compress.GzipCodec</td>
</tr>
<tr>
<td>bzip2</td>
<td>org.apache.hadoop.io.compress.BZip2Codec</td>
</tr>
<tr>
<td>LZO</td>
<td>com.hadoop.compression.lzo.LzopCodec</td>
</tr>
<tr>
<td>Snappy</td>
<td>org.apache.hadoop.io.compress.SnappyCodec</td>
</tr>
</tbody></table>
<blockquote>
<p>压缩参数配置</p>
</blockquote>
<table>
<thead>
<tr>
<th>参数</th>
<th>默认值</th>
<th>阶段</th>
<th>建议</th>
</tr>
</thead>
<tbody><tr>
<td>io.compression.codecs<br>（在core-site.xml中配置）</td>
<td>无，这个需要在命令行输入<br>hadoop checknative查看</td>
<td>输入压缩</td>
<td>Hadoop使用文件扩展名判断是否支持某种编解码器</td>
</tr>
<tr>
<td>mapreduce.map.output.compress<br>（在mapred-site.xml中配置）</td>
<td>false</td>
<td>mapper输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.map.output.compress.codec<br>（在mapred-site.xml中配置）</td>
<td>org.apache.hadoop.io<br>.compress.DefaultCodec</td>
<td>mapper输出</td>
<td>企业多使用LZO或Snappy编解码器在此阶段压缩数据</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress<br>（在mapred-site.xml中配置）</td>
<td>false</td>
<td>reducer输出</td>
<td>这个参数设为true启用压缩</td>
</tr>
<tr>
<td>mapreduce.output.fileoutputformat.compress.codec<br>（在mapred-site.xml中配置）</td>
<td>org.apache.hadoop.io<br>.compress.DefaultCodec</td>
<td>reducer输出</td>
<td>使用标准工具或者编解码器，如gzip和bzip2</td>
</tr>
</tbody></table>
<h2 id="22、MapReduce开发总结"><a href="#22、MapReduce开发总结" class="headerlink" title="22、MapReduce开发总结"></a>22、MapReduce开发总结</h2><ol>
<li>输入数据接口：InputFormat<ul>
<li>默认使用的实现类是：TextInputFormat</li>
<li>TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为 key，行内容作为 value 返回。</li>
<li>CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。</li>
</ul>
</li>
<li>逻辑处理接口：Mapper <ul>
<li>用户根据业务需求实现其中三个方法：setup() map() cleanup()</li>
</ul>
</li>
<li>分区：Partitioner<ul>
<li>有默认实现 HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个分区号；key.hashCode() &amp; Integer.MAXVALUE % numReduces</li>
<li>如果业务上有特别的需求，可以自定义分区</li>
</ul>
</li>
<li>排序：Comparable<ul>
<li>当我们用自定义的对象作为 key 来输出时，就必须要实现 WritableComparable 接口，重写其中的 compareTo() 方法。</li>
<li>部分排序：对最终输出的每一个文件进行内部排序</li>
<li>全排序：对所有数据进行排序，通常只有一个 Reduce</li>
<li>二次排序：排序的条件有两个</li>
</ul>
</li>
<li>合并：Combiner<ul>
<li>Combiner 合并可以提高程序执行效率，减少IO传输。但是使用时必须不能影响原有的业务处理结果。</li>
</ul>
</li>
<li>逻辑处理接口：Reducer<ul>
<li>用户根据业务需求实现其中三个方法：setup() reduce() cleanup ()</li>
</ul>
</li>
<li>输出数据接口：OutputFormat<ul>
<li>默认实现类是 TextOutputFormat，功能逻辑是：将每一个 KV 对，向目标文本文件输出一行。</li>
<li>用户还可以自定义 OutputFormat。</li>
</ul>
</li>
</ol>
<h1 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h1><p><code>Yarn</code> 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的<code>操作系统平台</code>，而 <code>MapReduce</code> 等运算程序则相当于运行于操作系统之上的<code>应用程序</code>。</p>
<h2 id="23、Yarn-基础架构"><a href="#23、Yarn-基础架构" class="headerlink" title="23、Yarn 基础架构"></a>23、Yarn 基础架构</h2><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/Snipaste_2022-07-25_10-24-16_2022-07-25_10-26-45.png" alt="Yarn 基础架构"></p>
<h2 id="24、Yarn-工作机制"><a href="#24、Yarn-工作机制" class="headerlink" title="24、Yarn 工作机制"></a>24、Yarn 工作机制</h2><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/Snipaste_2022-07-25_10-59-59_2022-07-25_11-01-30.png" alt="Yarn 工作机制"></p>
<ol>
<li>MR 程序提交到客户端所在的节点</li>
<li>YarnRunner 向 ResourceManager 申请一个 Application</li>
<li>RM 将该应用程序的资源路径返回给 YarnRunner</li>
<li>该程序将运行所需资源提交到 HDFS 上</li>
<li>程序资源提交完毕后，申请运行 mrAppMaster</li>
<li>RM 将用户的请求初始化成一个 Task</li>
<li>其中一个 NodeManager 领取到Task任务</li>
<li>该 NodeManager 创建容器 Container，并产生 MRAppmaster</li>
<li>Container 从 HDFS 上拷贝资源到本地</li>
<li>MRAppmaster 向 RM 申请运行 MapTask 资源</li>
<li>RM 将运行 MapTask 任务分配给另外两个 NodeManager，另两个 NodeManager 分别领取任务并创建容器</li>
<li>MR 向两个接收到任务的 NodeManager 发送程序启动脚本，这两个 NodeManager 分别启动 MapTask，MapTask 对数据分区排序</li>
<li>MrAppMaster 等待所有 MapTask 运行完毕后，向RM申请容器，运行 ReduceTask</li>
<li>ReduceTask 向 MapTask 获取相应分区的数据</li>
<li>程序运行完毕后，MR 会向 RM 申请注销自己</li>
</ol>
<h2 id="25、Yarn调度器和调度算法"><a href="#25、Yarn调度器和调度算法" class="headerlink" title="25、Yarn调度器和调度算法"></a>25、Yarn调度器和调度算法</h2><p>Hadoop 作业调度器主要有三种：FIFO 调度器（First In Firest Out）、容量调度器（Capacity Scheduler）、公平调度器（Fair Scheduler），Apache Hadoop 3.x 默认的资源调度器是 Capacity Scheduler</p>
<h3 id="25-1、先进先出调度器（FIFO）"><a href="#25-1、先进先出调度器（FIFO）" class="headerlink" title="25.1、先进先出调度器（FIFO）"></a>25.1、先进先出调度器（FIFO）</h3><p>根据提交任务的先后顺序，先来先服务。</p>
<p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220725141854005.png" alt="FIFO 调度器"></p>
<h3 id="25-2、容量调度器（Capacity-Scheduler）"><a href="#25-2、容量调度器（Capacity-Scheduler）" class="headerlink" title="25.2、容量调度器（Capacity Scheduler）"></a>25.2、容量调度器（Capacity Scheduler）</h3><p>Capacity Scheduler 是 Yahoo 开发的多用户调度器</p>
<p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/%E5%AE%B9%E9%87%8F%E8%B0%83%E5%BA%A6%E5%99%A8%E7%89%B9%E7%82%B9_2022-07-25_14-48-08.png" alt="容量调度器特点"></p>
<p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/%E5%AE%B9%E9%87%8F%E8%B0%83%E5%BA%A6%E5%99%A8%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E7%AE%97%E6%B3%95_2022-07-25_14-52-07.png" alt="容量调度器资源分配算法"></p>
<h3 id="25-3、公平调度器（Fair-Scheduler）"><a href="#25-3、公平调度器（Fair-Scheduler）" class="headerlink" title="25.3、公平调度器（Fair Scheduler）"></a>25.3、公平调度器（Fair Scheduler）</h3><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%99%A8%E7%9A%84%E7%89%B9%E7%82%B9_2022-07-25_15-11-11.png" alt="公平调度器的特点"></p>
<blockquote>
<p>缺额</p>
</blockquote>
<ul>
<li>公平调度器设计目标是：在时间尺度上，所有作业获得公平的资源，某一时刻一个作业应获资源和实际获取资源的差额称之为”缺额“</li>
<li>调度器会优先为缺额大的作业分配资源</li>
</ul>
<p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%99%A8%E9%98%9F%E5%88%97%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E6%96%B9%E5%BC%8F_2022-07-25_15-25-42.png" alt="公平调度器队列资源分配方式"></p>
<h2 id="26、Yarn-常用命令"><a href="#26、Yarn-常用命令" class="headerlink" title="26、Yarn 常用命令"></a>26、Yarn 常用命令</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列出所有的 Application</span></span><br><span class="line">yarn application -list</span><br><span class="line"><span class="comment"># Application 状态查询过滤</span></span><br><span class="line">yarn application -list -appStates ALL/NEW/NEW_SAVING/SUBMITTED/ACCEPTED/RUNNING/FINISHED/FAILED/KILLED</span><br><span class="line"><span class="comment"># kill Application</span></span><br><span class="line">yarn application -<span class="built_in">kill</span> &lt;ApplicationId&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 日志查看</span></span><br><span class="line"><span class="comment"># 查询 Application 日志</span></span><br><span class="line">yarn logs -applicationId &lt;ApplicationId&gt;</span><br><span class="line"><span class="comment"># 查看 Container 日志</span></span><br><span class="line">yarn logs -applicationId &lt;ApplicationId&gt; -containerId &lt;ContainerId&gt; </span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看尝试运行的任务</span></span><br><span class="line"><span class="comment"># 列出所有 Application 尝试的列表</span></span><br><span class="line">yarn applicationattempt -list &lt;ApplicationId&gt;</span><br><span class="line"><span class="comment"># 打印 ApplicationAttemp 状态</span></span><br><span class="line">yarn applicationattempt -status &lt;ApplicationAttemptId&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看容器</span></span><br><span class="line"><span class="comment"># 列出所有 Container</span></span><br><span class="line">yarn container -list &lt;ApplicationAttemptId&gt;</span><br><span class="line"><span class="comment"># 打印 Container 状态，只有在任务运行的途中才能看到 container 的状态</span></span><br><span class="line">yarn container -status &lt;ContainerId&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看节点状态</span></span><br><span class="line"><span class="comment"># 列出所有节点</span></span><br><span class="line">yarn node -list -all</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新配置</span></span><br><span class="line">yarn rmadmin -refreshQueues</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看队列</span></span><br><span class="line">yarn queue -status &lt;QueueName&gt;</span><br></pre></td></tr></table></figure>



<h2 id="27、Yarn-生产环境核心参数"><a href="#27、Yarn-生产环境核心参数" class="headerlink" title="27、Yarn 生产环境核心参数"></a>27、Yarn 生产环境核心参数</h2><p><img src="https://gis-visualization.oss-cn-beijing.aliyuncs.com/typoraImg/image-20220726140416070.png" alt="Yarn 生产环境核心参数"></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- yarn-site.xml 配置案例，若是不同节点的 cpu、内存不一样，要单独配置，不能集群分发 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 选择调度器，默认容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>The class to use as the resource scheduler.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- ResourceManager处理调度器请求的线程数量,默认50；如果提交的任务数大于50，可以增加该值，但是不能超过3台 * 4线程 = 12线程（去除其他应用程序实际不能超过8） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of threads to handle scheduler interface.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.client.thread-count<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 是否让yarn自动检测硬件进行配置，默认是false，如果该节点有很多其他应用程序，建议手动配置。如果该节点没有其他应用程序，可以采用自动 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>Enable auto-detection of node capabilities such as</span><br><span class="line">	memory and CPU.</span><br><span class="line">	<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.detect-hardware-capabilities<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 是否将虚拟核数当作CPU核数，默认是false，采用物理CPU核数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>Flag to determine if logical processors(such as</span><br><span class="line">	hyperthreads) should be counted as cores. Only applicable on Linux</span><br><span class="line">	when yarn.nodemanager.resource.cpu-vcores is set to -1 and</span><br><span class="line">	yarn.nodemanager.resource.detect-hardware-capabilities is true.</span><br><span class="line">	<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.count-logical-processors-as-cores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 虚拟核数和物理核数乘数，默认是1.0 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>Multiplier to determine how to convert phyiscal cores to</span><br><span class="line">	vcores. This value is used if yarn.nodemanager.resource.cpu-vcores</span><br><span class="line">	is set to -1(which implies auto-calculate vcores) and</span><br><span class="line">	yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The	number of vcores will be calculated as	number of CPUs * multiplier.</span><br><span class="line">	<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.pcores-vcores-multiplier<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- NodeManager使用内存数，默认8G，修改为4G内存 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>Amount of physical memory, in MB, that can be allocated </span><br><span class="line">	for containers. If set to -1 and</span><br><span class="line">	yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">	automatically calculated(in case of Windows and Linux).</span><br><span class="line">	In other cases, the default is 8192MB.</span><br><span class="line">	<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- nodemanager的CPU核数，不按照硬件环境自动设定时默认是8个，修改为4个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>Number of vcores that can be allocated</span><br><span class="line">	for containers. This is used by the RM scheduler when allocating</span><br><span class="line">	resources for containers. This is not used to limit the number of</span><br><span class="line">	CPUs used by YARN containers. If it is set to -1 and</span><br><span class="line">	yarn.nodemanager.resource.detect-hardware-capabilities is true, it is</span><br><span class="line">	automatically determined from the hardware in case of Windows and Linux.</span><br><span class="line">	In other cases, number of vcores is 8 by default.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 容器最小内存，默认1G --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM	in MBs. Memory requests lower than this will be set to the value of this	property. Additionally, a node manager that is configured to have less memory	than this value will be shut down by the resource manager.</span><br><span class="line">	<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 容器最大内存，默认8G，修改为2G --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM	in MBs. Memory requests higher than this will throw an	InvalidResourceRequestException.</span><br><span class="line">	<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 容器最小CPU核数，默认1个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>The minimum allocation for every container request at the RM	in terms of virtual CPU cores. Requests lower than this will be set to the	value of this property. Additionally, a node manager that is configured to	have fewer virtual cores than this value will be shut down by the resource	manager.</span><br><span class="line">	<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 容器最大CPU核数，默认4个，修改为2个 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>The maximum allocation for every container request at the RM	in terms of virtual CPU cores. Requests higher than this will throw an</span><br><span class="line">	InvalidResourceRequestException.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存检查，默认打开，修改为关闭 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether virtual memory limits will be enforced for</span><br><span class="line">	containers.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 虚拟内存和物理内存设置比例,默认2.1 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">description</span>&gt;</span>Ratio between virtual memory to physical memory when	setting memory limits for containers. Container allocations are	expressed in terms of physical memory, and virtual memory usage	is allowed to exceed this allocation by this ratio.</span><br><span class="line">	<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>2.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h2 id="28、容量调度器多队列"><a href="#28、容量调度器多队列" class="headerlink" title="28、容量调度器多队列"></a>28、容量调度器多队列</h2><p>好处：</p>
<ol>
<li>可以避免因为单队列耗尽所有资源而导致集群不可用</li>
<li>可以实现任务的降级，在特殊时期保证重要的任务队列资源充足</li>
</ol>
<h3 id="28-1、多队列配置"><a href="#28-1、多队列配置" class="headerlink" title="28.1、多队列配置"></a>28.1、多队列配置</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- capacity-scheduler.xml 中配置如下 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定多队列，增加hive队列 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.queues<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>default,hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      The queues at the this level (root is the root queue).</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 降低default队列资源额定容量为40%，默认100% --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>40<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 降低default队列资源最大容量为60%，默认100% --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 新加队列添加必要属性 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定hive队列的资源额定容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 用户最多可以使用队列多少资源，1表示 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.user-limit-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定hive队列的资源最大容量 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>80<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 启动hive队列 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.state<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>RUNNING<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 哪些用户有权向队列提交作业 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_submit_applications<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 哪些用户有权操作队列，管理员权限（查看/杀死） --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_administer_queue<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 哪些用户有权配置提交任务优先级 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.acl_application_max_priority<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 任务的超时时间设置：yarn application -appId appId -updateLifetime Timeout</span></span><br><span class="line"><span class="comment">参考资料：https://blog.cloudera.com/enforcing-application-lifetime-slas-yarn/ --&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 如果application指定了超时时间，则提交到该队列的application能够指定的最大超时时间不能超过该值。 </span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.maximum-application-lifetime<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 如果application没指定超时时间，则用default-application-lifetime作为默认值 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.default-application-lifetime<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>-1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 向集群分发配置 --&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 重启 Yarn 或者执行命令：yarn rmadmin -refreshQueues --&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 执行任务时指定队列</span></span><br><span class="line">hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.3.jar wordcount -D mapreduce.job.queuename=hive /input /optput</span><br><span class="line"><span class="comment"># 也可以在 Driver 类中中指定</span></span><br><span class="line"><span class="comment"># new Configuration().set(&quot;mapreduce.job.qunuename&quot;,&quot;hive&quot;)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="28-2、任务优先级"><a href="#28-2、任务优先级" class="headerlink" title="28.2、任务优先级"></a>28.2、任务优先级</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 修改yarn-site.xml文件，增加以下参数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.cluster.max-application-priority<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 分发配置，重启 yarn --&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提交任务时，指定优先级 mapreduce.job.priority=优先级</span></span><br><span class="line">hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.3.jar pi  -D mapreduce.job.priority=5 5 2000000</span><br><span class="line"><span class="comment"># 提升正在执行的任务的优先级</span></span><br><span class="line">yarn application -appID &lt;ApplicationID&gt; -updatePriority 优先级</span><br></pre></td></tr></table></figure>

<h2 id="29、公平调度器"><a href="#29、公平调度器" class="headerlink" title="29、公平调度器"></a>29、公平调度器</h2><p>创建两个队列，分别是 test 和 hqz（以用户所属组命名）。</p>
<p>期望实现以下效果：若用户提交任务时指定队列，则任务提交到指定队列运行；</p>
<p>若未指定队列，test 用户提交的任务到 root.group.test 队列运行，hqz提交的任务到 root.group.hqz 队列运行（注：group 为用户所属组）。</p>
<p>公平调度器的配置涉及到两个文件，一个是 yarn-site.xml，另一个是公平调度器队列分配文件 fair-scheduler.xml（文件名可自定义）。</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 修改yarn-site.xml文件，加入以下参数 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>配置使用公平调度器<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.fair.allocation.file<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.1.3/etc/hadoop/fair-scheduler.xml<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>指明公平调度器队列分配配置文件<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.fair.preemption<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>禁止队列间资源抢占<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 配置fair-scheduler.xml --&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">allocations</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 单个队列中Application Master占用资源的最大比例,取值0-1 ，企业一般配置0.1 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">queueMaxAMShareDefault</span>&gt;</span>0.5<span class="tag">&lt;/<span class="name">queueMaxAMShareDefault</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 单个队列最大资源的默认值 test hqz default --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">queueMaxResourcesDefault</span>&gt;</span>4096mb,4vcores<span class="tag">&lt;/<span class="name">queueMaxResourcesDefault</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 增加一个队列test --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">queue</span> <span class="attr">name</span>=<span class="string">&quot;test&quot;</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列最小资源 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minResources</span>&gt;</span>2048mb,2vcores<span class="tag">&lt;/<span class="name">minResources</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列最大资源 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maxResources</span>&gt;</span>4096mb,4vcores<span class="tag">&lt;/<span class="name">maxResources</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列中最多同时运行的应用数，默认50，根据线程数配置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maxRunningApps</span>&gt;</span>4<span class="tag">&lt;/<span class="name">maxRunningApps</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列中Application Master占用资源的最大比例 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maxAMShare</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 增加一个队列hqz --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">queue</span> <span class="attr">name</span>=<span class="string">&quot;hqz&quot;</span> <span class="attr">type</span>=<span class="string">&quot;parent&quot;</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列最小资源 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">minResources</span>&gt;</span>2048mb,2vcores<span class="tag">&lt;/<span class="name">minResources</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列最大资源 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maxResources</span>&gt;</span>4096mb,4vcores<span class="tag">&lt;/<span class="name">maxResources</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列中最多同时运行的应用数，默认50，根据线程数配置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maxRunningApps</span>&gt;</span>4<span class="tag">&lt;/<span class="name">maxRunningApps</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列中Application Master占用资源的最大比例 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">maxAMShare</span>&gt;</span>0.5<span class="tag">&lt;/<span class="name">maxAMShare</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 该队列资源权重,默认值为1.0 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">weight</span>&gt;</span>1.0<span class="tag">&lt;/<span class="name">weight</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 队列内部的资源分配策略 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">schedulingPolicy</span>&gt;</span>fair<span class="tag">&lt;/<span class="name">schedulingPolicy</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">queue</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 任务队列分配策略,可配置多层规则,从第一个规则开始匹配,直到匹配成功 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">queuePlacementPolicy</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 提交任务时指定队列,如未指定提交队列,则继续匹配下一个规则; false表示：如果指定队列不存在,不允许自动创建--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">rule</span> <span class="attr">name</span>=<span class="string">&quot;specified&quot;</span> <span class="attr">create</span>=<span class="string">&quot;false&quot;</span>/&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 提交到root.group.username队列,若root.group不存在,不允许自动创建；若root.group.user不存在,允许自动创建 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">rule</span> <span class="attr">name</span>=<span class="string">&quot;nestedUserQueue&quot;</span> <span class="attr">create</span>=<span class="string">&quot;true&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rule</span> <span class="attr">name</span>=<span class="string">&quot;primaryGroup&quot;</span> <span class="attr">create</span>=<span class="string">&quot;false&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">rule</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 最后一个规则必须为reject或者default。Reject表示拒绝创建提交失败，default表示把任务提交到default队列 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">rule</span> <span class="attr">name</span>=<span class="string">&quot;reject&quot;</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">queuePlacementPolicy</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">allocations</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分发配置之后，重启 Yarn</span></span><br><span class="line"><span class="comment"># 提交任务时指定队列，按照配置规则，任务会到指定的 root.test 队列</span></span><br><span class="line">hadoop jar /opt/module/hadoop-3.3.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.3.jar pi -Dmapreduce.job.queuename=root.test 1 1</span><br><span class="line"><span class="comment"># 提交任务时不指定队列，按照配置规则，任务会到 root.hqz 队列</span></span><br><span class="line">hadoop jar /opt/module/hadoop-3.3.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.3.jar pi 1 1</span><br></pre></td></tr></table></figure>

<h3 id="29-1、Yarn的Tool接口"><a href="#29-1、Yarn的Tool接口" class="headerlink" title="29.1、Yarn的Tool接口"></a>29.1、Yarn的Tool接口</h3><p>可以实现动态传参</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ToolRunner.run(conf, tool, Arrays.copyOfRange(args, <span class="number">1</span>, args.length));</span><br></pre></td></tr></table></figure>

 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://hqzqaq.github.io/2022/09/23/hadoop/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%9F%BA%E7%9F%B3/" rel="tag">基石</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag">大数据</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2022/09/23/java-lambda%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            java lambda表达式使用示例
          
        </div>
      </a>
    
    
      <a href="/2022/09/23/git%E6%93%8D%E4%BD%9C/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">git操作</div>
      </a>
    
  </nav>

  
   
  
   
    <script src="https://cdn.staticfile.org/twikoo/1.4.18/twikoo.all.min.js"></script>
    <div id="twikoo" class="twikoo"></div>
    <script>
        twikoo.init({
            envId: ""
        })
    </script>
 
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2022-2023
        <i class="ri-heart-fill heart_icon"></i> hqz
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="hqz的博客"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/java/">java</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/machineLearning/">机器学习</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/webGIS/">webGIS</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/bigData/">大数据</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/hellogis/">GIS</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/academic/">学术</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/other/">other</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2023/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js"></script>
<script src="https://cdn.staticfile.org/mathjax/2.7.7/config/TeX-AMS-MML_HTMLorMML-full.js"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->

<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->
 
<script src="/js/clickBoom2.js"></script>
 
<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->
 
<script src="/js/dz.js"></script>
 
<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>